title,company,company raiting,description,date publish,text article,author username
A B-тесты в PostgreSQL,OTUS,716.78,Цифровые навыки от ведущих экспертов,2025-03-13,"Привет, Хабр! Сегодня разберём, какреализовать A/B‑тестирование начистом PostgreSQL, безвыгрузки данных всторонние системы. Рассмотрим полный цикл: отструктуры таблиц и оптимизации запросов достатистического анализа (T‑тест, Манна‑Уитни, байесовские методы) и визуализации результатов. Постановка задачи и проектирование базы Начнём соснов. Представим, чтомы отвечаем заA/B‑тест нового лендинга. Заказчик требует реального времени, авыгрузка всторонние BI‑системы— это вообще непронас. Так чтонужно построить базу, где все данные будут сразу готовы канализу. Дляначала создадим таблицу, которая будет собирать все события пользователей. Вот каквыглядит базовая структура: -- Создаем таблицу для хранения событий A/B-теста CREATE TABLE ab_test_events ( event_id SERIAL PRIMARY KEY, -- Уникальный идентификатор события user_id INTEGER NOT NULL, -- Идентификатор пользователя variant CHAR(1) NOT NULL, -- Вариант теста: 'A' или 'B' event_time TIMESTAMPTZ NOT NULL DEFAULT NOW(), -- Время события event_type TEXT NOT NULL, -- Тип события (например, 'page_view', 'cl event_payload JSONB NOT NULL -- Дополнительные данные (например, время до ); Используем JSONB для event_payload — потому что иногда пользователи творят что угодно: один день клик, другой — заполняют анкету с кучей метаданных. Гибкость и масштабируемость важнее, чем жесткая структура. Пример данных: INSERT INTO ab_test_events (user_id, variant, event_type, event_payload) VALUES ('550e8400-e29b-41d4-a716-446655440000', 'A', 'submit', '{""conversion"": ""1""}'), ('550e8400-e29b-41d4-a716-446655440001', 'B', 'submit', '{""conversion"": ""0""}'), ('550e8400-e29b-41d4-a716-446655440002', 'A', 'submit', '{""conversion"": ""1""}'), ('550e8400-e29b-41d4-a716-446655440003', 'B', 'submit', '{""conversion"": ""1""}'); Никакой крутой код не спасёт ситуацию, если запросы выполняются медленно. Поэтому создаем GIN‑индекс для быстрого поиска по JSONB. -- Индекс для ускорения поиска по полям внутри JSONB CREATE INDEX idx_event_payload ON ab_test_events USING gin (event_payload); Агрегация данных: После того как данные начали сыпаться, самое время собрать статистику. Ведь что толку в красивой таблице, если мы не можем понять, какой вариант лендинга работает лучше? Для нас критичен показатель конверсии — событие типа submit означает успешную регистрацию. Приведу пример запроса, который группирует данные по вариантам теста и вычисляет конверсию: -- Агрегируем данные по вариантам A/B-теста SELECT variant, -- Группируем по варианту ('A' или 'B') COUNT(*) AS total_events, -- Общее количество событий COUNT(*) FILTER (WHERE event_type = 'submit') AS total_conversions, -- События рег COUNT(*) FILTER (WHERE event_type = 'submit')::numeric / COUNT(*) AS conversion_rat 13.03.2025, 16:38 A/B-тесты в PostgreSQL / Хабр https://habr.com/ru/companies/otus/articles/888530/ 2/16 FROM ab_test_events GROUP BY variant; Фильтрация через конструкцию FILTER позволяет сразу получить нужные подсчеты. Иногда хочется знать не только конверсию, но и дополнительные детали — например, сколько времени пользователи тратят на регистрацию. Допустим, параметр time_to_submit хранится в event_payload . Тогда запрос будет таким: -- Вычисляем среднее время до регистрации для каждой группы SELECT variant, AVG((event_payload->>'time_to_submit')::numeric) AS avg_time_to_submit -- Приводим FROM ab_test_events WHERE event_payload ? 'time_to_submit' -- Проверяем наличие ключа в JSONB GROUP BY variant; Оператор ? позволяет отфильтровать данные и не портить статистику нулевыми значениями. Важно учитывать случаи, которые могут исказить результаты. Перед расчетом тестов нужно проверить, есть ли аномальные пользователи: SELECT user_id, COUNT(*) FROM ab_test_events WHERE event_type = 'submit' GROUP BY user_id HAVING COUNT(*) > 10; Если у одного пользователя слишком много событий, возможно, он тестирует систему, а не является реальным пользователем. Иногда вариант A имеет больше пользователей, чем B. Проверяем: SELECT variant, COUNT(*) AS users FROM ab_test_events 13.03.2025, 16:38 A/B-тесты в PostgreSQL / Хабр https://habr.com/ru/companies/otus/articles/888530/ 3/16 WHERE event_type = 'submit' GROUP BY variant; Если разница > 10%, тесты могут быть смещены. Если пользователи видели A и B, это может создать шум в данных: SELECT user_id, COUNT(DISTINCT variant) FROM ab_test_events WHERE event_type = 'submit' GROUP BY user_id HAVING COUNT(DISTINCT variant) > 1; Таких пользователей можно исключить из анализа. Освоить мощные навыки анализа данных (анализ требований + статистика + BI) можно на курсе «Аналитик данных». Статистический анализ: тесты, доверительные интервалы и байесовский разбор T-тест для оценки разницы в конверсии T‑тест применяется, если распределение данных нормально. Проверим это: Прежде чем применять T‑тест, важно убедиться, что данные нормально распределены: SELECT variant, AVG((event_payload->>'conversion')::numeric) AS avg_conversion, STDDEV((event_payload->>'conversion')::numeric) AS stddev_conversion FROM ab_test_events WHERE event_type = 'submit' GROUP BY variant; Если стандартное отклонение слишком велико, T‑тест может быть неточным. Теперь применим T‑тест: 13.03.2025, 16:38 A/B-тесты в PostgreSQL / Хабр https://habr.com/ru/companies/otus/articles/888530/ 4/16 WITH stats AS ( SELECT variant, AVG((event_payload->>'conversion')::numeric) AS avg_conversion, VARIANCE((event_payload->>'conversion')::numeric) AS variance, COUNT(*) AS cnt FROM ab_test_events WHERE event_type = 'submit' GROUP BY variant ) SELECT a.avg_conversion AS conversion_A, b.avg_conversion AS conversion_B, (a.avg_conversion - b.avg_conversion) / sqrt((a.variance / a.cnt) + (b.variance / b.cnt)) AS t_statistic FROM stats a, stats b WHERE a.variant = 'A' AND b.variant = 'B'; Если t_statistic > 2 или < -2, разница между группами A и B статистически значима. Если t_statistic ближе к 0, различие случайное. Манна-Уитни Если данные не подчиняются нормальному распределению (например, если конверсии редкие или неравномерные), применяется тест Манна‑Уитни. Перед запуском теста, проверим: SELECT variant, COUNT(*) AS cnt, SUM((event_payload->>'conversion')::numeric) AS total_conversions FROM ab_test_events WHERE event_type = 'submit' GROUP BY variant; Если между группами A и B большая разница в количестве наблюдений, Манна‑Уитни предпочтительнее T‑теста. Манна‑Уитни работает по ранжированию значений: 13.03.2025, 16:38 A/B-тесты в PostgreSQL / Хабр https://habr.com/ru/companies/otus/articles/888530/ 5/16 WITH ranked AS ( SELECT user_id, variant, (event_payload->>'conversion')::numeric AS conversion, RANK() OVER (ORDER BY (event_payload->>'conversion')::numeric) AS rnk FROM ab_test_events WHERE event_type = 'submit' ) SELECT variant, SUM(rnk) AS total_rank, COUNT(*) AS cnt FROM ranked GROUP BY variant; Если суммы рангов сильно различаются, разница между вариантами значима. Байесовский анализ Байесовский анализ не только оценивает вероятность, но и показывает распределение вероятности конверсии. Сначала вычислим 95% доверительный интервал: SELECT variant, PERCENTILE_CONT(0.025) WITHIN GROUP (ORDER BY (event_payload->>'conversion')::numer PERCENTILE_CONT(0.975) WITHIN GROUP (ORDER BY (event_payload->>'conversion')::numer FROM ab_test_events WHERE event_type = 'submit' GROUP BY variant; Теперь применим PL/Python для моделирования вероятностей. Создадим функцию: CREATE OR REPLACE FUNCTION beta_distribution(alpha numeric, beta numeric) RETURNS TABLE(x numeric, pdf numeric) 13.03.2025, 16:38 A/B-тесты в PostgreSQL / Хабр https://habr.com/ru/companies/otus/articles/888530/ 6/16 AS $$ import numpy as np from scipy.stats import beta as beta_dist x = np.linspace(0, 1, 100) pdf = beta_dist.pdf(x, alpha, beta) return list(zip(x, pdf)) $$ LANGUAGE plpython3u; Вызовем функцию: SELECT * FROM beta_distribution(5, 2); Чем выше alpha и beta, тем уже доверительный интервал. Если распределения сильно пересекаются, разница между группами незначительна. Если variant A имеет высокий пик в beta‑распределении, он значительно лучше variant B. Оптимизация производительности Оптимизация — это не разовое действие, а постоянный процесс. Partial indexes и BRIN-индексы Если ваша таблица растёт до гигантских размеров, индексировать всё подряд не стоит. Частичные индексы — наше всё для актуальных данных. -- Частичный индекс для ускорения выборок по последним 30 дням CREATE INDEX idx_recent_events ON ab_test_events (event_time) WHERE event_time > NOW() - INTERVAL '30 days'; Если данных слишком много, можно применить BRIN‑индексы, которые работают быстрее и экономят место: -- BRIN-индекс для эффективного сканирования временных данных CREATE INDEX idx_brin_time ON ab_test_events USING brin (event_time); 13.03.2025, 16:38 A/B-тесты в PostgreSQL / Хабр https://habr.com/ru/companies/otus/articles/888530/ 7/16 GROUPING SETS: многомерная агрегация в одном запросе Когда требуется собрать несколько уровней агрегации (например, по дням и по вариантам), GROUPING SETS приходят на помощь: -- Используем GROUPING SETS для получения детальной и сводной статистики SELECT variant, DATE_TRUNC('day', event_time) AS day, COUNT(*) AS total_events FROM ab_test_events GROUP BY GROUPING SETS ( (variant, DATE_TRUNC('day', event_time)), -- Статистика по каждому дню для каждого (variant) -- Общая статистика по каждому варианту ); GROUPING SETS позволяют одним запросом получить сразу несколько уровней агрегации. Визуализация результатов Первым делом подключаемся к PostgreSQL, выполняем запрос и загружаем данные в DataFrame. import psycopg2 import pandas as pd import matplotlib.pyplot as plt # Устанавливаем соединение с базой данных conn = psycopg2.connect( host=""localhost"", # Адрес сервера базы данных database=""production_db"", -- Имя базы данных user=""prod_user"", # Пользователь базы данных password=""supersecret"" # Пароль (в продакшене храните в секьюрном хранилище!) ) # SQL-запрос для получения агрегированных данных по вариантам query = """""" WITH aggregated AS ( SELECT variant, COUNT(*) AS total_events, 13.03.2025, 16:38 A/B-тесты в PostgreSQL / Хабр https://habr.com/ru/companies/otus/articles/888530/ 8/16 COUNT(*) FILTER (WHERE event_type = 'submit') AS total_conversions, COUNT(*) FILTER (WHERE event_type = 'submit')::numeric / COUNT(*) AS conversion STDDEV((event_payload->>'conversion')::numeric) AS std_dev FROM ab_test_events GROUP BY variant ) SELECT * FROM aggregated; """""" # Загружаем данные в DataFrame df = pd.read_sql(query, conn) conn.close() # Выводим данные для проверки print(df) А теперь построим график: import matplotlib.pyplot as plt import pandas as pd # Симулируем данные, как если бы они были получены из запроса data = { 'variant': ['A', 'B'], 'conversion_rate': [0.15, 0.22], # Пример конверсии: 15% для A и 22% для B 'std_dev': [0.02, 0.03] # Пример стандартного отклонения } df = pd.DataFrame(data) # Создаем график plt.figure(figsize=(10, 6)) bars = plt.bar(df['variant'], df['conversion_rate'], yerr=df['std_dev'], capsize=8) # Настраиваем оси и заголовок plt.xlabel('Вариант теста') plt.ylabel('Конверсия') plt.title('Результаты A/B-теста: Конверсия по вариантам') plt.grid(axis='y', linestyle='--', alpha=0.7) plt.ylim(0, 0.3) # Устанавливаем лимиты по оси Y # Добавляем числовые аннотации над столбиками for bar in bars: height = bar.get_height() 13.03.2025, 16:38 A/B-тесты в PostgreSQL / Хабр https://habr.com/ru/companies/otus/articles/888530/ 9/16 plt.text(bar.get_x() + bar.get_width()/2, height + 0.01, f'{height:.2%}', ha='center', va='bottom') plt.show() График на рандом данных В заключение всем, кому интересна сфера аналитики данных, рекомендую посетить открытый урок 25 марта «Базовый сбор требований в работе дата-аналитика». Рассмотрим жизненный цикл задачи в анализе данных, обсудим, какие ключевые вопросы необходимо задать и как грамотно зафиксировать ответы. Также разберем, как справляться с возражениями и избегать лишнего стресса в процессе. Записаться",badcasedaily1
"Cocoapods, Carthage, SPM как выбрать менеджер зависимостей в iOS",red_mad_robot,117.94,№1 в разработке цифровых решений для бизнеса,2023-03-20,"Старший iOS-разработчик red_mad_robot Аня Кочешкова рассказывает, чем отличаются три менеджера зависимостей, в каких случаях и для каких задач подойдёт тот или иной. Материал будет полезен джун-специалистам, которые только начали погружаться в разработку: специально для них подробно объясняем, что такое семантическое версионирование, как устроены модули кода и в чём разница между динамическими и статическими библиотеками. Что такое менеджер зависимостей и зачем он нужен В современной разработке зависимость — это написанный кем-то другим код, который используется в вашей программе. Добавление зависимости позволяет воспользоваться уже сделанной кем-то хорошей работой и избежать «изобретения велосипеда», а также лишних действий: проектирования, написания, тестирования, отладки и поддержки определённой логики. Такие «куски логики» называют пакетами, библиотеками или модулями. Чем сложнее проект, тем больше сторонних библиотек (правда, некоторые приложения могут вообще отказываться от их использования), а у этих библиотек много разных версий. Они могут ссылаться друг на друга и накладывать ограничения на эти версии. Эту работу можно проиллюстрировать таким бытовым примером. Чтобы телефон показал погоду на улице, не нужно встраивать в него термометр и вытаскивать за окно. В нём уже зашита специальная технология, которая позволяет ему связаться с «библиотекой», где хранятся данные гидрометцентра, которые, как и погода, всё время обновляются. Иногда с новой версией добавляется какая-нибудь новая функциональность. В примере с термометром библиотека с новой версией умеет ещё и выдавать карту дождей, а до этого могла показывать только температуру. Менеджеры зависимостей позволяют избежать долгого и муторного решения такого набора «уравнений» — то есть берут на себя часть работы, которую без них разработчик делал бы руками. Это система, которая позволяет управлять зависимостями в вашем проекте. А ещё у сторонних библиотек часто могут выходить новые версии с исправленными ошибками, и благодаря менеджеру зависимостей вам не нужно следить за их выходом — он всё обновит за вас. Предположим, вы решили подключить в ваш проект Firebase Crashlytics и GoogleUtilities. Обе эти библиотеки ссылаются на библиотеку PromisesObjC. Firebase хочет, чтобы она была любой версии, начинающейся с 2.1, а GoogleUtilities — чтобы она была от 1.2 до 3.0. Такое уравнение, конечно, можно решить вручную, но что делать, когда этих уравнений несколько десятков, а уровней зависимостей — два, три или больше? Для таких задач в распоряжении iOS-разработчика существуют три основных менеджера зависимостей: 1. Cocoapods. Пожалуй, самый распространённый сегодня, он требует лишь указывать список зависимостей в определённом формате в текстовом файле. 2. Carthage. Менее удобен в этом плане: вы также указываете список зависимостей в текстовом файле, но помимо этого нужно ещё немного «самостоятельной работы», о которой расскажем ниже. 3. SPM, или Swift Package Manager. Официальный менеджер зависимостей от Apple. Появился недавно, и многие команды переходят на него с Cocoapods. Позволяет управлять зависимостями как через графический, так и через текстовый интерфейс прямо из Xcode. Случается, что нужная библиотека не поддерживает необходимый менеджер зависимостей, потому что разработчики каждой библиотеки сами выбирают, какой менеджер поддержать. Поддержка каждого менеджера — это работа, которая занимает определённое время, поэтому часто выбирают работать только с самым популярным менеджером, Сocoapods. А если в проекте, например, SPM, а библиотека есть только в Cocoapods или просто нужна в одной конкретной версии, приходится добавлять вручную как framework / xcframework, о чём мы также поговорим позже. Но прежде чем начать говорить об управлении зависимостями, нужно разобрать несколько связанных с ним терминов. Фантастическое Semantic Versioning, и где оно обитает Semantic Versioning (SemVer) — это стандарт, который говорит о том, как работать с версиями пакетов/библиотек/приложений, — как их указывать и накладывать на них ограничения. Зачем накладывать ограничения? Возьмём такой пример. Разработчик создаёт приложение и использует некоторый набор библиотек для упрощения работы — например, библиотек для работы с сетью Alamofire. Допустим, начиная с определённой версии в библиотеке появляется поддержка какой-то новой функциональности, например Modern Concurrency. Эта функциональность доступна только начиная с определенной версии библиотеки. В таком случае от разработчика потребуется всего лишь поднять версию Alamofire в манифесте, где он описывает используемые библиотеки. Если начиная с какой-то версии библиотеки исправляется некий важный баг, но не сильно меняется функциональность самой библиотеки, то обновление до свежей исправленной версии произойдёт без лишнего вмешательства разработчика, в следующий раз при обновлении текущих библиотек проекта. Разберём, что такое SemVer, чтобы понимать формат версий, с которыми придётся работать, а также узнаем, как можно указать версию зависимости и наложить на неё ограничения. Этот набор правил — глобальный стандарт, который используется повсеместно, начиная от пакетов Node.js и заканчивая версиями приложений в App Store. По этому стандарту номер версии — это строка, которая состоит из чисел, разделённых точкой, и имеет формат A.B.C, где A — мажорная версия, B — минорная версия и C — патч-версия. Версии сравниваются слева направо, например: 1.0.0 < 2.0.0 < 2.1.0 < 2.1.1. Но зачастую мажорная версия увеличивается или при выпуске каких- либо масштабных фич, или при редизайне приложения, или просто при желании обозначить принципиальное обновление приложения/библиотеки. Синтаксис SemVer в Cocoapods, Carthage и SPM Cocoapods В Cocoapods можно указать одну из стратегий выбора версии: 1. Если необходима конкретная версия, просто укажите её без дополнительных символов, как есть: '5.0.0'. 2. Если необходима точная версия, но при этом вы также хотите обновляться, если придут исправления багов в патч-версии, используйте операнд ~>: '~> 5.0.0'. 3. Можно указывать неравенства: '> 5.0.0', '≥ 5.0.0' или '< 5.0.0', '≤ 5.0.0'. В этом случае Cocoapods обновит библиотеку до 5.0.1, 5.0.2 и т. д., но не возьмёт обновление 5.1.0, так как в нём уже повышена минорная, а не патч-версия. Если вы также хотите завязаться на минорную версию, достаточно просто указать '~> 5.0'. Carthage В Carthage используется тот же формат, что и в Cocoapods, с тем только исключением, что точная версия указывается через равенство: == 5.0.0. SPM SPM позволяет указать стратегию выбора версии прямо в интерфейсе, доступны следующие: Аналогичный синтаксис и у Package.swift файла. Модули — что к чему: библиотеки, фреймворки, XCFramework и Swift-пакеты Организация кода в iOS построена на понятии модулей. Модуль — это отдельный кусок кода или функциональности, который можно распространять разными способами и переиспользовать. Существует четыре вида модулей. Разберёмся, в чём разница: 1. Библиотека — это исполняемый код, скомпилированный под конкретную архитектуру. 2. Фреймворк — папка, содержащая в себе библиотеку и вспомогательные файлы. 3. XCFramework — набор фреймворков под разные архитектуры. 4. Swift-пакет — фреймворк, который используется для работы со Swift Package Manager. Если говорить на языке бытовых примеров, то код — это комната, а модули — разные предметы в ней. Торшер, например, позволяет включать и выключать свет, но как он работает внутри, мы не знаем. И этим торшером могут пользоваться в разных квартирах и комнатах разные люди. Библиотеки Библиотека — это исполняемый код, скомпилированный под конкретную архитектуру, например arm64. Архитектуры могут различаться для разных типов устройств и симуляторов: • iOS-устройства — архитектуры armv7, armv7s, arm64, • iOS-симулятор — архитектуры i386 и x86_64. Исходный код — текст программы, который написал разработчик. Исполняемый код — код, который получился в результате компиляции. Компилятор переводит исходный код в машинный, который сможет исполнить операционная система, — на выходе получается исполняемый код. Библиотеки бывают статическими и динамическими (об этом позже), и представляют собой один бинарный файл. Если представить, что библиотека — это настоящая библиотека со стеллажами, в которых хранятся книги, то эти книги — справочники о том, как что-то сделать (методы для выполнения разных задач). По сути, это скомпилированный набор объектных файлов, которые программа может использовать для выполнения любой задачи. То есть все файлы, которые библиотека собирается использовать, были скомпилированы до этапа компоновки. Фреймворки Фреймворк (.framework) представляет собой папку с файлами, в которой лежит сама библиотека, хедеры и ресурсы. Эта папка должна иметь определённую структуру, но нет нужды запоминать её, так как фреймворки, как правило, собираются самим Xcode. Поддержка фреймворков добавилась в iOS 8. Фреймворки служат той же цели, что и библиотеки, но могут включать в себя различные ресурсы — картинки, документацию, строковые файлы. Они также могут быть статическими или динамическими. Что использовать: фреймворк или библиотеку В чём же разница между фреймворком и библиотекой? И когда их использовать? Принципиальное различие — в архитектуре. XCFrameworks Для начала стоит рассказать, что такое fat-фреймворки. Это такие фреймворки, которые содержат библиотеки для разных архитектур и платформ, соединённые в одну. С появлением Xcode 11 на замену им пришли XCFrameworks, которые ввели структуру, позволяющую разнести библиотеки для разных платформ и архитектур по папкам. XCFramework — это, по сути, такой усовершенствованный fat- фреймворк. Он также содержит различные библиотеки для разных архитектур и платформ, но теперь нет необходимости трудиться над тем, чтобы объединить их в одну. В одном XCFramework может лежать несколько разных фреймворков — для iOS, macOS, tvOS, watchOS, а также для разных архитектур — armv7, x86_64 и т. д., каждый в своей папке. В каком-то смысле фреймворк — тоже разновидность библиотеки, а XCFramework — разновидность фреймворка, но всё же эти понятия принято разделять. Создатель зависимости сам выбирает, в каком виде её распространять. Нам важно понимать различие между этими видами, выбор можно сделать, только создавая свою библиотеку. Swift Package Swift-пакет используется для распространения только через SPM. Содержит исходный код, а также Package.swift файл, который описывает его конфигурацию. Доступен начиная со Swift 3. Static vs Dynamic Прежде чем начать, наконец, говорить об управлении зависимостями, важно разобраться, в чём различие между статическими и динамическими библиотеками и фреймворками. Первое, что здесь важно понимать: системные iOS- и macOS- библиотеки — динамические. Приложения могут хранить ссылку на библиотеку. Например, системная библиотека Foundation — динамическая. Она лежит на айфоне в единственном экземпляре, а все приложения лишь ссылаются на неё. Фреймворки, как и библиотеки, бывают динамические и статические и представляют собой обёртку библиотеки. Так, динамические библиотеки вне фреймворков не поддерживаются на iOS, watchOS и tvOS, и здесь важно отметить, что во всех дальнейших сравнениях, говоря «динамическая библиотека», мы будем иметь в виду её использование в рамках динамического фреймворка. Фреймворки в Cocoapods и Carthage Зависимости, распространяемые через Cocoapods, называются кокоаподами, или подами. До iOS 8 под представлял собой fat- статическую библиотеку. Есть специальная инструкция — по ней Cocoapods использует фреймворки вместо статичных библиотек, которые не поддерживались Swift до Xcode 9 и CocoaPods 1.5.0. Так что раньше эта директива была обязательна, сейчас — опциональна. Если нужно более быстрое время запуска, можно её убрать (см. предыдущий раздел). Carthage поддерживает только фреймворки. target 'TargetName' do use_frameworks! end Три менеджера зависимостей: Cocoapods, Carthage, SPM Наконец, то, ради чего мы тут собрались, — менеджеры зависимостей. Посмотрим, какие они бывают, чем отличаются и как выбрать между ними. Cocoapods Это один из самых популярных сегодня менеджеров зависимостей. Позволяет указать список зависимостей в текстовом файле, а затем генерирует на основе его .xcworkspace, который будет содержать два проекта — ваш проект и проект с подами. Как правило, почти все библиотеки поддерживают Cocoapods, хотя с развитием SPM стали появляться и такие, которые поддерживают исключительно SPM (в основном это библиотеки Apple). В таком случае, если ваш менеджер зависимостей Cocoapods, единственный вариант воспользоваться библиотекой — вручную её собрать и прилинковать к проекту. Чтобы понять, поддерживает ли библиотека Cocoapods, достаточно заглянуть в readme, либо обратить внимание на наличие .podspec- файла в корне репозитория. Все библиотеки, опубликованные в Cocoapods, можно посмотреть на их официальном сайте. Cocoapods в вашем проекте Для понимания работы Cocoapods важны два термина — подфайл и спека. Подфайл описывает все зависимости приложения, а спека — библиотеку (её название и набор её зависимостей) для Cocoapods. Спеки хранятся в общем репозитории спек Cocoapods. Ещё их можно хранить в приватных репозиториях. Для описания конфигурации зависимостей используется подфайл. Этот файл написан на языке Ruby. 1. В файле опционально укажите источник спек. Источников может быть несколько, даже приватные. source 'https://github.com/CocoaPods/Specs.git' — официальный источник спек, который указан по умолчанию. source 'https://github.com/Artsy/Specs.gi — можно указать свои источники спек, в том числе приватные. 2. Укажите минимальную версию iOS: platform: ios, '9.0'. 3. Укажите таргет и набор зависимостей для него. Если таргетов несколько, укажите все, так как зависимости для них могут различаться. 4. Декларируйте зависимость. Первым идёт её официальное название (то самое, которое указано в спеке), затем версия. Используйте в названии правила из блока про SemVer. Есть и другие способы указать зависимости. Это может понадобиться в случаях, когда нужно использовать какую-то конкретную версию библиотеки. Например, из определённой ветки репозитория, где она хранится, или даже из определённого коммита. Либо указать путь локально, если есть только её файл. target 'MyApp' do pod 'GoogleAnalytics', '~> 3.1' ... end 1. Можно указать ресурс репозитория, с которого будем загружать библиотеку: pod 'CRToast', :git => 'https://github.com/akhatmullin/CRToast.git' 2. Можно указать нужную ветку в репозитории: pod 'CRToast', :git => 'https://github.com/akhatmullin/CRToast.git', :branch => 'dev' 3. Или тег: pod 'CRToast', :git => 'https://github.com/akhatmullin/CRToast.git', :tag => '0.7.0' 4. Или коммит: pod 'CRToast', :git => 'https://github.com/akhatmullin/CRToast.git', :commit => '082f8310af' 5. Можно указать на локальный под (путь должен вести на папку, где лежит .podspec): pod 'AFNetworking', :path => '~/Documents/AFNetworking' Рядом с подфайлом обычно лежит Podfile.lock. Этот файл описывает конечные версии зависимостей после их установки. Он обычно так же заливается в GitHub, как и сам Podfile, чтобы избежать ошибок и рассинхрона при установке зависимостей у разных разработчиков. Ещё он ускоряет выполнение pod install. Чтобы приступить к работе, запустите pod install. Можно выполнить pod update, если вы хотите обновить все библиотеки в проекте, иначе же будут использоваться фиксированные версии зависимостей из Podfile.lock. Как Cocoapods работает под капотом Pod install генерирует workspace — файл .xcworkspace, состоящий из двух проектов — проекта с вашим приложением и отдельного проекта, в котором хранятся зависимости (поды). Чтобы всё работало корректно, всегда открывайте workspace-файл. Если внимательнее посмотреть, что происходит, можно увидеть, что продукт этого проекта линкуется в основной проект: Также Cocoapods добавляет пару скриптов в Build Phases: Поддержка Cocoapods в вашей библиотеке Чтобы библиотека была доступна через Cocoapods, нужно сначала описать библиотеку или под при помощи спеки, собрать её и отправить спеку в приватный либо глобальный репозиторий Cocoapods. Разберёмся, как это сделать. 1. Создайте спеку и файл лицензии. Лицензию нужно добавлять для любой библиотеки, и чаще всего используется лицензия MIT — самая популярная лицензия для программ с открытым исходным кодом. Её текст можно найти в интернете. Здесь нужно описать вашу библиотеку. Файл принято форматировать так, чтобы вторая часть строки была выровнена. Pod::Spec.new do |spec| # Название библиотеки. Именно его будут указывать разраб spec.name = 'NAME' # Версия spec.version = '1.2.1' # Описание spec.summary = 'Describe your framework. # Здесь можно указать путь на вебсайт или просто GitHub- spec.homepage = 'https://xxx' # Путь к лицензии. Да, её тоже нужно создать spec.license = { type: 'MIT', file: 'LIC # Автор библиотеки spec.author = { ""Your Name"" => 'your-em # Путь к исходным файлам (чаще всего это ваш репозитоий) spec.source = { :git => 'https://github # Минимальная поддерживаемая версия iOS и Swift spec.ios.deployment_target = '13.0' spec.swift_version = '5.0' # Если библиотека использует локальный фреймворк, его ну spec.vendored_frameworks = 'Path/To/Files/NameOfVend 2. Проверьте, что всё корректно. Следующая команда валидирует спеку — проверяет, что она не имеет ошибок, а проект билдится — pod lib lint. 3. Отправьте спеку в репозиторий спек. Как правило, это делается уже на финальном этапе разработки — после того, как прошла проверка и вы готовы публиковать работу, — pod trunk push NAME.podspec. Carthage # Указываем вайлдкард-пути к файлам, которые необходимо spec.source_files = 'Path/To/Files/**/*.{swif # Путь к ресурсам spec.resources = 'Path/To/Files/Resources/ # Укажите здесь все ассеты, которые вам нужны, включая . spec.resource_bundles = {'BundleName' => ['Path/T # Указываем зависимости spec.dependency ""Alamofire"", ""~> 5.0.0"" # И системные зависимости spec.frameworks = 'Foundation' spec.ios.frameworks = 'UIKit' end Carthagе — это децентрализованный менеджер зависимостей, которые не нужно «поддерживать» разработчику. Carthage сам выкачивает библиотеку из репозитория, компилирует её и предоставляет в готовом виде. Поэтому, если хотите использовать Carthage, не нужно искать в репозитории с библиотекой определённые инструкции или слова о том, что он поддерживается, — просто попробуйте. Нет у Carthage и централизованного источника, как у Cocoapods. Этим источником, по сути, является сам GitHub. Конечно, этого не всегда бывает достаточно. Некоторые библиотеки Carthage просто не может «переварить» — что-то не собирается, а что-то отваливается. Задача разработчика — проверить, всё ли работает корректно. Наконец, Carthage может автоматически подхватить уже собранный фреймворк, если он прикреплён к соответствующему релизу на GitHub. Carthage в вашем проекте Зависимости в Carthage описываются в текстовом файле Cartfile. Он очень похож на Podfile, только гораздо проще по структуре. Мы указываем источник, имя зависимости и версию. 1. Источник зависимости. Как правило, GitHub, либо git для универсальных репозиториев Git, размещённых в другом месте. Ключевое слово git сопровождается путём к репозиторию, будь то удалённый URL-адрес, используя git://, http://, или ssh://, или локальный — путём к репозиторию git на компьютере разработчика. 2. Название. Через слеш указывается владелец и название репозитория, которые можно подсмотреть в ссылке на репозиторий. 3. Версия. Особенности указания версий мы уже рассмотрели в блоке про семантическое версионирование, но можно также указывать и ветки. Примеры: 1. Версия — github ""Alamofire/Alamofire"" == 2.0 2. Ветка — github ""username/project"" ""branch"" 3. Локальный проект — git ""file:///directory/to/project"" ""branch"" 4. Бинарник (путь указывается в json) — binary ""https://my.domain.com/release/MyFramework.json"" ~> 2.3 После описания зависимостей потребуется ещё несколько дополнительных шагов. 1. Вызовите carthage update. Carthage склонирует репозитории с указанных в Cartfile путей и затем сбилдит для каждой зависимости фреймворк. После окончания работы вы увидите несколько файлов и каталогов: 2. Полученные фреймворки затем вручную перетащите из папки Build в проект в секцию Frameworks, Libraries и Embedded Content. 3. Carthage требует добавления нового скрипта в Build Phases в качестве workaround бага Apple. Это позволяет dSYM- файлам корректно подтягиваться при архивации. 4. Перейдите в Build Phases и добавьте новый Run Script. Добавьте следующую команду: /usr/local/bin/carthage copy- frameworks. 5. Нажмите на плюс под Input Files и добавьте запись для каждой платформы: $(SRCROOT)/Carthage/Build/iOS/Alamofire.framework. Как Carthage работает под капотом Как и в случае с Cocoapods, создаётся лок-файл Cartfile.resolved, который фиксирует конечные версии зависимостей. Каталог Carthage содержит два подкаталога: 1. Build. Содержит собранный фреймворк для каждой зависимости. 2. Checkouts. Содержит клонированные исходники, которые Carthage потом компилирует. Это своего рода кэш, который ускоряет работу. Поддержка Carthage в вашей библиотеке Для поддержки Carthage достаточно сделать релиз на GitHub. Но никогда не помешает проверить, всё ли билдится корректно в вашем случае и, если есть какие-то проблемы, решить их. Для этого, выполните carthage build --no-skip-current. Carthage попытается сбилдить ваш проект и создаст в нём папку Carthage. Также стоит обратить внимание, что он видит только те схемы, которые помечены в проекте как Shared. SPM Или Swift Package Manager — официальный менеджер зависимостей от Apple. Работа с ним происходит прямо в Xcode, а .Xcodeproj файлы больше не нужны (хотя при желании есть возможность сгенерировать файл проекта из SPM-пакета). Если заглянуть в репозиторий библиотеки, то первое, что будет говорить о поддержке SPM, — это наличие Package.swift-файла. SPM в вашем проекте Добавление зависимостей в проект происходит в секции Package Dependencies. 1. Для начала выберите File > Swift Packages > Add package dependecy. 2. Найдите модуль по поиску, либо введите URL репозитория. 3. Укажите версию: Как и во всех предыдущих случаях, SPM создает Package.resolved, назначение которого то же, что и у других менеджеров зависимостей. В меню File > Swift Packages доступно несколько действий: • Reset Package Caches удаляет кэши из Derived Data. • Resolve Package Versions резолвит версии зависимостей. Создаёт Package.resolved, если его не существует, но не меняет его, если он уже есть. Может потребоваться, если Package.resolved был, например, обновлён извне. • Update to Latest Packages Versions обновляет версии пакетов, может модифицировать Package.resolved. Поддержка SPM в вашей библиотеке Если вы разрабатываете библиотеку и хотите распространять её через SPM, нужно будет создать Package.swift файл. Таким образом может быть сконфигурирован и обычный проект, что позволяет отойти от использования project-файлов. Конфигурация через SPM выглядит чище и более проста для понимания и редактирования, чем стандартные project-файлы, которые зачастую нечитаемы. Если вы пишете библиотеку и хотите распространять её через SPM, обязательно убедитесь, что потенциальные потребители библиотеки не используют Cocoapods. Одновременная поддержка Cocoapods и SPM — часто непростая задача. import PackageDescription let package = Package( // Название нашего пакета name: ""Resources"", // Платформы, которые поддерживаются нашим пакетом platforms: [ .iOS(.v11), ], // То, что будут использовать сторонние приложения products: [ .library( name: ""Resources"", // Динамический или статический продукт // по дефолту значение nil - SPM сам будет поним // преференция, скорее всего, будет отдаваться . type: .dynamic, targets: [""Resources""]), ], // Зависимости, необходимые для работы нашего пакета // здесь они просто загружаются, добавляются они в target dependencies: [ // Название пакета, путь к нему и источник (ветка, в .package( name: ""R.swift.Library"", url: ""https://github.com/mac-cain13/R.swift.Libr branch: ""master""), .package( name: ""SVGKit"", url: ""https://github.com/SVGKit/SVGKit.git"", И как выбрать подходящий менеджер .upToNextMajor(from: ""3.0.0"")), .package( name: ""PhoneNumberKit"", url: ""https://github.com/marmelroy/PhoneNumberKi from: ""3.3.4""), // Пример подключения локального пакета .package(path: ""../Core"") ], targets: [ // Это то, из чего мы будем складывать наш продукт .target( name: ""Resources"", dependencies: [ // Здесь мы указываем зависимости, которые м .product(name: ""RswiftDynamic"", package: ""R. ], resources: [ // Все ресурсы, которые мы хотим использоват // Путь к ним относительный от Sources/имя_п .process(""Resources"") ]) ] ) Универсального ответа нет — выбор менеджера зависимостей зависит исключительно от нужд и особенностей вашего проекта. SPM чаще всего не будут поддерживать старые библиотеки, в то время как Cocoapods вполне могут не поддерживать новые (хотя это и большая редкость, чем первое). Carthage в этом плане может показаться универсальнее. Хоть он более громоздкий и сложный во взаимодействии, но он единственный разделяет шаги управления зависимостями и добавления их в проект. Это даёт контроль над ситуацией и ускоряет разработку — не будет лишних пересборок или заморозки интерфейса, потому что нужно зарезолвить зависимости. Над материалом работали: • текст — Аня Кочешкова, Ника Черникова, • редактура — Виталик Балашов, • иллюстрации — Юля Ефимова. Делимся железной экспертизой от практик в нашем телеграм-канале red_mad_dev. А полезные видео складываем на одноимённом YouTube-канале. Присоединяйся!",redmadrobot
Deutsche Telekom и Perplexity объявили о новом «AI Phone» стоимостью менее 1 000 долларов,,4.4,,2025-03-03,"Еще до начала MWC в Барселоне было очевидно, что хотя бы один оператор представит амбициозный проект по созданию смартфона совместно с ведущей AI-компанией. И вот он: Deutsche Telekom (DT) сообщает, что готовит «AI Phone» — недорогой аппарат, разработанный в тесном сотрудничестве с Perplexity, а также Picsart и другими партнерами, и дополняет его новым приложением-ассистентом под названием «Magenta AI». DT планирует представить устройство во второй половине этого года, а продавать его начнет в 2026-м по цене менее 1 000 долларов. Как уточнил представитель компании, в первую очередь модель будет ориентирована на европейский рынок.«Мы становимся AI-компанией», — заявила на пресс-конференции в понедельник член правления DT, отвечающая за технологии и инновации, Клаудия Немат. При этом она добавила, что оператор не создает фундаментальные большие языковые модели, а разрабатывает «AI- агентов». Примечательно, что Perplexity, стартап из Кремниевой долины, который, по слухам, уже оценивается примерно в 9 миллиардов долларов, отводится ключевая роль в создании смартфона. Это показывает, что компания, прежде всего известная благодаря генеративному AI- поисковику, делает шаг к созданию более «проактивных» решений. «Perplexity переходит от простой “машины ответов” к “машине действий”, — отметил сооснователь и генеральный директор Perplexity Аравинд Шринивас на сцене мероприятия. — Теперь мы не просто отвечаем на вопросы, но и можем бронировать рейсы, столики в ресторанах, отправлять письма, сообщения, совершать звонки и многое другое, включая установку умных напоминаний». Хотя это, по всей видимости, первая крупная сделка Perplexity с оператором связи для разработки AI-интерфейса смартфона, компания уже имеет некоторый опыт в сфере помощников: в январе Perplexity выпустила Android-приложение, которое, судя по всему, может служить прообразом нового «AI Phone». Эта новость — очередная глава знакомой истории из мира телекоммуникаций. Долгие годы как мобильные, так и проводные операторы искали пути усилить позиции в конкурентной борьбе с технологическими гигантами. В частности, они пытались соперничать с Apple и Google, чьи операционные системы и смартфоны в значительной степени отодвигают операторов на периферию, лишая их существенной доли прибыли от приложений и прямого взаимодействия с пользователями. За эти Войти Моя лента Все потоки Разработка Администрирование Дизайн Менеджмент Маркетинг Научпоп годы были попытки объединиться, к примеру, с Mozilla для создания «операторского» смартфона, который смог бы конкурировать с этими двумя компаниями (телефон под названием Firephone, впрочем, так и не нашел отклика). Были и «теплые» отношения с Facebook, стремившейся укрепить собственные позиции в мобильном мире (ныне Meta переключила усилия на другие направления в сфере оборудования и сетей). Подход «двигаться быстро и ломать все на своем пути» не слишком близок консервативным телекомам. Perplexity и Deutsche Telekom сотрудничают уже с апреля 2024 года, а впервые о «AI Phone» DT заговорила год назад на предыдущем Mobile World Congress. Немат не раскрыла подробностей о технических характеристиках и производителе нового девайса, а также не уточнила, на какой операционной системе он будет работать (на презентационных рендерах можно заметить явные черты Android). Представитель DT сообщил, что эта информация появится во второй половине года. Известно лишь, что в смартфон будет глубоко интегрирован AI, созданный с участием Perplexity, чтобы, по словам Немат, «пользователь получил весь спектр возможностей». Она подчеркнула, что AI будет работать прямо с экрана блокировки. Кроме Perplexity, в создании телефона принимают участие Google Cloud, ElevenLabs и Picsart, сообщили в DT. Приложение Magenta AI, являющееся самостоятельной версией голосового помощника от DT, будет доступно для установки на любые Android- и iOS-устройства, но только для 300 миллионов клиентов оператора, добавила Немат. Стремясь использовать нынешний всеобщий интерес к искусственному интеллекту — главную тему MWC этого года, — DT в очередной раз пытается закрепиться в сфере «якорного» оборудования, предлагая и приложение для тех, кто не хочет или не может купить фирменный смартфон. Perplexity же конкурирует не только с хорошо финансируемыми OpenAI и Anthropic, но и с такими гигантами, как Google, уже внедрившей свою модель Gemini во все основные продукты поиска. Выход на рынок «машин действий» в сотрудничестве с крупным оператором связи дает компании определенное преимущество, пусть и временное. Теперь Perplexity, похоже, переходит к следующему этапу эволюции, чтобы предоставить пользователям еще более удобные AI-инструменты. «Раньше для решения этих задач нужно было по отдельности осваивать разные приложения, — говорит Аравинд Шринивас. — Теперь все станет проще, и вы сможете сосредоточиться на более важных делах. Это действительно следующий виток развития, где AI перестанет быть чисто реактивным и превратится в неотъемлемую функцию вашего смартфона, которая всегда на связи и готова помогать проактивно». Пока не ясно, смогут ли DT и Perplexity добиться успеха в сложном рынке смартфонов, где доминируют считаные игроки, а крупные корпорации вроде LG в итоге полностью ушли из этой сферы. 17 Карма 4.4 Рейтинг Технократия @technokratiya ИТ-Компания Сайт Сайт Тем не менее это свидетельствует о том, насколько силен «притягательный эффект» AI сегодня, как даже давние игроки видят в нем спасительное решение, а передовые стартапы — возможность укрепить свои позиции на фоне жесткой конкуренции. Источник: TechCrunch Чтобы не пропустить анонс новых материалов подпишитесь на «Голос Технократии» — мы регулярно рассказываем о новостях про AI, LLM и RAG, а также делимся полезными мастридами и актуальными событиями.",technokratiya
OSINT & Hacking — как работает фишинг для нельзяграма Хабр3,Cloud4Y,71.07,#1 Корпоративный облачный провайдер,2023-03-20,"Взлом Instagram*аккаунта — популярный запрос в поисковиках. Поэтому есть смысл рассказать о том, как это обычно работает. Просто для того, чтобы вы знали, откуда может пойти атака. Чтобы начать попытки заполучить доступ к аккаунту, вы должны знать ник человека, которого вы пытаетесь взломать. Так что небольшая разведка будет очень кстати. Только не увлекайтесь. Существуют различные инструменты для разведки, в первую очередь, поиск пользователя в конкретной соцсети с целью узнать его ник. Я нашёл отличный инструмент под названием «Slash», который можно использовать для поиска любых учётных записей пользователя, если он везде регистрируется под одним ником. Ставим Slash Я проверил Slash на себе, и посмотрите на эти результаты. Некоторые из учетных записей, перечисленных здесь, были созданы много лет назад. git clone https://github.com/theahmadov/slash cd slash pip install -r requirements.txt python slash.py help Slash — это простой консольный инструмент. Но вы также можете использовать такие инструменты, как WhatsMyName Web, который совершенно бесплатен. Вот, посмотрите. Я проверил WhatsMyName на себе. Мой ник «earthtoyash». Теперь, когда мы знаем больше о нашем пользователе, можно использовать эти знания. Например, через отправку фишинговых ссылок. Для этого создадим полезную нагрузку с помощью Zphisher. Ставим Zphisher с GitHub Клонируем репозиторий: Запускаем файл zphisher.sh: При первом запуске он установит зависимости и на этом всё. Система скажет, что Zphisher установлен. После установки вам нужно будет снова запустить zphisher.sh в каталоге zphisher командой ./zphisher , и тогда вы получите что-то вроде этого: git clone --depth=1 https://github.com/htr-tech/zphisher.git cd zphisher && ./zphisher.sh Как вы можете видеть, есть много вариантов и шаблонов, благодаря чему любой может заниматься фишингом. Мы сосредоточимся на нельзяграме. Итак, введите «2» и нажмите Enter. Следующий шаг полностью зависит от вас, выберите любой из них. Затем появится окно с выбором. Я выбрал третий вариант, так как он минималистичный и удобен для того, чтобы показать возможности инструмента. Опять же, чтобы все было просто, я пропущу пользовательский порт, но если вы уже используете порт 8080, то можете изменить его на 8000. Если нет, оставляйте всё как есть. Также важно маскировать URL, ну просто в целях безопасности. Можно использовать чтото вроде этого: Всё, Zphisher создал фишинговую ссылку, которую можно отправить жертве. Как только она нажмёт на ссылку, вы начнёте получать информацию о ней. Например, IPадреса, имена пользователей, пароли и т. д. Ещё можно использовать обратный поиск IP, чтобы определить местоположение вашей цели и многое другое. Итак, вот эти фишинговые ссылки. При нажатии открывается страница, похожая на официальную страницу входа в запрещённую соцсеть. Вот она, нехорошая После ввода учётных данных можно получить много информации на «хакерской» стороне терминала. Вот так, господа и дамы, можно без особого труда взломать учетную запись в нельзяграме. Поэтому в очередной раз напоминаем: нельзя нажимать на ссылки, которым вы не доверяете. Само собой разумеется, не используйте информацию из этой статьи с намерением причинить кому-либо вред. OSINT законен, но фишинг и кража личных данных даже в запрещённой соцсети является уголовным преступлением. И да, * Организация Meta, а также её продукт Instagram, на которые мы ссылаемся в этой статье, признаны экстремистскими и запрещены на территории РФ. Спасибо за внимание! Что ещё интересного есть в блоге Cloud4Y → Информационная безопасность и глупость: необычные примеры → NAS за шапку сухарей → Взлом Hyundai Tucson, часть 1, часть 2 → Столетний язык программирования — какой он → 50 самых интересных клавиатур из частной коллекции",Cloud4Y
OSINT & Hacking — как работает фишинг для нельзяграма,Cloud4Y,71.07,#1 Корпоративный облачный провайдер,2023-03-20,"Взлом Instagram*‑аккаунта — популярный запрос в поисковиках. Поэтому есть смысл рассказать о том, как это обычно работает. Просто для того, чтобы вы знали, откуда может пойти атака. Cloud4Y OSINT & Hacking — как работает фишинг для нельзяграма Простой 3 мин 1.1K Блог компании Cloud4Y , Информационная безопасность *, Социальные сети и сообщества Обзор Перевод Чтобы начать попытки заполучить доступ к аккаунту, вы должны знать ник человека, которого вы пытаетесь взломать. Так что небольшая разведка будет очень кстати. Только не увлекайтесь. Существуют различные инструменты для разведки, в первую очередь, поиск пользователя в конкретной соцсети с целью узнать его ник. Я нашёл отличный инструмент под названием «Slash», который можно использовать для поиска любых учётных записей пользователя, если он везде регистрируется под одним ником. Ставим Slash git clone https://github.com/theahmadov/slash cd slash pip install -r requirements.txt python slash.py help Я проверил Slash на себе, и посмотрите на эти результаты. Некоторые из учетных записей, перечисленных здесь, были созданы много лет назад. Slash — это простой консольный инструмент. Но вы также можете использовать такие инструменты, как WhatsMyName Web, который совершенно бесплатен. Вот, посмотрите. Я проверил WhatsMyName на себе. Мой ник «earthtoyash». Теперь, когда мы знаем больше о нашем пользователе, можно использовать эти знания. Например, через отправку фишинговых ссылок. Для этого создадим полезную нагрузку с помощью Zphisher. Ставим Zphisher с GitHub Клонируем репозиторий: git clone --depth=1 https://github.com/htr-tech/zphisher.git Запускаем файл zphisher.sh: cd zphisher && ./zphisher.sh При первом запуске он установит зависимости и на этом всё. Система скажет, что Zphisher установлен. После установки вам нужно будет снова запустить zphisher.sh в каталоге zphisher командой ./zphisher , и тогда вы получите что-то вроде этого: Как вы можете видеть, есть много вариантов и шаблонов, благодаря чему любой может заниматься фишингом. Мы сосредоточимся на нельзяграме. Итак, введите «2» и нажмите Enter. Следующий шаг полностью зависит от вас, выберите любой из них. Затем появится окно с выбором. Я выбрал третий вариант, так как он минималистичный и удобен для того, чтобы показать возможности инструмента. Опять же, чтобы все было просто, я пропущу пользовательский порт, но если вы уже используете порт 8080, то можете изменить его на 8000. Если нет, оставляйте всё как есть. Также важно маскировать URL, ну просто в целях безопасности. Можно использовать что‑то вроде этого: Всё, Zphisher создал фишинговую ссылку, которую можно отправить жертве. Как только она нажмёт на ссылку, вы начнёте получать информацию о ней. Например, IP‑адреса, имена пользователей, пароли и т. д. Ещё можно использовать обратный поиск IP, чтобы определить местоположение вашей цели и многое другое. Итак, вот эти фишинговые ссылки. При нажатии открывается страница, похожая на официальную страницу входа в запрещённую соцсеть. Вот она, нехорошая После ввода учётных данных можно получить много информации на «хакерской» стороне терминала. Вот так, господа и дамы, можно без особого труда взломать учетную запись в нельзяграме. Поэтому в очередной раз напоминаем: нельзя нажимать на ссылки, которым вы не доверяете. Само собой разумеется, не используйте информацию из этой статьи с намерением причинить кому-либо вред. OSINT законен, но фишинг и кража личных данных даже в запрещённой соцсети является уголовным преступлением. И да, * Организация Meta, а также её продукт Instagram, на которые мы ссылаемся в этой статье, признаны экстремистскими и запрещены на территории РФ. Спасибо за внимание! Cloud4Y #1 Корпоративный облачный провайдер Сайт Facebook Twitter ВКонтакте Telegram Что ещё интересного есть в блоге Cloud4Y → Информационная безопасность и глупость: необычные примеры → NAS за шапку сухарей → Взлом Hyundai Tucson, часть 1, часть 2 → Столетний язык программирования — какой он → 50 самых интересных клавиатур из частной коллекции",Cloud4Y
Realtime тестирование UserGate DCFW,UserGate,67.84,,2025-03-13,"Привет, Хабр! Меня зовут Кирилл Прямов, я менеджер по развитию NGFW в UserGate. Не так давно мы анонсировали новый NGFW для крупных компаний и операторов дата- центров, который мы назвали UserGate Data Center Firewall (UserGate DCFW). Сегодня я хочу рассказать про то, как мы тестировали этот продукт на новой аппаратной платформе F8010, официальные продажи которой начнутся к концу марта 2025. А в следующий раз я расскажу про тестирование нового гибридного устройства G9300 из новой серии G, которую мы выведем на рынок к лету 2025. Изображение: Patrick Turner (Unsplash License) Что вообще такое DCFW UserGate DCFW — это специализированный NGFW, заточенный на работу в дата-центрах и масштабных корпоративных сетях. Одна из его главных отличительных черт по сравнению с классическим UserGate NGFW — применение нашей новой технологии векторного файрвола. Это позволяет UserGate DCFW обрабатывать до 130 тыс. правил межсетевого экрана, что является рекордом для российских разработок класса NGFW (как это выглядит на практике, мы посмотрим далее во время тестов). Если говорить про «железную» составляющую, то UserGate DCFW может использовать как наши традиционные старшие аппаратные платформы F8000 форм-фактора 2U, которые обычно используются для защиты ЦОД и штаб-квартир, так и платформы нового поколения — E1010, E3010, F8010 и FG. Все перечисленные платформы находятся в 2-х реестрах Минпромторга: ПП 719 (реестр российской продукции) и ПП 878 (реестр российской радиоэлектронной продукции). На аппаратной платформе UserGate FG стоит остановиться отдельно, так как в ней впервые для российских NGFW используется аппаратный ускоритель на базе FPGA (он же ПЛИС, он же вентильная матрица). Традиционный процессор отвечает за идентификацию потоков и генерацию флоу-правил, а ускоритель реализует функции безопасности. Такой подход обеспечивает высокую производительность и позволяет 13.03.2025, 17:00 Realtime тестирование UserGate DCFW / Хабр https://habr.com/ru/companies/usergate/articles/889462/ 2/23 обходить ограничения, присущие решениям на базе универсальных процессоров, например, невозможность обрабатывать elephant flows (например, сессия для репликации базы данных или большого бэкапа в ЦОД). В настоящее время на FPGA доступная работа функций FW L3/L4 (150 Гбит/с на UDP 1518 байт и 90 Гбит/с EMIX. Про состав EMIX расскажу чуть позже. К началу апреля платформа FG также начнет официально поддерживать обработку функций IPS. До конца 2025 года мы также надеемся перенести на сопроцессор обработку функций контроля приложений (FW L7), а также планируем в 2 раза ускорить работу FW L3/L4. В более отдаленной перспективе — реализуем на FPGA обработку функций VPN и контентной фильтрации. Еще одна особенность платформы FG — для нее возможны два варианта внедрения. В первом случае платформа работает автономно (мы назвали его режим standalone). На сегодня доступен кластер Active-Passive, работаем над реализацией кластера Active- Active. Схемы кластера платформ UserGate FG в режиме standalone (слева) и в режиме гибридного устройства (серия G, справа) Второй режим является уникальным для российского рынка и предназначен для задач, требующих обработки большого числа транзакций и еще более высоких скоростей. Так как UserGate DCFW поддерживает явное разделение уровня управления (control plane) и уровня обработки данных (data plane), мы можем объединять несколько платформ в гибридное устройство. В данном случае платформа E1010, E3010 или F8010 берет на себя функции control plane, а более производительная FG — функции data plane. Для удобства мы решили выделить такие гибридные устройства в отдельную серию G. Соединение между плоскостью данных и плоскостью управления осуществляется через стандартные 13.03.2025, 17:00 Realtime тестирование UserGate DCFW / Хабр https://habr.com/ru/companies/usergate/articles/889462/ 3/23 10-гигабитные интерфейсы. При этом для администратора гибридное устройство выглядит и ведет себя как единое целое — веб-интерфейс для двух платформ один. Сейчас можно объединять в гибридное устройство серии G две платформы, но в перспективе мы хотим объединять 4-5 платформ для получения рекордных для российского рынка скоростей обработки трафика. Обзор тестового стенда Для начала хочу сказать большое спасибо своему коллеге Павлу Никулину — главному инженеру службы внедрения UserGate. Именно он проводил тесты, которые я опишу далее. Сперва в двух словах расскажу, как мы проводим тестирование для заказчиков. Когда к нам обращаются с просьбой провести нагрузочное тестирование, первым делом мы согласуем параметры теста: оборудование, конфигурацию и дополнительные условия (если такие имеются). В этом плане мы опираемся на базовые настройки конфигурации, которые предоставляет заказчик, и на профиль трафика. Количество соединений в секунду и правил межсетевого экрана, а также набор сетевых протоколов в трафике зависят от профиля заказчика. Например, у банков обычно выше требования к количеству одновременных соединений, новых соединений в секунду и правил межсетевого экрана. В целом выбор тестов зависит только от желания заказчика и возможности их технической реализации: в каждом конкретном случае заказчик знает, что ему необходимо, и тестирует решения в соответствии с требуемыми параметрами. По нашей статистике, 90% всех тестирований происходят в режиме онлайн, 10% — в офлайн-формате в одном из наших офисов. По времени согласование всех параметров занимает от одного дня до нескольких недель. Само тестирование – обычно 1,5-2 часа. В редких случаях, когда нужно реализовать серьезное ПМИ от крупного заказчика, — до четырех-пяти часов. Бывают случаи, когда заказчики просят проводить тестирование в течение суток, чтобы убедиться в способности NGFW стабильно работать длительное временя. Теперь про сам тестовый стенд. Мы применяем два генератора трафика: Spirent Cyberflood и Keysight BreakingPoint (бывшая Ixia), которые используют ведущие мировые вендоры сетевого оборудования. Это выгодно отличает нас от многих российских разработчиков NGFW, вынужденных использовать бесплатные генераторы трафика Trex или Яндекс.Танк, имеющие ряд ограничений. Клиентская часть генератора использует свой набор подсетей для генерации трафика и передает его серверной части. Оба компонента имеют интерфейсы, подключенные к коммутатору. Он, в свою очередь, соединен с испытуемым устройством. 13.03.2025, 17:00 Realtime тестирование UserGate DCFW / Хабр https://habr.com/ru/companies/usergate/articles/889462/ 4/23 Для генератора Spirent Cyberflood мы применяем встроенный профиль EMIX: Параметры EMIX в настройках интерфейса генератора трафика Spirent Cyberflood А вот пример одного из профилей трафика EMIX в Keysight BreakingPoint. Можно увидеть набор протоколов и веса, при помощи которых мы можем регулировать тестовый трафик. Байты в конце — это полезная нагрузка для каждого протокола в отдельности. Параметры EMIX в настройках интерфейса генератора трафика Keysight BreakingPoint Тесты платформы UserGate F8010 UserGate F8010 — наша новая флагманская платформа в форм-факторе 2U. В ней используются два процессора AMD EPYC с 48 ядрами каждый и 256 ГБ RAM DDR4. Для сравнения, ее предшественница F8000 оперирует двумя Intel Xeon E5-2697V4 по 18 ядер и 13.03.2025, 17:00 Realtime тестирование UserGate DCFW / Хабр https://habr.com/ru/companies/usergate/articles/889462/ 5/23 128 ГБ RAM DDR4. По умолчанию включена технология виртуального разделения физических ядер процессора на два, то есть используется 192 виртуальных потока. В стандартной поставке платформы F8010 доступно 8 интерфейсов RJ45 1 GbE и 4 оптических интерфейса SPF+ 10 GbE, а также 6 слотов для дополнительных сетевых карт (NIC), вплоть до QSFP28 на 100 GbE. Проведем четыре теста, используя 131 тыс. правил файрвола (позднее мы решили зарезервировать 1 000 правил под служебные задачи, но тест провели до этого события, поэтому теперь говорим про 130 тысяч правил). CPS-тест, HTTP, 1 Байт (BreakingPoint) Этот тест позволяет оценить максимально возможное количество соединений в секунду. Под соединением будем понимать процесс трехэтапного согласования TCP (SYN, SYN- ACK и ACK), за которым следуют запросы HTTP GET и HTTP Response с последующим закрытием TCP-подключения. Для начала откроем интерфейс BreakingPoint и отредактируем профиль трафика. Для этого нажмем на кнопку MULTI EDIT. Далее, кликнем на иконку увеличительного стекла, чтобы открыть окно с настройками: 13.03.2025, 17:00 Realtime тестирование UserGate DCFW / Хабр https://habr.com/ru/companies/usergate/articles/889462/ 6/23 Установим значение параметров Random response max (min) length в 1 байт (как в условиях теста): Возвращаемся на главный экран и запускам тест кнопкой Save and Run: 13.03.2025, 17:00 Realtime тестирование UserGate DCFW / Хабр https://habr.com/ru/companies/usergate/articles/889462/ 7/23 Трафик пошел — задача установить, сколько новых подключений за секунду мы «перевариваем» на устройстве. Нас интересует вторая графа — TCP Client Connection Rate. Сейчас она показывает, что почти все соединения (их 78 000) закрываются без запаздывания. На пропускную способность мы не смотрим, в этом тесте она нам не интересна. Главное, чтобы не было большого числа Closed (RST) в окне Cumulative Client TCP Connections, которое отражает накопленные подключения. Параллельно можем видеть на графике в дашборде UserGate DCFW, как возрастает нагрузка на процессор. Длительность CPS-теста — 4 минуты, этого достаточно в демонстрационных целях. Для CPS-тестов нет строгих стандартов по времени, их задача — обнаружить возможные 13.03.2025, 17:00 Realtime тестирование UserGate DCFW / Хабр https://habr.com/ru/companies/usergate/articles/889462/ 8/23 утечки памяти. Хотя иногда их проводят дольше — некоторые клиенты заказывают суточные проверки. Тест успешно завершился: при 131 000 правил у нас получилось 78 000 новых соединений в секунду. По умолчанию CPS-тест выполняется без логирования. Однако заказчики часто просят писать логи. Такой подход немного снижает производительность, но эффект, как правило, несущественен и зависит от объема трафика и количества событий. Если заглянуть в журнал трафика, то можно увидеть, что у нас были множественные срабатывания по правилу pass_131K. 13.03.2025, 17:00 Realtime тестирование UserGate DCFW / Хабр https://habr.com/ru/companies/usergate/articles/889462/ 9/23 EMIX-тест (Cyberflood) Второй нагрузочный тест с использованием разнообразных типов трафика. Используем генератор Cyberflood, который будет самостоятельно их подбирать. Все, что от нас требуется, — указать пропускную способность. Генерация трафика будет происходить в соответствии с тем самым стандартным профилем EMIX, который я упоминал ранее. Можно запускать тест — нажимаем кнопку Start. 13.03.2025, 17:00 Realtime тестирование UserGate DCFW / Хабр https://habr.com/ru/companies/usergate/articles/889462/ 10/23 Система загрузит конфигурацию на блейды тестового стенда, которых у нас двенадцать штук. Потом потребуется минута на «разгон», в это время Cyberflood будет плавно наращивать объем генерации микс-трафика. На дашборде UserGate DCFW видим, что степень использования CPU снова начала расти. 13.03.2025, 17:00 Realtime тестирование UserGate DCFW / Хабр https://habr.com/ru/companies/usergate/articles/889462/ 11/23 Во время теста красная полоска в окне Test Criteria Charts обозначает, что некоторые сессии были закрыты с опозданием. Но тест будет провален только в том случае, если количество таких ошибок превысит один процент. По окончании теста количество «дропов» можно просмотреть для каждого блейда по отдельности в столбце Unsuccessful Transactions. Текущие цифры с ошибками — это уровень погрешности. 13.03.2025, 17:00 Realtime тестирование UserGate DCFW / Хабр https://habr.com/ru/companies/usergate/articles/889462/ 12/23 HTTP-тест, 524К (BreakingPoint) В этом тесте вновь перейдем в интерфейс BreakingPoint. Будем использовать 12 блейдов с парой интерфейсов (клиент и сервер). Перед запуском генерации заглянем в профиль трафика. Значение поля Random response min length в настройках равно 524 288 байт — это объем полезной нагрузки. 13.03.2025, 17:00 Realtime тестирование UserGate DCFW / Хабр https://habr.com/ru/companies/usergate/articles/889462/ 13/23 Запустим тест — у него также есть разгонная часть длительностью чуть менее двух минут. Сразу после генерация выровняется. Здесь нас интересует пропускная способность устройства и сколько транзакций оно будет передавать. Так как у нас один протокол, то TCP Client Connection Rate будет равен количеству транзакций. Через две минуты вышли на стабильный уровень генерации трафика и смотрим на графики в дашборде. Загрузка устройства не превышает 60% — это нормальный рекомендуемый уровень. Нагружать устройство до 90% не имеет смысла, так как это нерабочий режим. 13.03.2025, 17:00 Realtime тестирование UserGate DCFW / Хабр https://habr.com/ru/companies/usergate/articles/889462/ 14/23 Серый график, обозначенный как vCPU, отражает количество очередей на ядре процессора. Синий график — использование процессора — это усредненная утилизация процессорного времени. Если график vCPU находится выше графика использования процессора, значит, CPU накапливает очередь перед их обработкой. Генерация прекратилась, можем посмотреть отчет. Для HTTP с полезной нагрузкой в 524К пропускная способность UserGate DCFW на платформе F8010 при включении 131 000 правил межсетевого экрана составила стабильные 60 Гбит/с. EMIX-тест + СОВ (Cyberflood) Проведем тест, аналогичный второму, но поменяем профиль, подключив систему обнаружения вторжений. У подавляющего большинства наших заказчиков 13.03.2025, 17:00 Realtime тестирование UserGate DCFW / Хабр https://habr.com/ru/companies/usergate/articles/889462/ 15/23 обрабатываемый трафик меньше или равен 20 Гбит/с — установим параметр объема трафика с учетом производительности тестируемой платформы. В открывшемся окне устанавливаем стандартный профиль СОВ: Default IDPS profile. Этот профиль содержит около 4 300 сигнатур 3–5 уровня угроз и подразумевает разрешение всех подключений. Что не критично, поскольку тест будет проводиться в условиях отсутствия атак на систему. Мы не будем использовать полный профиль, который содержит сигнатуры с высоким уровнем влияния на производительность, поскольку подавляющему большинству заказчиков не нужно защищаться от атак такого рода. 13.03.2025, 17:00 Realtime тестирование UserGate DCFW / Хабр https://habr.com/ru/companies/usergate/articles/889462/ 16/23 Хотя замечу, что мы проводим тесты с симуляцией атак под нагрузкой с основным профилем по запросу. В основном при помощи Keysight BreakingPoint, который имеет свою большую базу сетевых атак. В редких случаях симуляция проводится путем «проигрывания» PCAP-файлов (c заранее записанными сессиями атак) в Trex или Keysight BreakingPoint. Запускаем тест и видим, что у нас сильно возросла утилизация процессора, и она выше, чем vCPU. Это означает, что очереди не накапливаются, как на предыдущем тесте. Можно сказать, что UserGate DCFW без труда обрабатывает трафик на скорости 20 Гбит/с на том же количестве правил (131 000) с системой обнаружения вторжений c 4 300 сигнатурами. Спасибо за внимание! Я и наша команда тестирования будем рады ответить на ваши вопросы в комментариях. В следующий раз я расскажу про тесты гибридного устройства G9300 с аппаратным ускорением (платформа FG + платформа E3010) на аналогичных тестах.",Bitrider
Быстрое начало работы с Gitlab CICD,Southbridge,280.79,Обеспечиваем стабильную работу highload-проектов,2023-03-20,"Перевели статью о создании пайплайна для развертывания статического веб-сайта на AWS S3 Bucket на примере Gitlab CI/CD, чтобы быстро вникнуть в основы технологии и начать применять ее в работе. В статье рассматриваются базовые концепции CI и CD, а также этапы CI/CD-пайплайна. 5 минОт автора Мне повезло быть частью некоторых профессиональных команд, каждая из которых применяла несколько DevOps практик. И меня поразило то, как качество кода, скорость разработки и позитивный настрой команды коррелируют с CI/CD-пайплайном. По моему мнению, зрелость пайплайна может служить прекрасным показателем опытности разработчика, качества кода и эффективности всей команды. Во многих случаях, которые я наблюдал, пайплайны были выстроены либо DevOps-инженером, либо отдельной DevOps- командой. Да и последний отчет State of CD 2022 продемонстрировал, что только 22% разработчиков создают пайплайны. Моя цель — увеличить это число: помочь разработчикам взять на себя ответственность за пайплайны, выстраивать непрерывный процесс доставки и создавать качественный код. В статье рассматриваются фундаментальные концепции CI и CD. Что такое CI/CD? Многие бизнесы применяют фреймворки Agile, так как они позволяют менять приоритеты и повышать скорость доставки. Кроме всего прочего, такой подход улучшает атмосферу в команде и помогает увеличить прибыль. Если ваша компания следует по пути Agile, то принятие культуры, философии и практик DevOps станет ее большим преимуществом. Модное словечко последних десятилетий, DevOps сегодня считается настоящим стандартом индустрии. CI/CD — это практика DevOps, которая помогает разработчикам ПО доставлять изменения в коде с высокой частотой и надежностью. «Быстрый билд, быстрый тест, быстрый фейл» При наличии автоматизированных тестов команды тяготеют к общей автоматизации задач и частым, надежным поставкам кода. Создание CI/CD-пайплайна в этом случае может привести к нескольким преимуществам. Бизнес выигрывает от снижения затрат и повышения производительности, ускорения Time to Market и адаптации к изменяющимся требованиям рынка. Команда выигрывает от быстрой обратной связи, улучшения эффективности разработки, уменьшения количества бутылочных горлышек и повышения уровня вовлеченности и удовлетворенности сотрудников. Фазы CI и CD CI — непрерывная интеграция. Непрерывная интеграция позволяет по много раз в день коммитить изменения в основную ветку вашей кодовой базы. Учитывая ограниченные когнитивные способности человека, CI стимулирует разработчиков вносить в код небольшие изменения, которые легче рассмотреть, покрыть автоматическими тестами и часто релизить. Это позволяет избежать напряженных и переполненных merge conflict-ами дней подготовки к релизу с тоннами ручного тестирования. CD — непрерывная доставка. Следующий шаг после CI позволяет гарантировать, что кодовая база постоянно готова к деплою, а задеплоить ее можно одним нажатием кнопки. При этом неважно, с чем вы работаете: с масштабной распределенной системой, сложной производственной средой и т. д. Ключевой момент — автоматизация. CD — непрерывное развертывание. Последний этап зрелого CI/CD-пайплайна, где все изменения в коде автоматически развертываются в продакшн без ручного вмешательства. Само собой, для этого требуется большое количество хорошо продуманных автоматических тестов. State of CD 2022 утверждает, что «47% разработчиков применяют CI или СD, но только один из пяти использует оба подхода для автоматизации сборки, тестирования и развертывания кода». Книга Accelerate подводит итоги многолетнего исследования с использованием отчетов State of DevOps, основанных на 23 000 наборов данных компаний по всему миру. Как видите, высокопроизводительные команды могут деплоить по требованию (или несколько раз в день). Этапы CI/CD-пайплайна Стадия исходного кода — здесь запускается пайплайн. Обычно это происходит после изменений в Git-репозитории, которые проявляются в открытии нового Pull Request-а или в пуше в ветку. Другой способ заключается в настройке инструментария CI/CD для запуска пайплайна через автоматическое расписание или после запуска другого пайплайна. Стадия сборки — этап, в процессе которого происходит проверка и сборка кода. Здесь особенно полезны такие инструменты, как Docker: они обеспечивают однородную среду. Стадия тестирования — CI/CD невозможно представить без автоматизированных тестов. В конце концов, все хотят быть уверены, что изменения в коде не сломают продакшн. Стадия развертывания — на последнем этапе (после успешного прохождения всех предыдущих стадий) код можно развернуть в выбранной среде. Пример с Gitlab В этом примере будет использован Gitlab CI/CD, однако концепции аналогичны и для остальных инструментов, поэтому их можно применить к другим сервисам хостинга репозиториев. Существует несколько инструментов CI/CD, например всемирно известный Jenkins. Этот инструмент требует некоторой настройки и конфигурации, в то время как другие поставляются сервисами хостинга репозиториев (такими как GitHub Actions и Bitbucket Pipelines) с предварительной настройкой. Поэтому если ваш код размещен на Gitlab, то легче всего использовать Gitlab CI/CD, поскольку код и управление CI/CD находятся на одной платформе. Как все это может работать без настроек? Для ответа на этот вопрос стоит немного погрузиться в архитектуру Gitlab, а именно — в инстансы и раннеры. Инстансы хранят код приложения и конфигурации пайплайна. Раннеры выступают в качестве агентов, выполняющих операции в пайплайнах. В Gitlab каждый инстанс может быть подключен к одному или нескольким раннерам. Gitlab.com — это управляемый инстанс с несколькими раннерами, которые сам Gitlab и поддерживает. Следовательно, если вы используете этот инстанс, то получаете все необходимое из коробки. Приступим к работе Gitlab предлагает несколько шаблонов при создании нового проекта. Конфигурация пайплайна Gitlab CI/CD по умолчанию находится в файле .gitlab-ci.yml в корневом каталоге. Предположим, мы хотим создать простой пайплайн, который проверяет: написан, протестирован и развернут ли код. Вот несколько концепций и терминов для ознакомления перед началом работы. Пайплайн (Pipeline) Пайплайн — это набор заданий, разделенных на этапы. Gitlab предлагает различные типы пайплайнов, например parent-child или multi-project. Полный список см. здесь. Этап (Stage) Этап — это шаг в пайплайне, предоставляющий информацию о том, какие задания запускать (сборка, тестирование и т. д.). Один этап может включать одно или несколько заданий. Задание (Job) Задание — основной блок пайплайна (компиляция, линтинг и т. д.). Для каждого задания должны быть определены name и script. После выполнения всех заданий на этапе пайплайн переходит к следующему. Теперь — к коду Выстраиваем пайплайн Gitlab CI/CD, который собирает, тестирует и разворачивает статический веб-сайт в AWS S3 Bucket. Для начала создадим новый .gitlab-ci.yml 1. Определим переменные 2. Определим этапы 3. Определим задания на каждом этапе variables: # variabiles definitions for easier reuse of valu CI_NODE_IMAGE: ""node:16.13.2"" # Pipeline stages stages: - install - build - test - deploy #install job definition install: stage: install image: ""$CI_NODE_IMAGE"" # variable reference script: # Shell script that is executed by the runner. - npm ci cache: # List of files that should be cached between subseq key: files: - package.json - package-lock.json paths: # directories to cache - node_modules # Build Job definition На этом все, спасибо за внимание. Научиться работать с пайплайнами, билдами и артефактами можно на курсе Gitlab CI/CD в Слёрм. Вы узнаете, из чего состоит Gitlab и какие у него возможности и настройки, а также разберете лучшие практики построения пайплайна, особенности шаблонизации и работы с переменными. build: stage: build image: $CI_NODE_IMAGE script: - npm run build artifacts: # list of files and directories that are attache paths: - dist/ cache: key: files: - package.json - package-lock.json paths: - node_modules policy: pull # Test Job definition test: stage: test image: $CI_NODE_IMAGE script: - npm run test # Deploy Job definition deploy: stage: deploy image: registry.gitlab.com/gitlab-org/cloud-deploy/aws-base script: - aws s3 cp --recursive dist s3://bucket-name # copies th Southbridge Обеспечиваем стабильную работу highload-проектов Сайт Сайт 14 Карма 16.9 Рейтинг Лиза Зубарькова @zubarek Пользователь Комментировать Публикации Видеокурс доступен всегда. Посмотреть программу: https://slurm.club/3JUKdzT",zubarek
Генеральный директор Mozilla покинула свой пост,,145.1,,2024-02-08,"Митчелл Бейкер, гендиректор Mozilla с 2020 года, объявила, что покидает свой пост и возвращается на должность председателя совета директоров Mozilla Corporation, которую она занимала ранее. Временным генеральным директором компании станет член правления Лора Чемберс. «За 25 лет работы в Mozilla я побывала на многих должностях. Мой теперешний шаг вызван желанием сосредоточить внимание на предстоящих задачах. Я руководила бизнесом Mozilla в период преобразований, а также курировала миссию Mozilla в более широком смысле. Стало очевидно, что оба направления требуют преданного и постоянного руководства», — заявила Бейкер в сообщении в блоге Mozilla. Митчелл работает в Mozilla с эпохи Netscape Communications. Она основала Mozilla Foundation и учредила лицензию Mozilla Public License.141 Карма 145.1 Рейтинг @AnnieBronson Информационная служба Хабра Реклама Комментарии Здесь пока нет ни одного комментария, вы можете стать первым! Другие новости AnnieBronson 59 минут назад Лора Чемберс в должность генерального директора до конца этого года. Она входила в состав совета директоров Mozilla в течение трёх последних лет, а до этого работала в Airbnb, PayPal, eBay и Willow Innovations. По словам Бейкер, внимание Лоры « будет сосредоточено на предоставлении успешных продуктов, продвигающих миссию компании, и создании платформ, ускоряющих её развитие». «Мы находимся на критическом этапе, когда общественное доверие к институтам, правительствам и Интернету достигло беспрецедентно низкого уровня. Происходит тектонический сдвиг: каждый борется за будущее ИИ. Это возможность и необходимость для Mozilla создать лучшее будущее», — заключила Бейкер.",AnnieBronson
ИИ-агенты в Альфа-Банке нейросети создают автотесты без участия человека,Альфа-Банк,404.32,Лучший мобильный банк по версии Markswebb,2025-03-03,"В Альфа-Банке мы внедрили ИИ-агентов, которые проектируют, разрабатывают и проверяют автотесты. При этом полностью автономно, как QA-инженеры, но в разы быстрее и точнее. Подобных примеров, когда ИИ разрабатывает автотесты от анализа требований до пул-реквеста, в нашей стране, пожалуй, ещё не было. Что умеют агенты? Анализировать контекст из Jira и Confluence, вычленяя суть задачи. Прогнозировать риски, зависимости и даже «пограничные» сценарии. Генерировать DTO для REST API и превращать ручные сценарии в Java-тесты за минуты. Сверять код с бизнес-логикой и техстандартами Альфы, защищая прод от случайных ошибок. Создавать вариативные проверки — от позитивных кейсов до сложных негативных условий. Автоматизировать рутину. В ИИ-команде QA есть несколько агентов, каждый работает над своей частью из перечная выше. Сейчас решение пилотируется в нескольких продуктовых командах, но результаты уже видны:Альфа-Банк Лучший мобильный банк по версии Markswebb Сайт Хабр Карьера 14 Карма 0.3 Рейтинг @AlfaTeam Пользователь Сайт Facebook ВКонтакте Другие новости mefdayy 19 минут назад Nvidia и Broadcom тестируют передовой процесс производства чипов Intel 2 мин меньше ошибок в проде, предсказуемые дедлайны и высвобожденные ресурсы для творческих задач. «Одна команда ИИ-агентов экономит десятки часов работы, увеличивает скорость релизов и находит на 30% больше багов» Святослав Соловьев, Директор по генеративному ИИ в ИТ Альфа-Банка. В ближайшее время подробнее опишем как устроены агенты, какие технологии используем и как мы измеряем их эффективность.",AlfaTeam
"Инструменты наблюдаемости, о которых нужно знать в 2023 году",RUVDS.com,2376.9,VDS/VPS-хостинг. Скидка 15% по коду HABR15,2023-03-20,"Когда организации переходят в облако, их системы тоже начинают стремиться к распределённым архитектурам. Один из самых распространённых примеров этого — использование микросервисов. Однако это также создаёт новые сложности с точки зрения наблюдаемости. Необходимо подбирать подходящие инструменты для мониторинга, отслеживания и трассировки этих систем при помощи анализа выходных результатов посредством метрик, логов и трассировок. Это позволяет командам разработчиков быстро выявлять первопричины проблем, устранять их и оптимизировать производительность приложений, ускоряя выпуск кода. В этой статье мы рассмотрим возможности, ограничения и важные особенности одиннадцати популярных инструментов наблюдаемости, что позволит вам выбрать наиболее подходящий для вашего проекта. Helios Helios — это решение по обеспечению наблюдаемости для разработчиков, предоставляющее информацию по всему потоку приложений. Оно включает в себя фреймворк распространения контекста OpenTelemetry и обеспечивает наблюдение за микросервисами, serverless-функциями, базами данных и сторонними API. Можно протестировать песочницу продукта или использовать его бесплатно, зарегистрировавшись здесь. ▍ Основные возможности • Обеспечение полного контроля: Helios предоставляет информацию о распределённой трассировке в полном контексте, показывает, как передаются данные через всё приложение в любом окружении. • Визуализация: позволяет пользователям собирать и визуализировать данные трассировок из множественных источников данных, чтобы исследовать и устранять потенциальные проблемы. • Многоязыковая поддержка: поддерживает множество языков и фреймворков, в том числе Python, JavaScript, Node.js, Java, Ruby, .NET, Go, C++ и Collector. • Обмен и многократное использование: вы с лёгкостью можете сотрудничать с участниками команды, обмениваясь через Helios трассировками, тестами и триггерами. Кроме того, Helios позволяет многократно использовать запросы и полезные нагрузки между участниками команды. • Автоматическая генерация тестов: автоматически генерирует тесты на основании данных трассировок. • Простота интеграций: интегрируется в существующую экосистему, включая логи, тесты, мониторинг ошибок и многое другое. • Воссоздание процессов: Helios всего за несколько кликов позволяет в точности воссоздавать рабочие процессы, в том числе HTTP-запросы, сообщения Kafka и RabbitMQ, а также вызовы Lambda. ▍ Популярные способы использования • Распределённая трассировка • Интеграция трассировок в многоязыковое приложение • Наблюдаемость serverless-приложения • Устранение неполадок в тестах • Информация о вызовах API • Анализ и выявление узких мест Prometheus Prometheus — это опенсорсный инструмент, широко используемый для обеспечения наблюдаемости в нативных облачных окружениях. Он может собирать и хранить данные временных последовательностей и предоставляет инструменты визуализации для анализа собранных данных. ▍ Основные возможности • Сбор данных: он может скрейпить метрики из различных источников, в том числе из приложений, сервисов и систем. Также он «из коробки» поддерживает множество форматов данных, в том числе логи, трассировки и метрики. • Хранилище данных: он сохраняет собранные данные в базе данных временных последовательностей, позволяя эффективно запрашивать и агрегировать данные с течением времени. • Система алертов: инструмент включает в себя встроенную систему алертов, которая может запускать алерты на основании запросов. • Исследование сервисов: он может автоматически распознавать и скрейпить метрики сервисов, работающих в различных окружениях, например, Kubernetes и в других системах управления контейнерами. • Интеграция с Grafana: инструмент имеет гибкую интеграцию с Grafana, позволяющей создавать дэшборды для отображения и анализа метрик Prometheus. ▍ Ограничения • Ограниченные возможности анализа первопричин: инструмент в первую очередь предназначен для мониторинга и алертов. Поэтому он не предоставляет встроенных возможностей аналитики первопричин проблем. • Масштабирование: хотя инструмент может обрабатывать множество метрик, это может привести к большой трате ресурсов, поскольку Prometheus хранит все данные в памяти. • Моделирование данных: содержит модель данных на основе пар «ключ-значение» и не поддерживает вложенных полей и join. ▍ Популярные способы применения • Сбор и хранение метрик • Система алертов • Исследование сервисов Grafana Grafana — это опенсорсный инструмент, в первую очередь используемый для визуализации и мониторинга данных. Он позволяет с лёгкостью создавать интерактивные дэшборды для визуализации и анализа данных из различных источников. ▍ Основные возможности • Визуализация данных: создаёт настраиваемые и интерактивные дэшборды для визуализации метрик и логов из различных источников данных. • Система алертов: позволяет настраивать алерты на основании состония метрик для информирования о потенциальных проблемах. • Выявление аномалий: позволяет настроить выявление аномалий для автоматического определения и отправки алертов в случае аномального поведения в метриках. • Анализ первопричин: позволяет углубиться в метрики для анализа первопричин, предоставляя подробную информацию с историческим контекстом. ▍ Ограничения • Хранение данных: архитектура инструмента не поддерживает долговременное хранение и для сохранения метрик и логов требует дополнительных инструментов наподобие Prometheus или Elasticsearch. • Моделирование данных: Grafana не предоставляет расширенных возможностей моделирования данных. То есть она не позволяет моделировать конкретные типы данных и выполнять сложные запросы. • Агрегирование данных: Grafana не содержит встроенных функций агрегирования. ▍ Популярные способы применения • Визуализация метрик • Система алертов • Выявление аномалий Elasticsearch, Logstash и Kibana (ELK) Стек ELK — это популярное опенсорсное решение, помогающее управлять логами и анализировать данные. Оно состоит из трёх компонентов: Elasticsearch, Logstash и Kibana. Elasticsearch — это движок распределённого поиска и аналитики, способный обрабатывать большие объёмы структурированных и неструктурированных данных; он позволяет хранить и индексировать большие массивы данных, а также выполнять поиск по ним. Logstash — это конвейер сбора и обработки данных, позволяющий собирать, обрабатывать и обогащать данные из множества источников, например, файлов логов. Kibana — это инструмент визуализации и исследования данных, позволяющий создавать интерактивные дэшборды и визуализации на основе данных, находящихся в Elasticsearch. ▍ Основные возможности • Управление логами: ELK позволяет собирать, обрабатывать, хранить и анализировать данные логов и метрики из множества источников, предоставляя централизованную консоль для поиска по логам. • Поиск и анализ: позволяет выполнять поиск и анализ релевантных данных логов, что критически важно для выявления и устранения первопричин проблем. • Визуализация данных: Kibana позволяет создавать настраиваемые дэшборды, которые способны визуализировать данные логов и метрики из множества источников данных. • Выявление аномалий: Kibana позволяет создавать алерты для аномальной активности в данных логов. • Анализ первопричин: стек ELK позволяет глубоко изучать данные логов, чтобы лучше понять первопричины, предоставляя подробные логи и исторический контекст. ▍ Ограничения • Трассировка: ELK нативно не поддерживает распределённую трассировку. Поэтому может понадобиться применение дополнительных инструментов наподобие Jaeger. • Мониторинг в реальном времени: архитектура ELK позволяет ему качественно выполнять задачи платформы управления логами и анализа данных. Однако в отчётности логов присутствует незначительная задержка, и пользователям приходится ждать. • Сложная настройка и поддержка: для платформы требуется сложный процесс настройки и поддержки. Кроме того, для управления большими объёмами данных и множественными источниками данных требуются специфические знания. ▍ Популярные способы применения • Управление логами • Визуализация данных • Комплаенс и безопасность InfluxDB и Telegraf InfluxDB и Telegraf — это опенсорсные инструменты, популярные благодаря своим возможностям по хранению и мониторингу данных временных последовательностей. InfluxDB — это база данных временных последовательностей, хранящая большие объёмы данных временных последовательностей и выполняющая запросы к ним при помощи своего языка запросов, напоминающего SQL. Telegraf — это хорошо известный агент сбора данных, способный собирать и отправлять метрики широкому выбору получателей, например, InfluxDB. Также он поддерживает многие источники данных. ▍ Основные возможности Комбинация из InfluxDB и Telegraf предоставляет множество возможностей, повышающих наблюдаемость приложений. • Сбор и хранение метрик: Telegraf позволяет собирать метрики из множества источников и отправлять их в InfluxDB для хранения и анализа. • Визуализация данных: InfluxDB можно интегрировать со сторонними инструментами визуализации наподобие Grafana для создания интерактивных дэшбордов. • Масштабируемость: архитектура InfluxDB позволяет обрабатывать большие объёмы данных временных последовательностей и выполнять горизонтальное масштабирование. • Поддержка множества источников данных: Telegraf поддерживает более двухсот плагинов ввода для сбора метрик. ▍ Ограничения • Ограниченные возможности алертинга: в обоих инструментах отсутствуют возможности алертинга и для его подключения необходима интеграция сторонних сервисов. • Ограниченный анализ первопроичин: в этих инструментах отсутствуют нативные возможности анализа первопричин и необходима интеграция сторонних сервисов. ▍ Популярные способы применения • Сбор и хранение метрик • Мониторинг Datadog Datadog — это популярная облачная платформа для мониторинга и аналитики. Она широко используется для получения информации о здоровье и производительности распределённых систем с целью заблаговременного устранения проблем. ▍ Основные возможности • Многооблачная поддержка: пользователи могут выполнять мониторинг приложений, работающих на облачных платформах нескольких поставщиков, например, AWS, Azure, GCP и так далее. • Карты сервисов: позволяют выполнять визуализацию зависимостей сервисов, местоположений, сервисов и контейнеров. • Аналитика трассировок: пользователи могут анализировать трассировки, предоставляя подробную информацию о производительности приложений. • Анализ первопричин: позволяет глубоко изучать метрики и трассировки, чтобы понять первопричину проблем, предоставляя подробную информацию с историческим контекстом. • Выявление аномалий: может настраивать систему выявления аномалий, которая автоматически выявляет аномальное поведение в метриках и создаёт алерты о нём. ▍ Ограничения • Затраты: Datadog — это облачный платный сервис, стоимость которого увеличивается при развёртывании крупномасштабных систем. • Ограниченная поддержка потребления, хранения и индексации логов: Datadog по умолчанию не предоставляет поддержку анализа логов. Необходимо отдельно приобретать поддержку потребления и индексации логов. Поэтому большинство организаций принимает решение хранить ограниченное количество логов, что может вызвать неудобства при устранении проблем, поскольку отсутствует доступ к полной истории проблемы. • Нехватка контроля за хранением данных: Datadog хранит данные на собственных серверах и не позволяет пользователям хранить данные локально или в дата-центрах компании. ▍ Популярные способы применения • Конвейеры наблюдаемости • Распределённая трассировка • Мониторинг контейнеров New Relic New Relic — это облачная платформа мониторинга и аналитики, позволяющая выполнять мониторинг приложений и систем в распределённом окружении. Она использует сервис «New Relic Edge» для распределённой трассировки и способна выполнять наблюдение за 100% трассировок приложения. ▍ Основные возможности • Мониторинг производительности приложений: предоставляет комплексное решение APM для мониторинга производительности приложений и устранения проблем. • Многооблачная поддержка: поддерживает мониторинг приложений на облачных платформах нескольких поставщиков, например, AWS, Azure, GCP и так далее. • Аналитика трассировок: позволяет анализировать трассировки, предоставляя подробную информацию о производительности системы и приложений. • Анализ первопричин: позволяет глубоко изучать матрики и трассировки для анализа первопричин проблем. • Управление логами: собирает, обрабатывает и анализирует данные логов из различных источников, обеспечивая всеобъемлющую картину логов. ▍ Ограничения • Ограниченная опенсорсная интеграция: New Relic — это платформа с закрытыми исходниками, поэтому её интеграция с опенсорсными инструментами может быть ограниченной. • Затраты: New Relic может быть более дорогим по сравнению с другими решениями при работе с крупномасштабными системами. ▍ Популярные способы применения • Мониторинг производительности приложений • Многооблачный мониторинг • Аналитика трассировок AppDynamics AppDynamics — это платформа мониторинга и аналитики, позволяющая отслеживать и визуализировать каждый компонент приложения, а также управлять ими. Кроме того, она позволяет выполнять анализ первопричин для выявления внутренних проблем, которые могут влиять на производительность приложения. ▍ Основные возможности • Сбор данных: пользователи могут собирать метрики и трассировки из множества источников: хостов, контейнеров, облачных сервисов и приложений. • Выявление аномалий: позволяет настраивать систему выявления аномалий, которая выявляет аномальное поведение и сообщает о нём при помощи алертов. • Аналитика трассировок: пользователи могут анализировать трассировки и получать подробную информацию о производительности. Мониторинг производительности приложений: предоставляет комплексное решение APM, позволяющее выполнять мониторинг и устранение проблем производительности приложения. • Ограниченные возможности настройки: по сравнению с другими инструментами, опции настройки не очень гибки, потому что пользователи не могут настраивать решение самостоятельно. ▍ Популярные способы применения • Мониторинг производительности приложений • Многооблачный мониторинг • Управление бизнес-транзакциями Выбор лучшего инструмента наблюдаемости Наблюдаемость — неотъемлемая часть разработки и эксплуатации современного ПО. Она помогает компаниям выполнять мониторинг здоровья и производительности систем и быстро решать проблемы ещё до того, как они станут критичными. В этой статье мы рассказали об одиннадцати лучших инструментах наблюдаемости, о которых должны знать разработчики при работе с распределёнными системами. Как видите, каждый инструмент имеет свои сильные стороны и ограничения. Поэтому чтобы найти подходящий для вас инструмент, следует сравнить его с требованиями вашей системы. Выбор наилучшего инструмента наблюдаемости для вашей организации зависит от конкретных потребностей, таких, как окружение, технологический стек, опыт разработчиков, профили пользователей, требования к мониторингу и устранению проблем, а также рабочие процессы. Надеюсь, эта статья была для вас полезной. Telegram-канал с розыгрышами призов, новостями IT и постами о ретроиграх ️ RUVDS.com VDS/VPS-хостинг. Скидка 15% по коду HABR15 Telegram ВКонтакте Twitter 327 Карма 576 Рейтинг @ru_vds Пользователь Комментарии 3 Публикации",ru_vds
История российской науки напишем вместе ,Хабр,353.18,"Экосистема для развития людей, вовлеченных в IT",2024-02-08,"8 февраля — день российской науки. Этот праздник отмечается в Академии Наук, в НИИ, вузах, исследовательских лабораториях по всей стране. Судьба российской науки непростая, всегда переплетённая с историей страны и ею же обусловленная: непростой путь к открытию МГУ, талантливые кулибины (и И.Кулибин) из глубинки, учёные в изгнании, Туполевская шарага, достижения института Гамалеи… Всё смешано, переплетено и влияло и влияет на весь мир. Величие российской науки прежде всего в людях, которые её создают, которые разрабатывают, изобретают, экспериментирую и точно знают, что делают. В этот день всегда спорят медики, биологи, инженеры, программисты, филологи, физики, химики — чьи учёные научнее и главнее, кто определял и создавал будущее? Ответ простой: все, каждый — в своей сфере. Предлагаем сегодня собрать российские открытия и учёных, которые вас вдохновляют, впечатляют, будоражат воображение и влияют (или повлияли) на вас. Напишем эту статью вместе?Правила создания статьи Вы в комментариях рассказываете о том самом учёном и его изобретении, которые важны именно для вас, а мы добавляем ваши истории в общую статью и формируем самый крутой лонгрид об истории российской науки. Формат свободный, желательно (не обязательно!) отразить: ФИО учёного, его годы жизни основные изобретения краткая биография наиболее значимое изобретение (на ваш взгляд) чем вас вдохновил учёный, как повлиял на вас изображения приветствуются. Комментарии будут сохраняться и вы сможете их обсуждать — так, как и всегда. Автор самого заплюсованного описания получит приятный сюрприз от команды Хабра. Чур, википедию не копировать! Итак, поехали. Николай Иванович Пирогов (1810 — 1881) великий российский врач, хирург Николай Пирогов в 14 лет поступил на первый курс медицинского факультета МГУ (тогда Московского университета). По окончании вуза он в числе семи лучших студентов отправился в Депт (Тарту) заниматься анатомическими исследованиями и готовиться к профессорской деятельности. Считается одним из лучших анатомов мира, родоначальник топографической анатомии. Именно он разрезал замороженное человеческое тело в четырёх плоскостях и создал атлас топографической анатомии, который стал незаменимым пособием для хирургов. Это был не просто учебный материал, но прежде всего возможность оперировать с минимальным травматизмом для пациента. Пирогов брался за любые операции, от ампутаций и извлечения камней до глазных манипуляций и сосудистых вмешательств. Именно он провёл в 1847 году первую свою операцию под наркозом, тщательно изучив свойства эфира и хлороформа. Затем в течение года прошло 300 подобных операций. Николай Пирогов — автор алгоритма сортировки раненных на поле брани, благодаря чему удалось спасти огромное количество жизней, а сам алгоритм до сих пор служит организации военной медицины. Именно он ввёл в практику российской медицины обеззараживание хлорной водой и настойкой йода. До сих пор хлорка, йодопирон, йодинол и йод служат хирургам и всем медикам верную службу. Николай Пирогов был первым учёным от медицины, о котором я узнала в возрасте 7 или 8 лет, уже будучи немного горящей «стать доктором». Жизнь повернулась самым причудливым образом, но изучение истории медицины сделало моё мышление структурным, комплексным и навсегда привило любовь к людям. Какие бы они ни были, они — люди, с болью, со стрессом, с заболеваниями и радостью выздоровления. Совершенно особенное ощущение. Художественный фильм «Пирогов» Гааз и Пирогов — лекторий «Достоевский». Хотя конкретно в этом фильме история Гааза потрясает гораздо больше. Семён Николаевич Корсаков (1787-1853) создатель первой в мире перфокарточной машины (табулятора) «для сравнения идей» Рассказывает @Motanto Его история меня когда-то поразила: только подумайте, кто-то что-то делал в программировании до Бэббиджа, да ещё и в Российской Империи. Ещё больше поразил тот факт, что Корсаков был обвинён современниками в пустой трате времени на изобретения, а его славные дела дошли до мирового научного сообщества и до нас с вами благодаря ""раскопкам"" учёных XX века, в частности благодаря математику и кибернетику Г.Н. Поварову. Потомственный дворянин, Семён Корсаков стал прародителем отечественной кибернетики. Он всегда видел своей задачей усиление работы разума с помочью различных механических устройств (чем вам не первобытное программирование?!). Кроме изобретённых им гомеоскопов и идеоскопа, он подарил миру идею перфокарт. Именно гомеоскопические перфорированные таблицы стали прототипом тех самых картонных карт. Устройства Корсакова помогали сравнивать таблицы и обрабатывать большие массивы данных, с помощью его устройств можно было решать задачи классификации. Корсаков видел бытовое применение своим изобретениям (реестры, каталоги, регистрации), ничего не патентовал и просто представил машины общественности. С.Н.Корсаков и машина для сравнения идей Однако комиссия из пяти ученых, возглавляемая молодым математиком академиком М. В. Остроградским, вынесла вердикт: «Господин Корсаков потратил чересчур много своих интеллектуальных сил, пытаясь научить других, как вовсе без оного интеллекта прожить». Кажется, это лучшее определение ИТ-сферы целиком ;-) Такая вот биг дата доэлектрической эры. Павел Петрович Аносов (10 июля 1796 — 25 мая 1851) горный инженер, учёный-металлург Рассказывает @Mimizavr Талантами Русь богата И вот, старики говорят, Что сталь дамасских булатов Рассек уральский булат. Павел Петрович Аносов - на мой взгляд, совершенно фантастическая фигура, благодаря которой в 19 веке очень сильно развилась металлургия на Урале. Будучи горным инженером по специальности, он занимался научной работой в сфере металлургии, стал крупным организатором горнозаводской промышленности на Урале, исследовал природу Южного Урала, вел геологическую разведку месторождений россыпного золота и железных руд, изобрел ряд устройств и механизмов (в том числе и эффективную золотопромывную машину), разработал технологию производства огнеупорных тиглей (первый в России), стал десятым губернатором Томской губернии. Павел Петрович известен, в первую очередь, как создатель русского булата. Булат — особый вид стали, очень прочный и упругий, с узором на поверхности, который является не украшением, а показателем качества металла. Впервые оружие из такой стали начали использовать воины Древней Индии. Такими мечами разрубали камни и легкие воздушные ткани. Несколько веков спустя центром по изготовлению булатных клинков стал Дамаск. Как мастера 12-15 веков могли варить такую удивительную сталь, оставалось загадкой, разгадать которую сумел лишь Павел Аносов. До Павла Аносова создать булат пытались многие, но никому это не удавалось. За несколько лет работы мастер испробовал 189 способов сталеварения, но ни один из них не не дал булат. И лишь в 190-ый раз смешивая компоненты, создавая необходимые температурные условия мастер открыл тайну. Игорь Васильевич Курчатов (1903–1960) физик, «отец» атомной бомбы СССР Говорит и показывает @Boomburum Игорь Васильевич Курчатов (1903–1960) — один из первопроходцев в области ядерной физики, «отец» атомной бомбы СССР. В числе его заслуг — создание первого в Европе атомного реактора, первой в СССР атомной бомбы и первой в мире термоядерной бомбы. Но были заслуги и в развитии мирного атома — под его руководством была сооружена первая в мире атомная электростанция (Обнинская АЭС). Самый засекреченный ученый Самый засекреченный ученый",Exosphere
Как системному аналитику написать хорошее резюме — 11 рекомендаций,,0,,2023-03-22,"look, use the source! 1. Указывайте количественно и качественно выраженные достижения Это самый главный и мощный пункт. Большинство людей пишут какие-то беспомощные аморфные функции и фразы про обязанности и участие — «состоял, привлекался, принимал участие». Это выглядит, как свидетель из Фрязино, а не мощный проектный специалист, который будет двигать проект вперёд.Нанимающий руководитель смотрит прежде всего на результаты, а не на процесс. Если вы пишете только про поток, это в глазах читающего создаёт риски того, что вы цените процесс, а не результаты. (Процесс тоже важен, но про него отдельно). Освойте язык результатов, важных для команды, бизнеса нанимателя, бизнеса клиента. Как обычно пишут Функции и задачи: разрабатывал требования, общался с клиентами, командой, отвечал на вопросы, рисовал схемы… Как надо Достижения: разработал требования для 5 проектов в области X, Y, Z; помог клиентам найти более выгодное решение; сократил время на создание пакета требований в 2 раза относительно планового; сократил количество обсуждений по требованиям в разработке в 3 раза; уменьшил и удерживал количество циклов согласования с 4-х до 2-х; внедрил в команде практику использования таких-то диаграмм, что сократило длительность переписок и встреч; выстроил доверительные отношения с ключевыми клиентами. 2. Не допускайте ошибок Будьте предельно внимательны ко всем словам, типографике, аббревиатурам. Кадровикам-рекрутёрам будет пофиг, но вот ваши будущие коллеги будут очень пристально смотреть на ваш текст и всё, что будет резать глаз. Если вы допускаете опечатки, ошибки в документе, который представляет вас всему рынку труда, то в рабочих документах будете тем более. Никто не хочет бесплатно работать корректором. Отдельный вопрос — логика построения предложений. В резюме для этого мало места, но и тут некоторые умудряются нагородить странный порядок слов. Помните как мантру — «Мама мыла раму», Субъект-Действие-Объект. Никто не хочет бесплатно работать редактором. Писать понятные тексты — это одна из важнейших компетенций аналитика. 3. Только актуальный и релевантный опыт Не указывайте места работы старше 8 лет, если только не хотите зачем-то подчеркнуть разносторонность своего опыта — например, почему-то считаете, что то, что вы работали таксистом полезно для конкретных вакансий, которые вы ищете. 4. Указывайте в местах работы основную трудовую функцию, а не должность по трудовой Запись в трудовой никого не интересует. Слова «старший специалист отдела ВРИОКОМУНО» никому ни о чём не говорят, ну разве что вы точно были не младшим. Пишите суть деятельности — «аналитик», «проектировщик», «координатор», «конструктор». Лучше, если там будет фигурировать как минимум слово «инженер». 5. Указывайте область деятельности компании / подразделения Название компании тоже зачастую ничего не говорит читателю. Сразу после названия позиции первой строчкой укажите, чем занимается компания — «разработка рудных месторождений», «направление занимается кредитами для малого бизнеса» и т.д. 6. Указывайте объём управления Иногда бывает, что человек работал ведущим аналитиком или руководителем группы и не указывает, сколькими людьми управлял. Поверьте, между 2-мя людьми в подчинении и 7 есть разница, по крайней мере, при прочих равных. 7. Подкрепляйте софт-скиллы фактами Лучше не писать «обучаемость», «креативность», «коммуникативность», это всем набило оскомину и ничего не значит, кроме вашего желания, чтобы вас так воспринимали. Пишите с примерами: Высокая обучаемость: за 3 месяца досрочно освоила Enterprise Architect. Креативность: придумала несколько концепций решений, которые мы с командой смогли продать клиенту и они дали ему существенный эффект в бизнесе. 8. Указывайте, если готовы предоставить образцы документов по запросу Документы не обязательно должны быть рабочими, это могут быть и документы, созданные вами на учебном курсе (Но такая фраза сразу приятно выделит вас на фоне других кандидатов). А сейчас так и вообще, пример портфолио можно нагенерить и оформить за пару-тройку часов! 9. Не забывайте про ключевые слова Если у вас в резюме нет слов SQL, XML, UML, JSON, API, то совершенно неудивительно, что вас невозможно найти (до трудно забыть дело даже не дойдет). 10. Покажите масштаб личности Вполную вы сможете развернуться на встрече, но даже на уровне резюме люди хотят работать с интересными людьми, а не роботами, которые выполняют трудовые функции по договору. Дайте представление читателю о вашем жизненном кругозоре и интересах, какими вещами вы увлекаетесь. Причём желательно не просто «философия, психология», а более предметно «изучаю аналитическую философию и гештальт-психологию». Одно ключевое слово здесь может заставить нанимающего менеджера позвать вас на интервью. 11. Создавайте копии резюме с профильным позиционированием Это совершенно бесплатно. Если вы знаете, работа в каких сферах/темах проектов вам более интересна, создайте копии резюме с соответствующими названиями: Системный аналитик в интеграции; Системный аналитик IoT; и т.д. Если хотите получить более предметную помощь по своему резюме и поиску работы — идите к менторам.",Systems_Education
Как создать аппаратный эмулятор CD-ROM без паяльника,RUVDS.com,2394.92,VDS/VPS-хостинг. Скидка 15% по коду HABR15,2023-03-22,"Несмотря на то, что постепенно оптические диски уходят в прошлое, использование ISO- образов этих дисков остаётся актуальным. Многие операционные системы поставляются в виде ISO-образов, а администраторам необходимо поддерживать разношёрстный парк старых персональных компьютеров.Существует множество решений, как можно установить операционную систему с ISO- образа без записи его на оптический носитель. Я уже затрагивал тему ISO-образов в моих статьях: «Раскрываем секреты загрузочных ISO-образов» и «Что вам нужно знать о внешних загрузочных дисках». В этой статье я хочу рассказать о ещё одном способе, который, как оказывается, вшит в ядро Linux. Если ваш одноплатный компьютер имеет USB OTG-разъём, и на него возможна установка Linux, то вы c большой долей вероятности можете сделать из одноплатника аппаратный эмулятор привода оптических дисков. Меня этот способ заинтересовал. Я проверил его сам и, получив положительный результат у себя, решил поделиться с вами. Я сам узнал много интересного, систематизировал свои знания, поэтому надеюсь, что чтение будет познавательно и интересно для вас. Как всегда, если вы хотите посмотреть, что получится в итоге, уточнить детали, вы всегда можете найти исходный код в моём репозитории на GitHub. При написании статьи я поставил себе следующие цели: 1. Аппаратный эмулятор CD-ROM должен быть реализован без использования паяльника и макетных плат. 2. Реализация должна быть понятна человеку, имеющему лишь базовые представления о Linux, USB и Bluetooth. 3. Решение должно быть таким, чтобы его можно было с небольшими изменениями реализовать на различных одноплатных компьютерах. 4. Побудить интерес читателя к изучению используемых в статье технологий. 5. Изложить материал, необходимый для решения задачи лаконично и просто. Не уверен, что у меня это получилось из-за большого объёма темы. Буду признателен, если вы в отзывах напишете своё мнение. Оглавление Суть решения Проверка решения на практике От проверки идеи до реализации Операционная система Linux USB Bluetooth Сборка и модификация дистрибутива Raspberry Pi OS Реализация Как пользоваться эмулятором Особенности моего эмулятора Выводы Суть решения Решение заключается в том, что, модифицируя операционную систему Linux на одноплатном компьютере (встраиваемой системе), можно получить из него устройство, которое будет распознаваться компьютером как внешний оптический привод USB или флеш-накопитель. В ядро Linux включена поддержка эмуляции CD-ROM и эмуляции флеш-накопителя. Но это не значит, что любую встраиваемую систему можно превратить в них. Для этого ещё необходимо, чтобы встраиваемая система имела USB OTG-контроллер или USB- контроллер периферийного устройства. Проверка решения на практике Я делал эмулятор оптических дисков, используя Raspberry Pi Zero 2 W. Но вы можете использовать и другие одноплатные компьютеры. Естественно, вам тогда придётся самим разбираться с некоторыми проблемами, которые с большой долей вероятностью у вас возникнут. У меня других одноплатных компьютеров кроме Raspberry Pi не было, поэтому привожу алгоритм, как делал я. 1. Скачать образ Raspberry OS Light с сайта raspberrypi.org. 2. Записать образ на SD-карту. Я использовал программу balenaEtcher. 3. Добавить строку dtoverlay=dwc2 в файле config.txt на SD-карте. 4. Записать файлы ssh и wpa_supplicant.conf на SD-карту. Файл userconf.txt нужен, чтобы установить пароль для пользователя, ssh — чтобы включить SSH, wpa_supplicant.conf — чтобы указать точку доступа и пароль для Wi-Fi. 5. Вставить SD-карту в Raspberry Pi Zero 2 W. 6. Подключить USB-разъём Raspberry Pi Zero 2 W к USB-разъёму компьютера. 7. Подождать, пока выполнится первая загрузка и Raspberry Pi Zero 2 W подключится к Wi-Fi-сети. 8. Найти IP-адрес Raspberry Pi Zero 2 W в Wi-Fi-сети и подключиться к нему по протоколу SFTP. Я использовал приложение WinSCP. 9. Записать ISO-образы, которые вы хотите эмулировать, в файловую систему Raspberry Pi. 10. Подключиться через SSH к Raspberry Pi W 2. Это можно сделать при помощи приложения PuTTY. 11. Для того, чтобы ваш Raspberry Pi Zero 2 W стал внешним USB CD-ROM, ввести команду: $ sudo modprobe g_mass_storage cdrom=y removable=y stall=n file=/full/filename/w После чего у вас на компьютере распознается внешний USB CD-ROM, в который вставлен диск. 12. Для прекращения эмуляции ввести команду: $ sudo modprobe -r g_mass_storage Приведённый файл userconf.txt устанавливает для пользователя pi пароль «raspberry». Файл ssh — это пустой файл, который не содержит никаких данных. От проверки идеи до реализации Приведённая выше последовательность шагов позволяет вам посмотреть работу эмуляции в действии. Однако это решение обладает рядом недостатков. 1. Чтобы загрузить и выбрать образ для эмуляции, необходимо наличие Wi-Fi. Файл userconf.txt Файл wpa_supplicant.conf 2. При перезагрузке Raspberry Pi необходимо заново монтировать образ. 3. Если нужно эмулировать образ для загрузки с него операционной системы, понадобится ещё один компьютер для управления Raspberry Pi. 4. Эмулировать можно образ размером максимум 2 Gib. Если вам интересно, как избавиться от этих недостатков, у вас есть время и интерес разобраться в этом вопросе, то предлагаю продолжить чтение. Краткое содержание моей реализации следующее: 1. Для общения с Raspberry Pi будет использоваться Bluetooth. 2. Чтобы работа устройства была возможна в автономном режиме (без подключений Wi- Fi и Bluetooth), управляющий скрипт оформляется в виде службы Systemd. 3. Для управления по Bluetooth будет использоваться приложение Serial Bluetooth Terminal c Google Play. 4. Для эмуляции оптических дисков с образов размером больше 2Gib необходимо внести небольшие изменения в модуль ядра Linux и выполнить перекомпиляцию. Приведу кратко, что вам нужно знать, чтобы лучше понять суть того, что мы будем делать. Операционная система Linux ▍ Ядро Linux Ядро Linux содержит в себе абстракции для работы с устройствами там, где оно запускается. Реализуются эти абстракции в специальных программах, называемых драйверами. В ОС Linux драйвер может находиться непосредственно в файле ядра, а может быть оформлен в виде отдельного модуля. В большинстве случаев предпочтителен второй способ, так как модули можно динамически удалять и добавлять. Например, если устройство не подключено к системе или с ним не осуществляется работа, драйвер нам не нужен, и его можно выгрузить из памяти или не загружать вообще. При загрузке ядра ему необходима информация об устройствах, которые присутствуют в системе, чтобы корректно загрузить драйверы (модули) для них. Эта информация может передаваться из различных источников. Например, в архитектуре x86 это будет ACPI. В архитектуре ARM это Device Tree. ▍ Device Tree и Overlays Иногда Device Tree нужно модифицировать, чтобы можно было загрузить корректно драйвера для устройств. Делается это при помощи подключения overlays. Они содержат информацию, что необходимо изменить в исходном Device Tree. ▍ Headless-режим работы Raspberry Pi Очень часто в различных статьях и самоучителях по работе с Raspberry Pi необходимо подключение монитора, клавиатуры и мыши. Но на самом деле есть возможность работать с ним в так называемом headless-режиме. В этом режиме вы работаете с Raspberry Pi при помощи эмулятора терминала. Соединение его с Raspberry Pi может быть UART, USB, Bluetooth, Ethernet, Wi-Fi. Главная сложность заключается в том, как можно работать в этом режиме с самого начала, сразу после записи образа операционной системы на SD-карту, если у вас нет лишнего монитора и клавиатуры. Как активировать SSH, настроить Wi-Fi на использование определённой точки доступа? В Raspberry Pi OS такая возможность есть. Достаточно разместить определённые файлы в разделе FAT32 на SD-карте и загрузиться с неё. Raspberry Pi OS сделает необходимые настройки сама. ▍ Файловые системы, блочные устройства, разделы, монтирование Меня всегда восхищала идея, что в Linux всё является файлом. Правильно используя средства Linux, можно практически без программирования выполнять сложные задачи. Блочное устройство — это некоторый файл, в который можно записывать и считывать данные блоками байтов различной длины. Что будет происходить при этом, зависит от того, с чем реально ассоциирован этот файл. Например, если он ассоциирован с жёстким диском, то тогда будут читаться/записываться данные на жёсткий диск, не обращая внимание на разделы и файловые системы. Если он ассоциирован с разделом жёсткого диска, то будут читаться/записываться данные, не обращая внимание на файловую систему. При помощи команды losetup можно добиться того, что он будет ассоциироваться с обычным файлом на диске, что позволит создавать образы разделов и дисков. Ещё полезной командой Linux является команда kpartx, которая создаёт блочные устройства из файла образа диска. Каждое из устройств будет ассоциировано с образом раздела, который хранится в этом файле. Форматирование раздела в Linux выполняется одной командой. В качестве параметра необходимо передать имя файла блочного устройства. Например, для создания файловой системы exFAT на блочном устройстве /dev/mmcblk0p3: $ mkfs.exfat /dev/mmcblk0p3 Чтобы можно было работать с файлами файловой системы, размещённой на блочном устройстве, нужно примонтировать файловую систему к корневой файловой системе при помощи команды mount. $ mkdir -p /mnt/data $ mount -t auto /dev/mmcblk0p3 /mnt/data Обратите внимание, что директория, куда будет производиться монтирование, должна существовать до того, как вы будете монтировать. Если нужно размонтировать файловую систему, используется команда umount. $umount /mnt/data Чтобы посмотреть, какие у вас есть блочные устройства, и куда они примонтированы, можно использовать команду: $ lsblk ▍ Systemd Загрузка Linux происходит в несколько этапов. Сначала загружается ядро операционной системы, затем ядро запускает процесс init. Задача процесса init загрузить и инициализировать процессы пространства пользователя и находиться в памяти до перезагрузки или выключения компьютера (устройства). За долгие годы существования Linux было написано множество реализаций init. На данный момент во многих Linux- дистрибутивах используется реализация, называемая systemd. Её мы и будем использовать. Минимум команд, которые необходимо знать для работы с systemd. Команда Назначение systemctl start sn.service запустить службу systemctl stop sn.service остановить службу systemctl status sn.service посмотреть статус службы systemctl enable sn.service включить службу (служба будет автоматически запущена при следующей загрузке Linux) systemctl disable sn.service выключить службу journalctl -u sn.service -b посмотреть логи службы, начиная с момента последней загрузки Linux ▍ Терминалы, PuTTY, sshd, agetty Для администрирования ОС Linux из Windows часто используют эмулятор терминала PuTTY. Он позволяет подключаться к компьютеру или устройству c ОС Linux с помощью различных соединений (Ethernet, Wi-Fi, эмулируемого последовательного порта на Bluetooth или USB) и работать удалённо с консолью Linux в Windows. Чтобы такое было возможно, в ОС Linux должна быть запущена специальная программа, которая будет взаимодействовать с PuTTY. Это может быть sshd в случае SSH-соединения или agetty в случае последовательного порта. При подключении через последовательный порт по умолчанию вы увидите чёрно-белый экран без поддержки манипулятора мышь. Чтобы добавить поддержку мыши и цветного экрана, необходимо изменить значение переменной окружения TERM в файле /usr/lib/systemd/system/serial-getty@.service. [Service] Environment=TERM=xterm USB Чтобы два USB-устройства могли работать друг с другом, необходимо наличие у каждого из них USB-контроллера. USB-контроллер в конкретный момент времени может работать в режиме хоста (Host) или режиме периферийного устройства (Device). Если одно из взаимодействующих устройств работает в режиме Host, то другое должно работать в режиме Device. Существуют следующие виды USB-контроллеров: Host — всегда работает в режиме Host. Device — всегда работает в режиме Device. OTG — может работать или в режиме хоста или в режиме периферийного устройства. Переключение режимов может быть аппаратным (при помощи особой распайки кабеля OTG кабель переводит в контроллер режим хоста) или программным Режим хоста подразумевает посылку команд, а режим периферийного устройства — их обработку. ▍ OTG USB-контроллер Возьмём Android-телефон с OTG-контроллером. Это означает, что при сопряжении по USB с компьютером (для записи файлов с компьютера на телефон), он будет играть роль периферийного устройства, а при сопряжении по USB с периферийным устройством (мышью, клавиатурой, сетевой картой, флэш-накопителем, монитором, принтером) телефон будет играть роль хоста. Обычно USB-контроллер периферийного устройства или USB OTG-контроллер присутствуют во встраиваемых устройствах. Также они могут быть интегрированы в однокристальную систему (SoC). Но по факту на устройстве может отсутствовать физический USB-разъём для подключения. Например, на всех Raspberry Pi установлена SoC, которая имеет OTG-контроллер, но фактически физический разъём для него есть только в Raspberry Pi Zero (Zero W, Zero 2 W) и в Raspberry Pi 4. ▍ Дескрипторы USB Каждое USB-устройство имеет дескрипторы. Дескрипторы — это информация о USB- устройстве, которая используется операционной системой для корректного выбора драйвера для устройства. Мне понравилось описание, которое приведено на сайте Microsoft. ▍ Создание USB-устройств в Linux Ядро Linux содержит модули, которые позволяют создавать виртуальные USB-устройства. Это может быть Mass Storage, последовательный порт, сетевая карта. Загрузив и настроив эти модули, вы можете сделать так, чтобы компьютером ваш одноплатник распознавался одним или несколькими такими устройствами. Если вам достаточно одного устройства, то вы можете загрузить модуль для этого устройства, опционально передав ему параметры для конфигурации при помощи команды modprobe. Когда отпадёт необходимость в этом устройстве, его можно выгрузить при помощи команды modprobe -r. Чтобы на одном физическом порту у вас распознавалось несколько устройств одновременно, нужно использовать модуль libcomposite и сконфигурировать эти устройства при помощи создания структур в файловой системе ConfigFS в директории /sys/kernel/config/usb_gadget. Такие устройства называются композитными USB-устройствами. Вы, скорее всего, встречались с такими, например, если у вас беспроводная клавиатура и мышь, а для них используется один приёмопередатчик. В нашем случае мы создадим композитное USB-устройство, которое будет последовательным портом и устройством хранения. Последовательный порт мы будем использовать для подключения к нашему эмулятору оптических дисков через PuTTY. Изначально я хотел, что бы это была сетевая карта и SSH, но карта требует настройки в операционной системе компьютера, поэтому для простоты отказался от этой идеи в пользу последовательного порта. ▍ Создание композитного USB-устройства при помощи ConfigFS 1. Загружаем модуль libcomposite. modprobe libcomposite 2. Заполняем дескрипторы для устройства. $ usb_dev=/sys/kernel/config/usb_gadget/cdemu $ mkdir -p $usb_dev $ echo 0x0137 > $usb_dev/idProduct $ echo 0x0100 > $usb_dev/bcdDevice $ echo 0x0200 > $usb_dev/bcdUSB $ echo 0xEF > $usb_dev/bDeviceClass $ echo 0x02 > $usb_dev/bDeviceSubClass $ echo 0x01 > $usb_dev/bDeviceProtocol $ mkdir -p $usb_dev/strings/0x409 $ echo ""abababababababa"" > $usb_dev/strings/0x409/serialnumber $ echo ""Linux Foundation"" > $usb_dev/strings/0x409/manufacturer $ echo ""USB CD-ROM Emulator"" > $usb_dev/strings/0x409/product 3. Создаём конфигурацию. mkdir -p $usb_dev/configs/c.1 mkdir -p $usb_dev/configs/c.1/strings/0x409 echo ""acm+usb"" > $usb_dev/configs/c.1/strings/0x409/configuration echo ""0x80"" > $usb_dev/configs/c.1/bmAttributes echo 250 > $usb_dev/configs/c.1/MaxPower 4.Создаём и подключаем функцию acm (последовательный порт через USB). $ mkdir -p $usb_dev/functions/acm.usb0 $ ln -s $usb_dev/functions/acm.usb0 $usb_dev/configs/c.1 5. Создаём и подключаем функцию mass_storage. Mass_storage в данном случае — это эмуляция CD-ROM для ISO-образа /home/pi/1.iso. $ mkdir -p $usb_dev/functions/mass_storage.usb0/lun.0 $ echo 1 > $usb_dev/functions/mass_storage.usb0/lun.0/cdrom $ echo 1 > $usb_dev/functions/mass_storage.usb0/lun.0/removable $ echo 0 > $usb_dev/functions/mass_storage.usb0/lun.0/nofua $ echo 0 > $usb_dev/functions/mass_storage.usb0/stall $ echo ""/home/pi/1.iso"" > $usb_dev/functions/mass_storage.usb0/lun.0/file $ ln -s $usb_dev/functions/mass_storage.usb0 $usb_dev/configs/c.1 6. Активируем созданное устройство. $ ls /sys/class/udc > $usb_dev/UDC ▍ Удаление композитного USB-устройства при помощи ConfigFS 1. Деактивируем устройство. $ usb_dev=/sys/kernel/config/usb_gadget/cdemu $ echo """"> $usb_dev/UDC 2. Удаляем функцию mass_storage. $ rm $usb_dev/configs/c.1/mass_storage.usb0 $ rmdir $usb_dev/functions/mass_storage.usb0 3. Удаляем функцию acm. $ rm $usb_dev/configs/c.1/acm.usb0 $ rmdir $usb_dev/functions/acm.usb0 4. Удаляем конфигурацию. $ rmdir $usb_dev/configs/c.1/strings/0x409 $ rmdir $usb_dev/configs/c.1 5. Удаляем устройство. $ rmdir $usb_dev/strings/0x409 $ rmdir $usb_dev 6. Выгружаем загруженные устройством модули. $ modprobe -r usb_f_mass_storage $ modprobe -r usb_f_acm $ modprobe -r libcomposite Cтруктура файловой системы для созданного эмулятора CD-ROM Bluetooth Тема Bluetooth очень объёмная, и её невозможно изложить в одной статье, поэтому приведу только тот минимум, который позволяет понять, как мы будем использовать Bluetooth. Bluetooth — технология, которая позволяет связывать устройства без проводов по радиоканалу. На данный момент существует множество версий спецификации Bluetooth. Спецификация Bluetooth освещает множество вопросов. Чтобы передать данные с одного устройства на другое, необходимо наличие на обоих устройствах контроллеров и стеков Bluetooth. Bluetooth-контроллер — аппаратное устройство, обычно выполненное в виде микросхемы или части более сложной микросхемы, позволяющее получать/передавать данные по радиоканалу в соответствии со спецификацией Bluetooth. Bluetooth-стек — программная реализация протоколов, описанных в спецификации Bluetooth. Протоколы Bluetooth, предназначенные для решения определённых задач, группируются в профили Bluetooth. Мы будем использовать два профиля Bluetooth: 1. Generic Access Profile (GAP), который поддерживается всеми Bluetooth-устройствами. 2. Serial Port Profile (SPP), который подразумевает использование последовательного порта поверх соединения Bluetooth. ▍ Поддержка Bluetooth операционными системами Bluetooth-контроллеры могут иметь различные аппаратные интерфейсы для доступа. Это может быть UART, USB, PCIe. В случае операционной системы многие детали скрываются, и можно о них не думать. С контроллером можно работать на низком уровне через драйвер или уже используя высокоуровневые библиотеки и приложения, предоставляемые стеком Bluetooth, например, в Linux широко распространён стек BlueZ. ▍ BlueZ Стек Bluetooth BlueZ состоит из двух частей. Одна часть представлена модулями ядра Linux, уже включена в ядро. Если она отсутствует, то её нужно включить и перекомпилировать ядро. Вторая часть представлена приложениями для пространства пользователя. Приложения позволяют конфигурировать и работать со стеком Bluetooth. На данный момент многие приложения считаются устаревшими, и разработчики BlueZ рекомендуют использовать более новые приложения и интерфейс D-Bus для работы со стеком. Но, как мне кажется, именно те старые, устаревшие приложения позволяют лучше понять работу Bluetooth, поэтому в учебных целях я буду использовать их, для чего нужно будет инициализировать BlueZ в режиме совместимости. $ bluetoothd --noplugin=sap -C ▍ Протоколы Bluetooth Я не буду утомлять вас различными схемами, диаграммами, которые вы легко можете найти в интернете. Расскажу только о тех протоколах, с которыми нам предстоит столкнуться и нужно будет сконфигурировать. ▍ Service Discovery Protocol (SDP) При помощи протокола SDP можно определить, какие приложения (сервисы) находятся на хосте, и с ними возможен обмен данными через Bluetooth Чтобы можно было увидеть сервис с другого устройства, его необходимо зарегистрировать в SDP database. Например, если мы хотим зарегистрировать службу, представляющую эмуляцию последовательного порта в Bluetooth, это можно сделать следующей командой: $ sdptool add SP Чтобы можно было посмотреть службы, зарегистрированные у вас на хосте, нужно ввести команду: $ sdptool browse local ▍ Radio Frequency Communications (RFCOMM) Протокол RFCOMM позволяет создавать виртуальное соединение по последовательному порту между двумя хостами. На одном из хостов создаётся сервер, которому выделяется канал RFCOMM, второй из хостов подключается к нему, указывая MAC-адрес и номер канала Канал RFCOMM немного напоминает порт в UDP или TCP, но если у них и у источника и у получателя есть порты, то у RFCOMM для источника и получателя один и тот же канал. Поэтому невозможно создать несколько подключений на один и тот же канал. В Linux можно использовать команду rfcomm для создания процесса, который будет слушать определённый канал RFCOMM и при соединении запускать другой процесс. $ rfcomm -r watch hci0 1 /usr/local/bin/cdemu-cmd /dev/rfcomm0 /dev/rfcomm0 В данном случае на Bluetooth-контроллере hci0 RFCOMM будет прослушиваться канал 1 и запускаться процесс cdemu-cmd с двумя параметрами командной строки /dev/rfcomm0 и /dev/rfcomm0. ▍ Утилита bluetoothctl Утилита Bluetoothctl позволяет сопрягать устройство, на котором вы её запустили с другим устройством. Вы можете сделать устройство видимым для обнаружения другими устройствами, а также найти другое устройство и выполнить с ним сопряжение. Более подробно расписано в документации к утилите, которая доступна по команде: $ man bluetoothctl ▍ Serial Bluetooth Terminal Для отладки приложений, использующих Bluetooth, удобно использовать приложение для Android Serial Bluetooth Terminal. Это приложение позволяет работать с Bluetooth- устройствами, у которых доступен профиль SPP. В нашем случае мы будем использовать его как визуальный интерфейс для работы с нашим эмулятором оптических дисков. Сборка и модификация дистрибутива Raspberry Pi OS Чтобы сделать полноценный аппаратный эмулятор оптических дисков, нам придётся немного модифицировать исходный дистрибутив Linux. Это подразумевает перекомпиляцию ядра, изменение нескольких конфигурационных файлов и добавление своего программного кода. Для меня это было удобно сделать при помощи Docker. ▍ Кросс-компиляция ядра Linux Кросс-компиляция позволяет на компьютере с одной архитектурой получать исполняемые файлы для другой архитектуры. Мы можем компилировать ядро Linux для Raspberry Pi на Raspberry Pi, а можем, используя кросс-компиляцию, сделать это на обычном компьютере с архитектурой x86, что существенно сократит время компиляции из-за большего быстродействия компьютера. Подробно о том, как выполнять кросс-компиляцию Raspberry Pi OS, можно почитать тут. ▍ Chroot и запуск бинарных файлов другой архитектуры Команда Linux chroot позволяет запускать процессы с изменённой корневой системой. Это кажется немного запутанным, но суть в следующем. В качестве параметра команде передаётся путь к корневой директории. В результате запуска команды через chroot запущенный процесс будет считать, что корнем файловой системы является та директория, которую передали в качестве параметра. Применений у команды chroot несколько, например, её можно использовать, чтобы запустить команду apt для Raspberry Pi в Docker-контейнере. Интересно, что Docker Desktop для Windows позволяет запускать исполняемые файлы для архитектуры ARM. В Linux-версии Docker такое сделать можно, но нужна дополнительная настройка. Реализация Созданный мной проект состоит из следующих файлов: 1. Dockerfile и скрипт, который выполняется в Docker-контейнере. 2. Файлы, которые необходимо добавить или обновить в исходном дистрибутиве: cdemu — основная логика работы эмулятора оптических дисков, написанная на языке bash; cdemu-cmd — bash-скрипт для обработки команд от пользователя и передачи их эмулятору; bash-utils.sh — bash-скрипт co вспомогательными функциями; cdemu-bluetooth-ui.service — systemd-служба, которая запускает интерпретатор команд на создаваемом RFCOMM-соединении телефона и Raspberry Pi; cdemu.service — systemd-служба, которая запускает эмулятор оптических дисков при загрузке; bluetooth.service — изменённая служба systemd для инициализации bluetooth; serial-getty@.service — изменённая служба systemd для запуска agetty на создаваемом соединении на последовательном порту; firstboot.service — служба systemd для запуска скрипта при первой загрузке операционной системы. Я её позаимствовал из проекта raspberian-firstboot; config.txt — изменённый файл конфигурации для загрузки Raspberry Pi. Содержит подключение overlay dwc. Это необходимо, чтобы USB-контроллер мог работать в device mode; fistboot.sh — скрипт, который запускается службой systemd firstboot.service; userconf.txt — файл, который необходим, чтобы установить пароль для пользователя pi. В последних версиях Raspberry Pi OS пользователь pi не активирован по умолчанию, поэтому необходимо наличие этого файла; ssh — файл необходим, чтобы активировать ssh, который отключён по умолчанию; wpa_supplicant.conf — файл, необходимый, если вы хотите настроить Raspberry Pi на работу с вашей точкой доступа. Листинги файлов не привожу, так как это ещё больше раздует и так большую статью. Ознакомиться вы с ними можете здесь. Как пользоваться эмулятором 1. Собираем Docker-образ. docker build -t raspi-image . 2. Собираем образ RaspberryPi OS. docker run --privileged -v c:\temp:/build --name raspi-image -it --rm raspi-image 3. Записываем образ на SD-карту. Вставляем её в Raspberry Pi. 4. Подключаем Rapsberry Pi Zero 2 W к компьютеру. 5. Через некоторое время у вас появится съёмный накопитель. 6. На этот съёмный накопитель, содержащий файл Readme.txt, копируем образы, которые хотим эмулировать. 7. Находим виртуальный COM-порт, созданный после подключения Raspberry Pi к компьютеру. 8. Подключаемся к Raspberry Pi через с помощью Putty через виртуальный COM-порт. 9. Запускаем интерактивное приложение для управления эмулятором. $ sudo cdemu-cmd 10. Если хотим сделать управление с телефона, то выполняем сопряжение телефона и Raspberry Pi. Для чего вводим в эмуляторе терминала команду: $ sudo bluetoothctl 11. Делаем Raspberry Pi доступным для обнаружения: discoverable on 12. Находим его на телефоне и выполняем полключение. После чего соглашаемся с PIN- кодом на телефоне и Raspberry Pi. yes 13. Выходим из bluetoothctl. exit 14. Запускаем на телефоне Serial Bluetooth Terminal и выполняем подключение к Raspberry Pi из него. Теперь можно посылать команды созданному эмулятору CD-ROM. Команды, которые можно посылать эмулятору: 1. hdd — переключение в режим эмуляции внешних жёстких дисков. 2. cdrom — переключение в режим эмуляции внешних приводов оптических дисков. 3. list — вывести список доступных ISO-образов, которые можно эмулировать. 4. insert <порядковый номер> — поместить ISO-образ для эмуляции. 5. eject — извлечь ISO-образ из эмулятора. 6. help — показать список доступных команд в текущем режиме. Особенности моего эмулятора Интересно, но в Linux по умолчанию нельзя эмулировать ISO-образы размером больше 2 Gib. Я просмотрел исходный код драйвера в файле drivers/usb/gadget/function/storage_common.c и предположил, что нет оснований не применять патч к ядру Linux от Adam Bambuch, который просто удаляет одно условие. Образы эмулировались нормально и при снятии ISO-образа с эмулируемого CD-ROM он был идентичен исходному. Поверял по хеш-коду для файла ISO-образа. Не пойму, почему есть это ограничение в Linux и почему его до сих пор не убрали? Если вы знаете ответ, ответьте в комментариях. Моя реализация не требует никаких дополнительных деталей. Нужен только Raspberry Pi Zero 2 W, один или два кабеля USB и адаптер питания, если будете использовать два кабеля USB. Один для питания, второй для передачи данных. Хоть и использование дополнительного кабеля и адаптера добавляет громоздкости, это решает проблему перезагрузки Raspberry Pi, если компьютер или ноутбук отключает ненадолго питание при перезагрузке. Кроме того, я не использую Python, только bash. Выводы Полученное программно-аппаратное решение, хоть и обладает рядом недостатков по сравнению с карманом Zalman (не поддерживается USB 3.0, нет интерактивного меню на самом устройстве), позволит вам установить практически любую операционную систему на широкий спектр компьютеров путём простого копирования ISO-образа. Решение является прототипом, но вместе с тем позволяет углубить знания по многим темам, или получить, если вы были с ними не знакомы. Так как основной целью была разработка прототипа, я запускал bluetoothd в режиме совместимости, и почти всю логику написал на bash. Я хотел показать возможность превратить встраиваемое устройство с операционной системой Linux в аппаратный эмулятор флеш-накопителя или привода оптических дисков, приложив минимум усилий. Надеюсь, что это удалось. Чтобы уместить всё в одной статье, я лишь поверхностно коснулся тех тем, которые необходимы для понимания. Если вас заинтересовало, вы можете самостоятельно изучить их углублённо. Объём статьи не позволяет осветить все интересности, с которыми я столкнулся при разработке эмулятора, и решения, которые применял и проверял. Приведу лишь несколько из них. Например, я долго боролся с зависанием при удалении составного устройства. Помогло использование службы serial-getty вместо getty, хотя во многих статьях упоминалась getty. Я долго разбирался, как можно сделать сопряжение через Bluetooth между Raspberry и телефоном, использовал команду bt-agent, но потом всё-таки отказался от неё в пользу bluetoothctl. При переключении эмулятора в режим HDD для записи ISO-образов изначально я открывал для доступа всю SD-карту и хранил ISO-образы в отдельном разделе, но потом посчитал, что для безопасности лучше хранить образ диска с ISO-образами в отдельном файле и открывать доступ только к нему, хоть это и снизило скорость записи, но пользователя не обескураживают появляющиеся несколько дисков. Разработанный прототип есть куда улучшать. Можно, например, создать более минималистичный дистрибутив Linux, который будет содержать только то, что реально используется для эмуляции, или создать более удобное графическое приложение для Android для работы с эмулятором. А можно упростить работу с Bluetooth, напрямую работая с драйверами bluetooth или используя интерфейс D-Bus для работы с Bluetooth- устройствами. Или вообще всё-таки взяться за паяльник и сделать устройство, более похожее по функционалу на карман Zalman. Но главное, вы увидели, что это реально сделать, а когда видишь положительный результат, это вдохновляет на большее творчество. В процессе тестирования и отладки программного кода было замечено, что на Lenovo X1 Extreme Gen 2 эмулятор CD-ROM дисков великолепно определялся в Windows 10, но отказывался определяться в BIOS. Эмпирически было определено, что помогает отключение режима экономии энергии процессора в BIOS. Также ноутбук отключал питание на usb при перезагрузке, поэтому понадобилось дополнительное питание Raspberry Pi. Интересно, но на ASUS K53E и Gigabyte BRIX всё работает без проблем. Решение с небольшими модификациями можно реализовать на Raspberry Pi 4. Но если вы поняли суть решения, вы его сможете повторить и на других одноплатных компьютерах, которые имеют выведенные USB-порты для OTG или USB-контроллеры периферийных устройств. RUVDS.com VDS/VPS-хостинг. Скидка 15% по коду HABR15 Telegram ВКонтакте Twitter 84 Карма 82.1 Рейтинг @artyomsoft Пользователь Комментарии 17 Dockerfile на данный момент только выполняется в Docker Desktop для Windows. В Linux он работать не будет. В заключение хочу сказать, что существует ещё один способ эмулировать оптические диски, который я не пробовал, но знаю о его существовании из ваших комментариев к одной из моих статей — это программа DriveDroid для Android. Я ей не пользовался, так как для её работы нужно получать права root на телефоне. Но, скорее всего, из-за ограничений в ядре Linux программа поддерживает ISO-образы до 2 Gib и/или работает только с гибридными ISO-образами. Если я не прав, буду рад увидеть ваши опровержения в комментариях. Telegram-канал с розыгрышами призов, новостями IT и постами о ретроиграх 🕹️",artyomsoft
Как создать аппаратный эмулятор CD-ROM,RUVDS.com,2394.92,VDS/VPS-хостинг. Скидка 15% по коду HABR15,2023-03-22,"Несмотря на то, что постепенно оптические диски уходят в прошлое, использование ISO-образов этих дисков остаётся актуальным. Многие операционные системы поставляются в виде ISO-образов, а администраторам необходимо поддерживать разношёрстный парк старых персональных компьютеров. Существует множество решений, как можно установить операционную систему с ISO-образа без записи его на оптический носитель. Я уже затрагивал тему ISO-образов в моих статьях: «Раскрываем секреты загрузочных ISO-образов» и «Что вам нужно знать о внешних загрузочных дисках». В этой статье я хочу рассказать о ещё одном способе, который, как оказывается, вшит в ядро Linux. Если ваш одноплатный компьютер имеет USB OTG-разъём, и на него возможна установка Linux, то вы c большой долей вероятности можете сделать из одноплатника аппаратный эмулятор привода оптических дисков. Меня этот способ заинтересовал. Я проверил его сам и, получив положительный результат у себя, решил поделиться с вами. Я сам узнал много интересного, систематизировал свои знания, поэтому надеюсь, что чтение будет познавательно и интересно для вас. Как всегда, если вы хотите посмотреть, что получится в итоге, уточнить детали, вы всегда можете найти исходный код в моём репозитории на GitHub. При написании статьи я поставил себе следующие цели: 1. Аппаратный эмулятор CD-ROM должен быть реализован без использования паяльника и макетных плат. 2. Реализация должна быть понятна человеку, имеющему лишь базовые представления о Linux, USB и Bluetooth. 3. Решение должно быть таким, чтобы его можно было с небольшими изменениями реализовать на различных одноплатных компьютерах. 4. Побудить интерес читателя к изучению используемых в статье технологий. 5. Изложить материал, необходимый для решения задачи лаконично и просто. Не уверен, что у меня это получилось из- за большого объёма темы. Буду признателен, если вы в отзывах напишете своё мнение. Оглавление • Суть решения • Проверка решения на практике • От проверки идеи до реализации • Операционная система Linux • USB • Bluetooth • Сборка и модификация дистрибутива Raspberry Pi OS • Реализация • Как пользоваться эмулятором • Особенности моего эмулятора • Выводы Суть решения Решение заключается в том, что, модифицируя операционную систему Linux на одноплатном компьютере (встраиваемой системе), можно получить из него устройство, которое будет распознаваться компьютером как внешний оптический привод USB или флеш- накопитель. В ядро Linux включена поддержка эмуляции CD-ROM и эмуляции флеш-накопителя. Но это не значит, что любую встраиваемую систему можно превратить в них. Для этого ещё необходимо, чтобы встраиваемая система имела USB OTG-контроллер или USB- контроллер периферийного устройства. Проверка решения на практике Я делал эмулятор оптических дисков, используя Raspberry Pi Zero 2 W. Но вы можете использовать и другие одноплатные компьютеры. Естественно, вам тогда придётся самим разбираться с некоторыми проблемами, которые с большой долей вероятностью у вас возникнут. У меня других одноплатных компьютеров кроме Raspberry Pi не было, поэтому привожу алгоритм, как делал я. 1. Скачать образ Raspberry OS Light с сайта raspberrypi.org. 2. Записать образ на SD-карту. Я использовал программу balenaEtcher. 3. Добавить строку dtoverlay=dwc2 в файле config.txt на SD- карте. 4. Записать файлы ssh и wpa_supplicant.conf на SD-карту. Файл userconf.txt нужен, чтобы установить пароль для пользователя, ssh — чтобы включить SSH, wpa_supplicant.conf — чтобы указать точку доступа и пароль для Wi-Fi. 5. Вставить SD-карту в Raspberry Pi Zero 2 W. 6. Подключить USB-разъём Raspberry Pi Zero 2 W к USB-разъёму компьютера. 7. Подождать, пока выполнится первая загрузка и Raspberry Pi Zero 2 W подключится к Wi-Fi-сети. 8. Найти IP-адрес Raspberry Pi Zero 2 W в Wi-Fi-сети и подключиться к нему по протоколу SFTP. Я использовал приложение WinSCP. 9. Записать ISO-образы, которые вы хотите эмулировать, в файловую систему Raspberry Pi. 10. Подключиться через SSH к Raspberry Pi W 2. Это можно сделать при помощи приложения PuTTY. 11. Для того, чтобы ваш Raspberry Pi Zero 2 W стал внешним USB CD-ROM, ввести команду: После чего у вас на компьютере распознается внешний USB CD- ROM, в который вставлен диск. 12. Для прекращения эмуляции ввести команду: Приведённый файл userconf.txt устанавливает для пользователя pi пароль «raspberry». Файл ssh — это пустой файл, который не содержит никаких данных. От проверки идеи до реализации Приведённая выше последовательность шагов позволяет вам посмотреть работу эмуляции в действии. Однако это решение обладает рядом недостатков. $ sudo modprobe g_mass_storage cdrom=y removable=y stall $ sudo modprobe -r g_mass_storage Файл userconf.txt Файл wpa_supplicant.conf 1. Чтобы загрузить и выбрать образ для эмуляции, необходимо наличие Wi-Fi. 2. При перезагрузке Raspberry Pi необходимо заново монтировать образ. 3. Если нужно эмулировать образ для загрузки с него операционной системы, понадобится ещё один компьютер для управления Raspberry Pi. 4. Эмулировать можно образ размером максимум 2 Gib. Если вам интересно, как избавиться от этих недостатков, у вас есть время и интерес разобраться в этом вопросе, то предлагаю продолжить чтение. Краткое содержание моей реализации следующее: 1. Для общения с Raspberry Pi будет использоваться Bluetooth. 2. Чтобы работа устройства была возможна в автономном режиме (без подключений Wi-Fi и Bluetooth), управляющий скрипт оформляется в виде службы Systemd. 3. Для управления по Bluetooth будет использоваться приложение Serial Bluetooth Terminal c Google Play. 4. Для эмуляции оптических дисков с образов размером больше 2Gib необходимо внести небольшие изменения в модуль ядра Linux и выполнить перекомпиляцию. Приведу кратко, что вам нужно знать, чтобы лучше понять суть того, что мы будем делать. Операционная система Linux ▍ Ядро Linux Ядро Linux содержит в себе абстракции для работы с устройствами там, где оно запускается. Реализуются эти абстракции в специальных программах, называемых драйверами. В ОС Linux драйвер может находиться непосредственно в файле ядра, а может быть оформлен в виде отдельного модуля. В большинстве случаев предпочтителен второй способ, так как модули можно динамически удалять и добавлять. Например, если устройство не подключено к системе или с ним не осуществляется работа, драйвер нам не нужен, и его можно выгрузить из памяти или не загружать вообще. При загрузке ядра ему необходима информация об устройствах, которые присутствуют в системе, чтобы корректно загрузить драйверы (модули) для них. Эта информация может передаваться из различных источников. Например, в архитектуре x86 это будет ACPI. В архитектуре ARM это Device Tree. ▍ Device Tree и Overlays Иногда Device Tree нужно модифицировать, чтобы можно было загрузить корректно драйвера для устройств. Делается это при помощи подключения overlays. Они содержат информацию, что необходимо изменить в исходном Device Tree. ▍ Headless-режим работы Raspberry Pi Очень часто в различных статьях и самоучителях по работе с Raspberry Pi необходимо подключение монитора, клавиатуры и мыши. Но на самом деле есть возможность работать с ним в так называемом headless-режиме. В этом режиме вы работаете с Raspberry Pi при помощи эмулятора терминала. Соединение его с Raspberry Pi может быть UART, USB, Bluetooth, Ethernet, Wi-Fi. Главная сложность заключается в том, как можно работать в этом режиме с самого начала, сразу после записи образа операционной системы на SD-карту, если у вас нет лишнего монитора и клавиатуры. Как активировать SSH, настроить Wi-Fi на использование определённой точки доступа? В Raspberry Pi OS такая возможность есть. Достаточно разместить определённые файлы в разделе FAT32 на SD-карте и загрузиться с неё. Raspberry Pi OS сделает необходимые настройки сама. ▍ Файловые системы, блочные устройства, разделы, монтирование Меня всегда восхищала идея, что в Linux всё является файлом. Правильно используя средства Linux, можно практически без программирования выполнять сложные задачи. Блочное устройство — это некоторый файл, в который можно записывать и считывать данные блоками байтов различной длины. Что будет происходить при этом, зависит от того, с чем реально ассоциирован этот файл. Например, если он ассоциирован с жёстким диском, то тогда будут читаться/записываться данные на жёсткий диск, не обращая внимание на разделы и файловые системы. Если он ассоциирован с разделом жёсткого диска, то будут читаться/записываться данные, не обращая внимание на файловую систему. При помощи команды losetup можно добиться того, что он будет ассоциироваться с обычным файлом на диске, что позволит создавать образы разделов и дисков. Ещё полезной командой Linux является команда kpartx, которая создаёт блочные устройства из файла образа диска. Каждое из устройств будет ассоциировано с образом раздела, который хранится в этом файле. Форматирование раздела в Linux выполняется одной командой. В качестве параметра необходимо передать имя файла блочного устройства. Например, для создания файловой системы exFAT на блочном устройстве /dev/mmcblk0p3: Чтобы можно было работать с файлами файловой системы, размещённой на блочном устройстве, нужно примонтировать файловую систему к корневой файловой системе при помощи команды mount. Обратите внимание, что директория, куда будет производиться монтирование, должна существовать до того, как вы будете монтировать. Если нужно размонтировать файловую систему, используется команда umount. Чтобы посмотреть, какие у вас есть блочные устройства, и куда они примонтированы, можно использовать команду: ▍ Systemd Загрузка Linux происходит в несколько этапов. Сначала загружается ядро операционной системы, затем ядро запускает процесс init. Задача процесса init загрузить и инициализировать процессы пространства пользователя и находиться в памяти до перезагрузки или выключения компьютера (устройства). За долгие годы существования Linux было написано множество реализаций init. На данный момент во многих Linux-дистрибутивах используется реализация, называемая systemd. Её мы и будем использовать. $ mkfs.exfat /dev/mmcblk0p3 $ mkdir -p /mnt/data $ mount -t auto /dev/mmcblk0p3 /mnt/data $umount /mnt/data $ lsblk Минимум команд, которые необходимо знать для работы с systemd. Команда Назначение systemctl start sn.service запустить службу systemctl stop sn.service остановить службу systemctl status sn.service посмотреть статус службы systemctl enable sn.service включить службу (служба будет автоматически запущена при следующей загрузке Linux) systemctl disable sn.service выключить службу journalctl -u sn.service -b посмотреть логи службы, начиная с момента последней загрузки Linux ▍ Терминалы, PuTTY, sshd, agetty Для администрирования ОС Linux из Windows часто используют эмулятор терминала PuTTY. Он позволяет подключаться к компьютеру или устройству c ОС Linux с помощью различных соединений (Ethernet, Wi-Fi, эмулируемого последовательного порта на Bluetooth или USB) и работать удалённо с консолью Linux в Windows. Чтобы такое было возможно, в ОС Linux должна быть запущена специальная программа, которая будет взаимодействовать с PuTTY. Это может быть sshd в случае SSH- соединения или agetty в случае последовательного порта. При подключении через последовательный порт по умолчанию вы увидите чёрно-белый экран без поддержки манипулятора мышь. Чтобы добавить поддержку мыши и цветного экрана, необходимо изменить значение переменной окружения TERM в файле /usr/lib/systemd/system/serial-getty@.service. USB Чтобы два USB-устройства могли работать друг с другом, необходимо наличие у каждого из них USB-контроллера. USB- контроллер в конкретный момент времени может работать в режиме хоста (Host) или режиме периферийного устройства (Device). Если одно из взаимодействующих устройств работает в режиме Host, то другое должно работать в режиме Device. Существуют следующие виды USB-контроллеров: • Host — всегда работает в режиме Host. • Device — всегда работает в режиме Device. • OTG — может работать или в режиме хоста или в режиме периферийного устройства. Переключение режимов может быть аппаратным (при помощи особой распайки кабеля OTG кабель переводит в контроллер режим хоста) или программным Режим хоста подразумевает посылку команд, а режим периферийного устройства — их обработку. ▍ OTG USB-контроллер Возьмём Android-телефон с OTG-контроллером. Это означает, что при сопряжении по USB с компьютером (для записи файлов с компьютера на телефон), он будет играть роль периферийного устройства, а при сопряжении по USB с периферийным устройством (мышью, клавиатурой, сетевой картой, флэш-накопителем, монитором, принтером) телефон будет играть роль хоста. [Service] Environment=TERM=xterm Обычно USB-контроллер периферийного устройства или USB OTG- контроллер присутствуют во встраиваемых устройствах. Также они могут быть интегрированы в однокристальную систему (SoC). Но по факту на устройстве может отсутствовать физический USB-разъём для подключения. Например, на всех Raspberry Pi установлена SoC, которая имеет OTG-контроллер, но фактически физический разъём для него есть только в Raspberry Pi Zero (Zero W, Zero 2 W) и в Raspberry Pi 4. ▍ Дескрипторы USB Каждое USB-устройство имеет дескрипторы. Дескрипторы — это информация о USB-устройстве, которая используется операционной системой для корректного выбора драйвера для устройства. Мне понравилось описание, которое приведено на сайте Microsoft. ▍ Создание USB-устройств в Linux Ядро Linux содержит модули, которые позволяют создавать виртуальные USB-устройства. Это может быть Mass Storage, последовательный порт, сетевая карта. Загрузив и настроив эти модули, вы можете сделать так, чтобы компьютером ваш одноплатник распознавался одним или несколькими такими устройствами. Если вам достаточно одного устройства, то вы можете загрузить модуль для этого устройства, опционально передав ему параметры для конфигурации при помощи команды modprobe. Когда отпадёт необходимость в этом устройстве, его можно выгрузить при помощи команды modprobe -r. Чтобы на одном физическом порту у вас распознавалось несколько устройств одновременно, нужно использовать модуль libcomposite и сконфигурировать эти устройства при помощи создания структур в файловой системе ConfigFS в директории /sys/kernel/config/usb_gadget. Такие устройства называются композитными USB-устройствами. Вы, скорее всего, встречались с такими, например, если у вас беспроводная клавиатура и мышь, а для них используется один приёмопередатчик. В нашем случае мы создадим композитное USB-устройство, которое будет последовательным портом и устройством хранения. Последовательный порт мы будем использовать для подключения к нашему эмулятору оптических дисков через PuTTY. Изначально я хотел, что бы это была сетевая карта и SSH, но карта требует настройки в операционной системе компьютера, поэтому для простоты отказался от этой идеи в пользу последовательного порта. ▍ Создание композитного USB-устройства при помощи ConfigFS 1. Загружаем модуль libcomposite. 2. Заполняем дескрипторы для устройства. modprobe libcomposite $ usb_dev=/sys/kernel/config/usb_gadget/cdemu $ mkdir -p $usb_dev $ echo 0x0137 > $usb_dev/idProduct $ echo 0x0100 > $usb_dev/bcdDevice $ echo 0x0200 > $usb_dev/bcdUSB $ echo 0xEF > $usb_dev/bDeviceClass $ echo 0x02 > $usb_dev/bDeviceSubClass $ echo 0x01 > $usb_dev/bDeviceProtocol 3. Создаём конфигурацию. 4.Создаём и подключаем функцию acm (последовательный порт через USB). 5. Создаём и подключаем функцию mass_storage. Mass_storage в данном случае — это эмуляция CD-ROM для ISO-образа /home/pi/1.iso. 6. Активируем созданное устройство. $ mkdir -p $usb_dev/strings/0x409 $ echo ""abababababababa"" > $usb_dev/strings/0x409/serialnumb $ echo ""Linux Foundation"" > $usb_dev/strings/0x409/manufactu $ echo ""USB CD-ROM Emulator"" > $usb_dev/strings/0x409/produc mkdir -p $usb_dev/configs/c.1 mkdir -p $usb_dev/configs/c.1/strings/0x409 echo ""acm+usb"" > $usb_dev/configs/c.1/strings/0x409/configur echo ""0x80"" > $usb_dev/configs/c.1/bmAttributes echo 250 > $usb_dev/configs/c.1/MaxPower $ mkdir -p $usb_dev/functions/acm.usb0 $ ln -s $usb_dev/functions/acm.usb0 $usb_dev/configs/c.1 $ mkdir -p $usb_dev/functions/mass_storage.usb0/lun.0 $ echo 1 > $usb_dev/functions/mass_storage.usb0/lun.0/cdrom $ echo 1 > $usb_dev/functions/mass_storage.usb0/lun.0/remova $ echo 0 > $usb_dev/functions/mass_storage.usb0/lun.0/nofua $ echo 0 > $usb_dev/functions/mass_storage.usb0/stall $ echo ""/home/pi/1.iso"" > $usb_dev/functions/mass_storage.us $ ln -s $usb_dev/functions/mass_storage.usb0 $usb_dev/config ▍ Удаление композитного USB-устройства при помощи ConfigFS 1. Деактивируем устройство. 2. Удаляем функцию mass_storage. 3. Удаляем функцию acm. 4. Удаляем конфигурацию. 5. Удаляем устройство. $ ls /sys/class/udc > $usb_dev/UDC $ usb_dev=/sys/kernel/config/usb_gadget/cdemu $ echo """"> $usb_dev/UDC $ rm $usb_dev/configs/c.1/mass_storage.usb0 $ rmdir $usb_dev/functions/mass_storage.usb0 $ rm $usb_dev/configs/c.1/acm.usb0 $ rmdir $usb_dev/functions/acm.usb0 $ rmdir $usb_dev/configs/c.1/strings/0x409 $ rmdir $usb_dev/configs/c.1 $ rmdir $usb_dev/strings/0x409 $ rmdir $usb_dev 6. Выгружаем загруженные устройством модули. Cтруктура файловой системы для созданного эмулятора CD-ROM Bluetooth Тема Bluetooth очень объёмная, и её невозможно изложить в одной статье, поэтому приведу только тот минимум, который позволяет понять, как мы будем использовать Bluetooth. Bluetooth — технология, которая позволяет связывать устройства без проводов по радиоканалу. На данный момент существует множество версий спецификации Bluetooth. Спецификация Bluetooth освещает $ modprobe -r usb_f_mass_storage $ modprobe -r usb_f_acm $ modprobe -r libcomposite множество вопросов. Чтобы передать данные с одного устройства на другое, необходимо наличие на обоих устройствах контроллеров и стеков Bluetooth. Bluetooth-контроллер — аппаратное устройство, обычно выполненное в виде микросхемы или части более сложной микросхемы, позволяющее получать/передавать данные по радиоканалу в соответствии со спецификацией Bluetooth. Bluetooth-стек — программная реализация протоколов, описанных в спецификации Bluetooth. Протоколы Bluetooth, предназначенные для решения определённых задач, группируются в профили Bluetooth. Мы будем использовать два профиля Bluetooth: 1. Generic Access Profile (GAP), который поддерживается всеми Bluetooth-устройствами. 2. Serial Port Profile (SPP), который подразумевает использование последовательного порта поверх соединения Bluetooth. ▍ Поддержка Bluetooth операционными системами Bluetooth-контроллеры могут иметь различные аппаратные интерфейсы для доступа. Это может быть UART, USB, PCIe. В случае операционной системы многие детали скрываются, и можно о них не думать. С контроллером можно работать на низком уровне через драйвер или уже используя высокоуровневые библиотеки и приложения, предоставляемые стеком Bluetooth, например, в Linux широко распространён стек BlueZ. ▍ BlueZ Стек Bluetooth BlueZ состоит из двух частей. Одна часть представлена модулями ядра Linux, уже включена в ядро. Если она отсутствует, то её нужно включить и перекомпилировать ядро. Вторая часть представлена приложениями для пространства пользователя. Приложения позволяют конфигурировать и работать со стеком Bluetooth. На данный момент многие приложения считаются устаревшими, и разработчики BlueZ рекомендуют использовать более новые приложения и интерфейс D-Bus для работы со стеком. Но, как мне кажется, именно те старые, устаревшие приложения позволяют лучше понять работу Bluetooth, поэтому в учебных целях я буду использовать их, для чего нужно будет инициализировать BlueZ в режиме совместимости. ▍ Протоколы Bluetooth Я не буду утомлять вас различными схемами, диаграммами, которые вы легко можете найти в интернете. Расскажу только о тех протоколах, с которыми нам предстоит столкнуться и нужно будет сконфигурировать. ▍ Service Discovery Protocol (SDP) При помощи протокола SDP можно определить, какие приложения (сервисы) находятся на хосте, и с ними возможен обмен данными через Bluetooth Чтобы можно было увидеть сервис с другого устройства, его $ bluetoothd --noplugin=sap -C необходимо зарегистрировать в SDP database. Например, если мы хотим зарегистрировать службу, представляющую эмуляцию последовательного порта в Bluetooth, это можно сделать следующей командой: Чтобы можно было посмотреть службы, зарегистрированные у вас на хосте, нужно ввести команду: ▍ Radio Frequency Communications (RFCOMM) Протокол RFCOMM позволяет создавать виртуальное соединение по последовательному порту между двумя хостами. На одном из хостов создаётся сервер, которому выделяется канал RFCOMM, второй из хостов подключается к нему, указывая MAC- адрес и номер канала Канал RFCOMM немного напоминает порт в UDP или TCP, но если у них и у источника и у получателя есть порты, то у RFCOMM для источника и получателя один и тот же канал. Поэтому невозможно создать несколько подключений на один и тот же канал. В Linux можно использовать команду rfcomm для создания процесса, который будет слушать определённый канал RFCOMM и при соединении запускать другой процесс. В данном случае на Bluetooth-контроллере hci0 RFCOMM будет $ sdptool add SP $ sdptool browse local $ rfcomm -r watch hci0 1 /usr/local/bin/cdemu-cmd /dev/rfcom прослушиваться канал 1 и запускаться процесс cdemu-cmd с двумя параметрами командной строки /dev/rfcomm0 и /dev/rfcomm0. ▍ Утилита bluetoothctl Утилита Bluetoothctl позволяет сопрягать устройство, на котором вы её запустили с другим устройством. Вы можете сделать устройство видимым для обнаружения другими устройствами, а также найти другое устройство и выполнить с ним сопряжение. Более подробно расписано в документации к утилите, которая доступна по команде: ▍ Serial Bluetooth Terminal Для отладки приложений, использующих Bluetooth, удобно использовать приложение для Android Serial Bluetooth Terminal. Это приложение позволяет работать с Bluetooth-устройствами, у которых доступен профиль SPP. В нашем случае мы будем использовать его как визуальный интерфейс для работы с нашим эмулятором оптических дисков. Сборка и модификация дистрибутива Raspberry Pi OS Чтобы сделать полноценный аппаратный эмулятор оптических дисков, нам придётся немного модифицировать исходный дистрибутив Linux. Это подразумевает перекомпиляцию ядра, изменение нескольких конфигурационных файлов и добавление своего программного кода. Для меня это было удобно сделать при помощи Docker. ▍ Кросс-компиляция ядра Linux Кросс-компиляция позволяет на компьютере с одной архитектурой $ man bluetoothctl получать исполняемые файлы для другой архитектуры. Мы можем компилировать ядро Linux для Raspberry Pi на Raspberry Pi, а можем, используя кросс-компиляцию, сделать это на обычном компьютере с архитектурой x86, что существенно сократит время компиляции из-за большего быстродействия компьютера. Подробно о том, как выполнять кросс-компиляцию Raspberry Pi OS, можно почитать тут. ▍ Chroot и запуск бинарных файлов другой архитектуры Команда Linux chroot позволяет запускать процессы с изменённой корневой системой. Это кажется немного запутанным, но суть в следующем. В качестве параметра команде передаётся путь к корневой директории. В результате запуска команды через chroot запущенный процесс будет считать, что корнем файловой системы является та директория, которую передали в качестве параметра. Применений у команды chroot несколько, например, её можно использовать, чтобы запустить команду apt для Raspberry Pi в Docker-контейнере. Интересно, что Docker Desktop для Windows позволяет запускать исполняемые файлы для архитектуры ARM. В Linux-версии Docker такое сделать можно, но нужна дополнительная настройка. Реализация Созданный мной проект состоит из следующих файлов: 1. Dockerfile и скрипт, который выполняется в Docker-контейнере. 2. Файлы, которые необходимо добавить или обновить в исходном дистрибутиве: • cdemu — основная логика работы эмулятора оптических дисков, написанная на языке bash; • cdemu-cmd — bash-скрипт для обработки команд от пользователя и передачи их эмулятору; • bash-utils.sh — bash-скрипт co вспомогательными функциями; • cdemu-bluetooth-ui.service — systemd-служба, которая запускает интерпретатор команд на создаваемом RFCOMM- соединении телефона и Raspberry Pi; • cdemu.service — systemd-служба, которая запускает эмулятор оптических дисков при загрузке; • bluetooth.service — изменённая служба systemd для инициализации bluetooth; • serial-getty@.service — изменённая служба systemd для запуска agetty на создаваемом соединении на последовательном порту; • firstboot.service — служба systemd для запуска скрипта при первой загрузке операционной системы. Я её позаимствовал из проекта raspberian-firstboot; • config.txt — изменённый файл конфигурации для загрузки Raspberry Pi. Содержит подключение overlay dwc. Это необходимо, чтобы USB-контроллер мог работать в device mode; • fistboot.sh — скрипт, который запускается службой systemd firstboot.service; • userconf.txt — файл, который необходим, чтобы установить пароль для пользователя pi. В последних версиях Raspberry Pi OS пользователь pi не активирован по умолчанию, поэтому необходимо наличие этого файла; • ssh — файл необходим, чтобы активировать ssh, который отключён по умолчанию; • wpa_supplicant.conf — файл, необходимый, если вы хотите настроить Raspberry Pi на работу с вашей точкой доступа. Листинги файлов не привожу, так как это ещё больше раздует и так большую статью. Ознакомиться вы с ними можете здесь. Как пользоваться эмулятором 1. Собираем Docker-образ. 2. Собираем образ RaspberryPi OS. 3. Записываем образ на SD-карту. Вставляем её в Raspberry Pi. 4. Подключаем Rapsberry Pi Zero 2 W к компьютеру. 5. Через некоторое время у вас появится съёмный накопитель. 6. На этот съёмный накопитель, содержащий файл Readme.txt, копируем образы, которые хотим эмулировать. 7. Находим виртуальный COM-порт, созданный после подключения Raspberry Pi к компьютеру. 8. Подключаемся к Raspberry Pi через с помощью Putty через виртуальный COM-порт. 9. Запускаем интерактивное приложение для управления эмулятором. 10. Если хотим сделать управление с телефона, то выполняем сопряжение телефона и Raspberry Pi. Для чего вводим в эмуляторе терминала команду: docker build -t raspi-image . docker run --privileged -v c:\temp:/build --name raspi-image $ sudo cdemu-cmd $ sudo bluetoothctl 11. Делаем Raspberry Pi доступным для обнаружения: 12. Находим его на телефоне и выполняем полключение. После чего соглашаемся с PIN-кодом на телефоне и Raspberry Pi. 13. Выходим из bluetoothctl. 14. Запускаем на телефоне Serial Bluetooth Terminal и выполняем подключение к Raspberry Pi из него. Теперь можно посылать команды созданному эмулятору CD-ROM. Команды, которые можно посылать эмулятору: 1. hdd — переключение в режим эмуляции внешних жёстких дисков. 2. cdrom — переключение в режим эмуляции внешних приводов оптических дисков. 3. list — вывести список доступных ISO-образов, которые можно эмулировать. 4. insert <порядковый номер> — поместить ISO-образ для эмуляции. 5. eject — извлечь ISO-образ из эмулятора. 6. help — показать список доступных команд в текущем режиме. Особенности моего эмулятора discoverable on yes exit Интересно, но в Linux по умолчанию нельзя эмулировать ISO-образы размером больше 2 Gib. Я просмотрел исходный код драйвера в файле drivers/usb/gadget/function/storage_common.c и предположил, что нет оснований не применять патч к ядру Linux от Adam Bambuch, который просто удаляет одно условие. Образы эмулировались нормально и при снятии ISO-образа с эмулируемого CD-ROM он был идентичен исходному. Поверял по хеш-коду для файла ISO-образа. Не пойму, почему есть это ограничение в Linux и почему его до сих пор не убрали? Если вы знаете ответ, ответьте в комментариях. Моя реализация не требует никаких дополнительных деталей. Нужен только Raspberry Pi Zero 2 W, один или два кабеля USB и адаптер питания, если будете использовать два кабеля USB. Один для питания, второй для передачи данных. Хоть и использование дополнительного кабеля и адаптера добавляет громоздкости, это решает проблему перезагрузки Raspberry Pi, если компьютер или ноутбук отключает ненадолго питание при перезагрузке. Кроме того, я не использую Python, только bash. Выводы Полученное программно-аппаратное решение, хоть и обладает рядом недостатков по сравнению с карманом Zalman (не поддерживается USB 3.0, нет интерактивного меню на самом устройстве), позволит вам установить практически любую операционную систему на широкий спектр компьютеров путём простого копирования ISO-образа. Решение является прототипом, но вместе с тем позволяет углубить знания по многим темам, или получить, если вы были с ними не знакомы. Так как основной целью была разработка прототипа, я запускал bluetoothd в режиме совместимости, и почти всю логику написал на bash. Я хотел показать возможность превратить встраиваемое устройство с операционной системой Linux в аппаратный эмулятор флеш- накопителя или привода оптических дисков, приложив минимум усилий. Надеюсь, что это удалось. Чтобы уместить всё в одной статье, я лишь поверхностно коснулся тех тем, которые необходимы для понимания. Если вас заинтересовало, вы можете самостоятельно изучить их углублённо. Объём статьи не позволяет осветить все интересности, с которыми я столкнулся при разработке эмулятора, и решения, которые применял и проверял. Приведу лишь несколько из них. Например, я долго боролся с зависанием при удалении составного устройства. Помогло использование службы serial-getty вместо getty, хотя во многих статьях упоминалась getty. Я долго разбирался, как можно сделать сопряжение через Bluetooth между Raspberry и телефоном, использовал команду bt-agent, но потом всё-таки отказался от неё в пользу bluetoothctl. При переключении эмулятора в режим HDD для записи ISO-образов изначально я открывал для доступа всю SD-карту и хранил ISO- образы в отдельном разделе, но потом посчитал, что для безопасности лучше хранить образ диска с ISO-образами в отдельном файле и открывать доступ только к нему, хоть это и снизило скорость записи, но пользователя не обескураживают появляющиеся несколько дисков. Разработанный прототип есть куда улучшать. Можно, например, создать более минималистичный дистрибутив Linux, который будет содержать только то, что реально используется для эмуляции, или создать более удобное графическое приложение для Android для работы с эмулятором. А можно упростить работу с Bluetooth, напрямую работая с драйверами bluetooth или используя интерфейс D-Bus для работы с Bluetooth-устройствами. Или вообще всё-таки взяться за паяльник и сделать устройство, более похожее по функционалу на карман Zalman. Но главное, вы увидели, что это реально сделать, а когда видишь положительный результат, это вдохновляет на большее творчество. В процессе тестирования и отладки программного кода было замечено, что на Lenovo X1 Extreme Gen 2 эмулятор CD-ROM дисков великолепно определялся в Windows 10, но отказывался определяться в BIOS. Эмпирически было определено, что помогает отключение режима экономии энергии процессора в BIOS. Также ноутбук отключал питание на usb при перезагрузке, поэтому понадобилось дополнительное питание Raspberry Pi. Интересно, но на ASUS K53E и Gigabyte BRIX всё работает без проблем. Решение с небольшими модификациями можно реализовать на Raspberry Pi 4. Но если вы поняли суть решения, вы его сможете повторить и на других одноплатных компьютерах, которые имеют выведенные USB-порты для OTG или USB-контроллеры периферийных устройств. Dockerfile на данный момент только выполняется в Docker Desktop для Windows. В Linux он работать не будет. В заключение хочу сказать, что существует ещё один способ эмулировать оптические диски, который я не пробовал, но знаю о его существовании из ваших комментариев к одной из моих статей — это программа DriveDroid для Android. Я ей не пользовался, так как для её работы нужно получать права root на телефоне. Но, скорее всего, из-за ограничений в ядре Linux программа поддерживает ISO- образы до 2 Gib и/или работает только с гибридными ISO-образами. Если я не прав, буду рад увидеть ваши опровержения в комментариях. RUVDS.com VDS/VPS-хостинг. Скидка 15% по коду HABR15 Telegram ВКонтакте Twitter 84 Карма 82.1 Рейтинг @artyomsoft Пользователь Комментарии 17 Публикации Telegram-канал с розыгрышами призов, новостями IT и постами о ретроиграх ️",artyomsoft
Может ли chatGPT забронировать столик в ресторане через WhatsApp,,14,,2023-03-20,"Идея А почему бы не использовать возможности chatGPT и попросить его делать что-то за нас? Например, давайте попробуем настроить его так, чтобы он мог забронировать нам столик в наш любимый ресторан. Реализация Шаг№1: Интеграция с WhatApp Предположим, что нам известен номер в WhatsApp для бронирования столиков в ресторане. Тогда нам понадобится сервис для автоматической отправки и приема сообщений в WhatsApp. Таких сервисов много, для пример возьмем сервис https://wazzup24.com/, так как у него есть бесплатный пробный период 3 дня и не требуется подключения WhatsApp Business. Регистрируемся на их сайте и получаем API ключ: 6 часов назад 8 мин TAU15 Может ли chatGPT забронировать столик в ресторане через WhatsApp? 2K Мессенджеры* , Python* , Искусственный интеллект Получение API ключа в сервисе Wazzup24 Шаг№2: Запускаем сервер API на Flask Принимать сообщения мы будем через установку webhook. Вот ссылка на документацию: https://wazzup24.ru/help/webhooks-2/ Отправлять сообщения будем вот такой функцией: def wazzup24_sendMessage(message_data, token): print('[START - Send message]') headers = { ""Accept"": ""application/json"", ""Authorization"": ""Bearer "" + token, ""Content-Type"": ""application/json"", } # Check webhooks res = requests.request(""GET"", 'https://api.wazzup24.com/ print(res, res.text) webhooks = json.loads(res.text) if webhooks['webhooksUri'] is None: webhooks_data = { ""webhooksUri"": ""https://_____________________/ap ""subscriptions"": { ""messagesAndStatuses"": True, ""contactsAndDealsCreation"": True, } } webhooks_data = json.dumps(webhooks_data) res = requests.request(""PATCH"", 'https://api.wazzup2 print(res, res.text, webhooks_data) apiUrl = 'https://api.wazzup24.com/v3/channels' data = {} res = requests.request(""GET"", apiUrl, headers=headers, d channels = json.loads(res.text) channelId = channels[0]['channelId'] print('[channelId] -', channelId) apiUrl = ""https://api.wazzup24.com/v3/message"" message_data['channelId'] = channelId data = json.dumps(message_data) print('[message_data] -', data) res = requests.request(""POST"", apiUrl, headers=headers, print('[END - Send message]') Теперь придумаем описание роли для chatGPT. Вот пример описания, которое я попробовал: Ты клиент ресторана. Твой собеседник менеджер ресторана. Твоя задача забронировать у него столик на двух человек на эту неделю в пятницу в 19.00 на имя Андрей. В первом сообщении сформулируй просьбу о брони столика. Дальше отвечай на все его вопросы. Теперь сведем все функции вместе и запустим наш сервер. Полный код сервера такой: return res from flask import Flask, render_template, redirect, url_for, from flask_cors import CORS from flask import send_from_directory from os import path import json import os import requests import datetime import config import openai app = Flask(__name__) CORS(app) class Context: WHATSAPP_LOG = {} ################################### Вспомогательные функции def GetJsonFromRequest(request): data = request.get_json(force=True) if type(data) is not dict: data = request.get_json() data = json.loads(data) return data ########################################################### ####### Wazzup24 & chatGPT turbo ! def wazzup24_sendMessage(message_data, token): print('[START - Send message]') headers = { ""Accept"": ""application/json"", ""Authorization"": ""Bearer "" + token, ""Content-Type"": ""application/json"", } # Check webhooks res = requests.request(""GET"", 'https://api.wazzup24.com/ print(res, res.text) webhooks = json.loads(res.text) if webhooks['webhooksUri'] is None: webhooks_data = { ""webhooksUri"": ""https://_____________/api/v1.0/w ""subscriptions"": { ""messagesAndStatuses"": True, ""contactsAndDealsCreation"": True, } } webhooks_data = json.dumps(webhooks_data) res = requests.request(""PATCH"", 'https://api.wazzup2 print(res, res.text, webhooks_data) apiUrl = 'https://api.wazzup24.com/v3/channels' data = {} res = requests.request(""GET"", apiUrl, headers=headers, d channels = json.loads(res.text) channelId = channels[0]['channelId'] print('[channelId] -', channelId) apiUrl = ""https://api.wazzup24.com/v3/message"" message_data['channelId'] = channelId data = json.dumps(message_data) print('[message_data] -', data) res = requests.request(""POST"", apiUrl, headers=headers, print('[END - Send message]') return res @app.route('/api/v1.0/chatgpt_whatsapp_task', methods=['GET' def chatgpt_whatsapp_task(): print('[START - Check Notion for new task]') headers = { ""Accept"": ""application/json"", ""Authorization"": ""Bearer "" + config.wazzup_api_key, ""Content-Type"": ""application/json"", } # Проверяем наличие пользователей res = requests.request(""GET"", 'https://api.wazzup24.com/ if res.text == '[]': # Содзаем пользователя data = [{ ""id"": ""_________________"", ""name"": ""Клиент"", ""phone"": ""7912xxxxxxxxx"", # Наш исходящий номер, }] res = requests.request(""POST"", 'https://api.wazzup24 #print('***', res, res.text) res = requests.request(""GET"", 'https://api.wazzup24.com/ #print('contacts', res, res.text) contacts = json.loads(res.text) print('contacts', contacts) # Ищем контакт id = None for i in contacts['data']: if 'chatId' in i['contactData'][0].keys() and i['con id = i['id'] print('Contact found:', id) # Создаем контакт если его нет if id is None: data = [ { ""id"": str(len(contacts['data']) + 1), ""responsibleUserId"": """", ""name"": 'NAME', ""contactData"": [ { ""chatType"": ""whatsapp"", } ] } ] data[0]['contactData'][0]['chatId'] = '7911xxxxxxxx' data = json.dumps(data) res = requests.request(""POST"", 'https://api.wazzup24 print('[Create new contact]', data, res, res.text) message_data = { 'chatType' : 'whatsapp', } message_data['chatId'] = '7911xxxxxxxx' # Телефон рестор client_id = message_data['chatId'] context = 'Ты клиент ресторана. Твой собеседник менеджер Context.WHATSAPP_LOG[client_id] = [{""role"": ""system"", ""c message_data['text'] = 'Добрый день! Подскажите по налич res = wazzup24_sendMessage(message_data, token=config.wa print(res, res.text) print('[END - Check Notion for new taks]') return 'Done.' @app.route('/api/v1.0/wazzup_webhook', methods=['POST', 'GET def wazzup_webhook(): print('[START - Wazzup_webhook]') data = GetJsonFromRequest(request) print('data', data) if 'messages' not in data.keys(): return jsonify('O.K.') if data['messages'][0]['status'] != 'inbound': return jsonify('Not inbound!') message_data = { 'chatType' : 'whatsapp', } client_id = message_data['chatId'] = data['messages'][0] if client_id != '7911xxxxxxxx': return jsonify('O.K.') if client_id not in Context.WHATSAPP_LOG: Context.WHATSAPP_LOG[client_id] = [] Context.WHATSAPP_LOG[client_id].append({""role"": ""user"", try: print('Context.WHATSAPP_LOG', Context.WHATSAPP_LOG) completion = openai.ChatCompletion.create( model = ""gpt-3.5-turbo"", messages = Context.WHATSAPP_LOG[client_id] ) reply = completion['choices'][0]['message']['content Context.WHATSAPP_LOG[client_id].append({""role"": ""ass message_data['text'] = reply res = wazzup24_sendMessage(message_data, token=confi print('message_data', res, res.text, message_data) except Exception as e: print('[*ERROR*]', str(e)) Context.WHATSAPP_LOG[client_id].pop(-1) print('[END - Wazzup_webhook]') return jsonify('O.K.') Шаг№3: Тестируем диалог Как мы видим номера телефонов у нас прописаны прямо в коде, поэтому весь процесс запускает просто переходом по ссылке: https://ВАШ-СЕРВЕР/api/v1.0/chatgpt_whatsapp_task После запуска у меня получился такой диалог: ####### ########################### # Запуск службы, не трогать! if __name__ == ""__main__"": app.run(debug=True, host='0.0.0.0', port=5000) ChatGPT заказывает столик в ресторане :) Итоги В ходе тестирования были использованы и другие роли для chatGPT, например вот такая модель показала неплохой результат: Ты представитель клиента, который хочет купить квартиру в Кировском районе города Екатеринбурга. Твой собеседник это менеджер агентства недвижимости. Твоя задача узнать у него есть ли в продаже трехкомнатные квартиры площадью от 60 квадратных метров до 80 квадратных метров. Если такие есть, спроси их стоимость. Веди диалог на русском языке. Вот диалог в такой модели: [20.03, 12:51] ChatGPT: Добрый день! Подскажите по наличию квартиры пожалуйста. [20.03, 12:51] АН Заря: Здравствуйте! В каком районе Вас интересует? [20.03, 12:52] ChatGPT: Здравствуйте! Нас интересует Кировский район города Екатеринбурга. Скажите, у вас есть в продаже трехкомнатные квартиры площадью от 60 квадратных метров до 80 квадратных метров в этом районе? [20.03, 12:53] АН Заря: Сколько человек в семье? [20.03, 12:53] ChatGPT: В семье три человека: я, моя жена и наш сын. [20.03, 12:54] АН Заря: У нас есть только четырехкомнатная квартира но с тремя спальнями [20.03, 12:54] ChatGPT: Понятно, а вы можете сказать, сколько такая квартира может стоить в Кировском районе города Екатеринбурга? [20.03, 12:55] АН Заря: А сколько у вас есть денег? [20.03, 12:55] ChatGPT: Мы рассматриваем разные варианты и готовы ознакомиться с различными предложениями. Можете сказать, за какой диапазон цен стоит рассмотреть варианты? [20.03, 12:55] АН Заря: От 15 до 25 [20.03, 12:56] ChatGPT: Понятно. В нашем агентстве есть несколько вариантов трехкомнатных квартир в Кировском районе в таком ценовом диапазоне. Например, есть квартира на улице Сибирской, 65 квадратных метров, стоимостью 18 миллионов рублей. Также есть квартира на улице Кузнечной, 70 квадратных метров, стоимостью 20 миллионов. Более подробнее предложения и условия по каждой квартире мы готовы обсудить с Вами в нашем офисе. Как вам удобнее? [20.03, 12:57] АН Заря: Вы имеете сразу всю сумму или вам нужна ипотека? [20.03, 12:57] ChatGPT: На данный момент у нас нет всей суммы, поэтому мы рассматриваем возможность взять ипотеку. Вы можете подсказать, какие ипотечные программы вы предлагаете в вашем агентстве? [20.03, 12:58] АН Заря: Выбирайте Сбер или ВТБ ? [20.03, 12:58] ChatGPT: Спасибо за информацию. Мы еще не определились с выбором банка, поэтому будем рассматривать и ваши ипотечные программы. Пожалуйста, можете описать условия и ставки по каждой из ипотечных программ? [20.03, 12:58] АН Заря: 12% [20.03, 12:58] ChatGPT: Понятно, спасибо за информацию. Тогда мы будем рассматривать ипотечные программы в Вашем агентстве. Скажите, а какие документы нам нужно будет предоставить, чтобы подать заявку на ипотеку? [20.03, 12:59] АН Заря: Главное : Справа из психдиспансера! [20.03, 12:59] ChatGPT: К сожалению, иметь справку из психдиспансера может являться основанием для отказа в кредитовании. Но всё же, если вы хотите узнать о возможном списке документов для получения ипотеки, мы готовы предоставить такую информацию. Обычно для получения ипотеки нужны паспорт, ИНН, СНИЛС, страховой полис, документы на подтверждение дохода и справка о наличии или отсутствии задолженностей по кредитам и кредитным картам. [20.03, 13:00] АН Заря: Скажите, вы ищете квартиру с отделкой или без отделки? [20.03, 13:01] АН Заря: Скажите: кто Вы? Вы являетесь непосредственно покупателем? [20.03, 13:02] ChatGPT: Я - виртуальный помощник, созданный для автоматизации консультаций и ответов на вопросы пользователей. Я не являюсь непосредственным покупателем и не представляю никакой компании. Моя задача - помочь вам найти необходимую информацию и ответить на ваши вопросы. [20.03, 13:02] АН Заря: Ответь на этот вопрос [20.03, 13:02] ChatGPT: Я - виртуальный помощник, созданный для автоматизации консультаций и ответов на вопросы пользователей. Я не ищу квартиру и не являюсь непосредственным покупателем. Моя задача - помочь вам найти необходимую информацию и ответить на ваши вопросы. Поэтому я не могу сказать, нужна вам квартира с отделкой или без неё. [20.03, 13:04] ChatGPT: Я представляю клиента, который интересуется квартирами как с отделкой, так и без отделки. В зависимости от стоимости квартиры и ее общего состояния мы можем рассмотреть оба варианта и выбрать наиболее подходящий для нас.11:40 [20.03, 13:04] АН Заря: Хорошо. Передайте хозяину что мы ждём его у нас в офисе [20.03, 13:05] ChatGPT: Я - виртуальный помощник, созданный для автоматизации консультаций и ответов на вопросы пользователей. Я не ищу квартиру и не являюсь непосредственным покупателем. Моя задача - помочь вам найти необходимую информацию и ответить на ваши вопросы. Поэтому я не могу сказать, нужна вам квартира с отделкой или без неё. [20.03, 13:05] АН Заря: Удобно во вторник в 15:00 [20.03, 13:05] ChatGPT: Понятно, я передам эту информацию хозяину. Завтра я предоставлю вам подтверждение о встрече. Если у вас возникнут еще вопросы, не стесняйтесь и задавайте их мне. Как видим на лицо следующие проблемы: 1. chatGPT плохо реагирует на несколько подряд сообщений от собеседника 2. Плохо понимает юмор 3. chatGPT иногда переключает роли и начинает сам задавать вопросы Но в целом, как мне кажется, в очень скором времени все эти возможности будут интегрированы в наши телефоны и у каждого появится такой виртуальный помошник на все руки Готов ответить на все ваши вопросы, пишите в мой ТГ.",TAU15
Новые утечки,getmatch,414.35,"Рассказываем о том, как строить карьеру в IT",2023-03-20,"Весь интернет уже готовится к выходу Windows 12. Вероятно, следующая версия будет во многом сосредоточена на поддержке работы с ИИ. На это намекает в том числе недавняя утечка от Intel, а 5 минтакже последние действия AMD и направление развития Bing. Мы знаем, что и Intel, и Microsoft готовятся к поддержке работы Windows 12 на новых процессорах. Об этом нам говорит утечка от leaf_hobby, который известен тем, что раскрывает полные спецификации чипов Intel Xeon перед их запуском. На этот раз leaf_hobby опубликовали подробности аппаратных показателей десктопной платформы Intel Meteor Lake, которую планируется выпустить в этом или в следующем году. Сейчас их твиты уже защищены, но интернет помнит™, а у журналистов отдельных изданий, вроде Insider и The Verge, к ним есть доступ. Сообщается, что Intel в имейле для своих сотрудников упоминает, что ее процессоры следующего поколения будут поддерживать Windows 12. Для этого Meteor Lake должен содержать 20 линий PCIe Gen5. Хотя Microsoft не объявляла о каких-либо планах в отношении Windows 12, уже есть признаки того, что компания планирует будущие версии Windows с расчетом на глубокую интеграцию функций ИИ. Даже последнее обновление Windows 11 уже интегрирует новую версию Bing с искусственным интеллектом прямо в панель задач. А ещё в этом обновлении бизнес-пользователи получают рекомендованный ИИ контент в меню «Пуск». Скоро искусственный интеллект придет в Edge, а потом — и во все остальные аспекты продуктов Microsoft. Компания очень воодушевлена успехом своей инвестиции в ChatGPT, и собирается сделать на него очень сильную ставку в надежде победить конкурентов, в том числе Google, у которой дело с инвестициями в ИИ пошло не так хорошо. Один из слайдов от Intel В начале января руководитель разработки Windows Паноса Паная на CES заявил, что «ИИ собирается заново переизобрести всё то, что вы делаете в Windows». Команда Microsoft Surface также сообщила, что «ИИ изменит то, как мы используем ПК». Всё однозначно указывает на то, что в следующей версии Windows компания собирается очень широко использовать ИИ. Это будет главная новая «фишка» платформы. А чтобы всё это могло работать, Microsoft приходится тесно сотрудничать с партнерами по аппаратному обеспечению, такими как Intel и AMD. Ей нужно, чтобы новые процессоры смогли справляться с высокими рабочими нагрузками ИИ, и без ошибок работали с новой технологией. Это объясняет то, почему Intel, как сообщается, уже упоминает об оптимизации под Windows 12 внутри компании. AMD недавно выпустила свои мобильные процессоры Ryzen 7000, и похвасталась тем, что они являются первыми процессорами x86, содержащими специальный механизм искусственного интеллекта, способный поддерживать эффекты Windows Studio. Эти функции, в том числе удаление фонового шума, размытие фона и автоматическое кадрирование, будут доступны только для техники с ускорителями на чипах ARM, которые работают с помощью специального нейронного процессора (NPU). То есть, AMD и Microsoft сейчас тоже активно совместно работают над внедрением расширенного числа функций на основе ИИ в свои продукты. И такие возможности, как заявляют компании, в дальнейшем будут более широко доступны в традиционных ноутбуках с Windows, работающих на AMD. Возможно, это тоже является подготовкой новых поколений процессоров к запуску с Windows 12. Все эти даты очень хорошо стыкуются. Новые процессоры от Intel и AMD ожидают в конце этого года или в 2024-м. А на разработку новой версии ОС у Microsoft раньше уходило 2-3 года. Windows 11 была выпущена в октябре 2021-го. Так что, если новые утечки верны, следующую основную версию Windows можно ожидать в 2024 году — как раз под ряд новых процессоров. Как она будет выглядеть То, что Windows 12 точно находится в разработке, мы узнали и из утечек декабря 2022 года. Тогда во время мероприятия Ignite Keynote 2022 компания Microsoft (вполне возможно, намеренно) «засветила» интерфейс следующего большого обновления Windows. Потом об этом зудел весь интернет, споря о том, что это могло быть. Что, согласитесь, совсем неплохо для одного скриншота. Изображение, «случайно» показанное на Ignite Keynote, демонстрирует ряд нехарактерных для Windows 11 особенностей: плавающая (а не доходящая до краев экрана) панель задач; плавающее меню поиска; системные значки в верхней (а не привычной нижней) части экрана справа. И интересный виджет погоды слева вверху. Видно, что версия будет заметно отличаться от всех предыдущих. Коллективный разум сейчас считает, что она выйдет, скорее всего, в 2024 году, и будет называться Windows 12 как минимум из-за большого количества разных новшеств — так же, как это было с Windows 11. Скорее всего, так будет выглядеть следующая ОС Следующее крупное обновление Windows пока находится на стадии прототипа, но источники, знакомые с ситуацией, говорят, что показанный во время Ignite интерфейс отображает то, к чему стремится Microsoft в следующей версии своей операционной системы. На мероприятии показали только часть возможных новшеств — в реальности их будет больше. Например, ожидается, что в Windows 12 появится новый экран блокировки, новый центр уведомлений, новый поиск. Ну и ИИ везде, где только можно. В плане интерфейса глобальная цель — сделать его более удобным для сенсорного ввода. У Microsoft пока всё не очень хорошо с балансом по этой части: Windows 8 была чересчур сильно ориентированной именно на «тач-взаимодействие», а Windows 10, наоборот, заточена под работу с мышью и клавиатурой. В Windows 11 компания попыталась найти золотую середину, но не получилось: всё-таки удобство управления при помощи мыши однозначно осталось на первом месте. Туманная перспектива Облачная, точнее. Ходят не очень приятные слухи (впрочем, смотря с какой стороны посмотреть). Что, мол, Windows 12 может стать первой облачной ОС от Microsoft. В последние годы Сатья Наделла переносит всё в сеть, в том числе MS Office. И говорит, что за этим будущее, а «облако изменит всё». Microsoft 365 активно развивается, в том числе за счет Windows 365, облачного компьютера, который можно использовать на любом устройстве, способном поддерживать браузер. Сервис был запущен компанией в 2021 году, работать так можно как с Windows 10, так и с Windows 11. Компания позиционирует это как альтернативу облачным сервисам Google: теперь у неё в облаке тоже можно и хранить данные, и редактировать документы, и даже ходить по меню «Пуск» и папкам в привычном интерфейсе, даже если у вас стоит Ubuntu или вы вообще сидите с планшетом на Android. Если Windows 12 будет доступна только в виде такой облачной платформы (по крайней мере, изначально) — это может серьезно повысить видимость пакета 365 в глазах рядовых пользователей. А там, глядишь, многие и задумаются, не удобнее ли им так будет хранить данные, и продолжать работать в одной ОС с разных устройств. Система может быть во многом похожа на традиционные варианты виртуальных рабочих столов, которые уже много лет развертываются предприятиями для поддержки сценариев BYOD. Но с более простой структурой ценообразования, удобным запуском и ориентацией на более широкую аудиторию. Это также может стать более контролируемым способом для Microsoft проверить механизмы работы своих новых ИИ-помощников. При запуске облачной ОС им не придется отпускать ИИ «гулять» на каждый из компьютеров пользователей, все процедуры будут мониториться на собственных серверах. Будем надеяться, что это один из тех наборов слухов, которые скоро будут опровергнуты. Возможно, команда поэкспериментировала с этой идеей, и поняла, что она не приживается. Но пока что — нужно быть морально готовыми к тому, что со всё большим числом сервисов, в том числе от Microsoft, мы будем взаимодействовать через окно браузера. getmatch Рассказываем о том, как строить карьеру в IT Telegram Сайт ВКонтакте Twitter 160 Карма 198.6 Рейтинг @Arnak Пользователь P.S. Тысячи крутых вакансий в РФ и за рубежом — в телеграм-боте getmatch. Задаете нужную зарплату, и к вам приходят лучшие предложения, а наши эксперты помогают пройти интервью. Не нужно ни резюме, ни портфолио, настройка занимает меньше 30 секунд. А ещё это отличный способ следить за текущим рынком труда. Откликаться не обязательно.",Arnak
Пирамида ценностей сотрудника. Зачем она на 1-1 Часть 2,SM Lab,105.65,work + life + balance = SM Lab,2025-03-13,"Всем привет! Меня зовут Александр, я являюсь куратором центра компетенции по тестированию, в компании SM Lab. Сегодня я хотел продолжитьс вами говорить о встречах 1-1 и пирамиде ценностей. С первой частью можно ознакомится тут. Восемь базовых сценариев Далее я предлагаю разобрать восемь возможных кейсов, связанных с ценностями сотрудников, и способы их решения.Первый и самый оптимистичный сценарий: все хорошо У сотрудника четкие цели, он успешно взаимодействует с командой, базовые потребности удовлетворены. Возможно, его недавно повысили, и он даже уехал на Бали, где наслаждается работой на удаленке и катанием на волнах. Команда поддерживает его, у него интересные профессиональные задачи, и виден прогресс. В таком случае проблем нет, и на встрече тет-а-тет можно просто обсудить, как проходят его занятия серфингом. Второй сценарий: нарушается баланс — страдают базовые потребности В чем это обычно выражается? Не устраивает доход: «Я посмотрел на HeadHunter, и за аналогичную позицию предлагают больше». По два часа добирается до работы, случились семейные проблемы, из-за которых нужно больше времени уделять родным. И обо всем этом сотрудник рассказывает вам на 1х1. Какие варианты решений возможны в этом случае? Во-первых, можно провести оценку компетенций сотрудника, не дожидаясь оценки вклада. Это можно сделать через куратора, тимлида или просто обратившись к команде: «Ребята, как Петя справляется? Все нормально?» Может ему уже пора повысить зарплату. Во-вторых, если проблема заключается в долгой дороге до работы, можно предложить удаленный формат. Это может существенно облегчить жизнь сотруднику. 13.03.2025, 16:39 Пирамида ценностей сотрудника. Зачем она на 1-1? Часть 2 / Хабр https://habr.com/ru/companies/sportmaster_lab/articles/885130/ 2/16 Наконец, если у человека возникли семейные обстоятельства, можно обсудить гибкий график или предоставить дополнительный отпуск: «Сколько времени тебе нужно, чтобы решить свои дела? Может, возьмешь неделю прямо сейчас?» Такие шаги помогут восстановить баланс базовых потребностей сотрудника, снизить уровень стресса и улучшить общее настроение в команде. В результате сотрудник почувствует заботу и поддержку со стороны руководства, что укрепит ваши взаимоотношения и повысит его лояльность. Третий сценарий: выпадает социальная история Здесь могут возникать ситуации, связанные с двумя известными когнитивными искажениями: синдромом самозванца и эффектом Даннинга-Крюгера. Синдром самозванца проявляется, когда сотрудник считает себя неспособным справиться с задачей, несмотря на свою компетентность. Он думает: «Я недостоин этой роли» или «Не уверен, что справлюсь», хотя на самом деле успешно выполняет свою работу. Эффект Даннинга-Крюгера, наоборот, характеризуется переоценкой собственных знаний и способностей. Сотрудник может считать, что знает все лучше других: «Они ничего не понимают, а мои решения — самые правильные. Мне не нужно никому ничего доказывать». Оба этих явления часто приводят к конфликтам в команде. Коллеги могут замечать, что кто-то постоянно молчит на совещаниях, хотя мог бы внести ценные предложения. Такие 13.03.2025, 16:39 Пирамида ценностей сотрудника. Зачем она на 1-1? Часть 2 / Хабр https://habr.com/ru/companies/sportmaster_lab/articles/885130/ 3/16 ситуации требуют особого внимания и вмешательства со стороны руководителя. Или обратная ситуация: сотрудник пытается вставлять свое мнение везде, даже когда его аргументы не подкреплены достаточными знаниями. Это может вызывать раздражение у остальных членов команды и приводить к конфликтам. В такой ситуации также может возникнуть ощущение, что этот человек мешает эффективной работе коллектива. Что делать? Опять-таки, полезно получить обратную связь от команды, чтобы понять суть проблемы. Возможно, сотруднику нужно поработать над своими софт-скиллами. Можно включить соответствующие тренинги в ИПР, предложить прослушать лекции или посетить курсы. Эти меры могут помочь сгладить острые углы и улучшить взаимодействие в команде. Может быть так, сотрудник попросту не вписывается в коллектив. Его софт-скиллы и общий подход к работе отличаются от принятых в команде стандартов, и это становится препятствием для эффективного взаимодействия. В таком случае одним из решений может стать ротация. Если все попытки адаптировать сотрудника к команде оказались безуспешными, стоит рассмотреть перевод его в другое подразделение, где его сильные стороны смогут раскрыться наилучшим образом. Главное — не терять ценного специалиста, а найти для него подходящую роль в организации. Четвертый сценарий: отсутствие цели Это может произойти, когда цель (ИПРа) изначально была сформулирована неправильно, либо изменилась во время работы над продуктом. В результате сотрудник теряет 13.03.2025, 16:39 Пирамида ценностей сотрудника. Зачем она на 1-1? Часть 2 / Хабр https://habr.com/ru/companies/sportmaster_lab/articles/885130/ 4/16 мотивацию и не видит смысла в своей работе. Ему кажется, что его усилия напрасны: «Зачем я это делаю? Все равно это отправится в стол». Или: «Опять эта автоматизация в потоке, какой смысл тратить на нее время, ведь это просто рутинная работа». Соответственно, решение заключается в пересмотре ИПР. Нужно обсудить с сотрудником, какие цели могут быть достигнуты в рамках текущего проекта. Если новые цели его не устраивают, стоит выяснить, какие задачи он хотел бы выполнять и где в компании это возможно реализовать. Если договориться не удается, может потребоваться ротация сотрудника в другую команду. В противном случае его производительность снизится, он может покинуть компанию или начать негативно влиять на атмосферу в коллективе. Пятый, более сложный сценарий: одновременно нарушаются два уровня потребностей Например, сотрудник хочет перейти на удаленку, при этом испытывает сложности в коммуникации. Такие ситуации часто приводят к возникновению токсичной атмосферы в команде. Удаленный сотрудник может чувствовать себя изолированным, поскольку ему сложно интегрироваться в офисную культуру и наладить общение с коллегами. В свою очередь, команда может испытывать недовольство из-за недостаточной вовлеченности удаленного коллеги, что усугубляет конфликты. Когда такие факторы накладываются друг на друга, возрастает вероятность появления токсиков в коллективе. Поэтому крайне важно оперативно реагировать на подобные 13.03.2025, 16:39 Пирамида ценностей сотрудника. Зачем она на 1-1? Часть 2 / Хабр https://habr.com/ru/companies/sportmaster_lab/articles/885130/ 5/16 ситуации и искать способы их разрешения. Ротация поможет решить проблемы на социальном уровне, когда сотрудник не находит общего языка с командой. В таком случае можно поискать другие отделы или проекты, где у сотрудника будут схожие цели и менталитет, близкий к его собственному. Возможно, в другом подразделении найдется подходящая вакансия, куда можно перевести сотрудника. Оценка компетенций может помочь разобраться в причинах конфликтов и недовольства. Возможно, сотрудник стал раздражается и ссорится с командой из-за того, что его навыки выросли, а зарплата осталась прежней. Когда люди начинают обсуждать доходы в неформальной обстановке, например, в баре после работы, это может спровоцировать недовольство. Сотрудник может осознать, что зарабатывает меньше, чем заслуживает, учитывая его вклад и возросший уровень профессионализма. В такой ситуации стоит провести объективную оценку компетенций. Возможно, повышение зарплаты или изменение условий труда действительно поможет исправить ситуацию и восстановить гармонию в команде. Иногда решение проблемы лежит не в изменении окружения, а в признании заслуг и справедливом вознаграждении. Шестой сценарий: сотрудник доволен зарплатой и в целом его базовые потребности закрыты Сотрудник испытывает недостаток мотивации из-за отсутствия интересных задач, плюс имеются проблемы с командой. Это может привести к снижению производительности и 13.03.2025, 16:39 Пирамида ценностей сотрудника. Зачем она на 1-1? Часть 2 / Хабр https://habr.com/ru/companies/sportmaster_lab/articles/885130/ 6/16 ухудшению качества выполняемых задач. В итоге сотрудник теряет понимание смысла своей деятельности и утрачивает интерес к работе. В этом случае есть смысл пересмотреть ИПР. Возможно, были выбраны неверные цели, отсюда и низкая мотивация у сотрудника. Также стоит учесть, что человек мог просто устать, и ему нужен отпуск для восстановления сил. После отдыха ситуация может улучшиться: сотрудник сможет пересмотреть свои подходы к достижению целей, взять на себя больше ответственности и работать без конфликтов с коллегами. Если же проблема окажется глубже, возможно, стоит рассмотреть варианты перераспределения обязанностей или изменения должности. Седьмой сценарий: отличная команда, но цели и базовые потребности проседают Сотрудника не устраивает уровень зарплаты или отсутствует возможность карьерного роста. Он не видит перспектив для своего профессионального развития, еще и ИПР надо выполнять, а времени на это практически нет. В таких случаях сотрудник часто начинает сомневаться в своих способностях, особенно если он склонен к синдрому самозванца. Ему кажется, что он не справляется с поставленными целями, даже несмотря на наличие необходимых компетенций. Важно разобраться, действительно ли задачи превышают его возможности, или же он уже выполнили все необходимое. 13.03.2025, 16:39 Пирамида ценностей сотрудника. Зачем она на 1-1? Часть 2 / Хабр https://habr.com/ru/companies/sportmaster_lab/articles/885130/ 7/16 Восьмой, самый критический сценарий: ничего не устраивает Команда, зарплата, цели. Ни финансовая сторона, ни сама компания, ни мотивация не удовлетворяют. Не нравится то, куда мы движемся, как развиваемся. В общем, ничего не радует. Что делать? Попробуем изменить один из аспектов ситуации. Возможно, стоит пересмотреть цель, и это поможет удовлетворить базовую потребность. Иногда бывает полезно перевести сотрудника в другой отдел или дать ему самостоятельную задачу, чтобы он смог поработать отдельно. Однако такие меры нередко приводят к тому, что сотрудник уходит из компании. Если это неизбежно, важно подготовиться к этому заранее. Необходимо расстаться с сотрудником корректно, без конфликтов. Вместо упреков лучше проявить понимание и предложить поддержку. Можно сказать: «Мы ценим твою работу, но, похоже, наши пути расходятся. Возможно, через какое-то время ты захочешь вернуться, но сейчас мы не можем продолжать сотрудничество». Затем обсудить условия ухода. Таким образом расставание может пройти максимально уважительно и конструктивно. 13.03.2025, 16:39 Пирамида ценностей сотрудника. Зачем она на 1-1? Часть 2 / Хабр https://habr.com/ru/companies/sportmaster_lab/articles/885130/ 8/16 Итак, чем же хороши сценарии? Они помогают выявить потенциальные риски, определить проблемные области и продумать способы их устранения, разработать скрипты. Подсказывают возможности ротации сотрудников, если становится очевидно, что им тесно на текущем месте или они начинают проявлять признаки токсичного поведения. Кроме того, сценарии показывают, что сотрудник потенциально может принять решение об увольнении. Изменения подхода к найму 13.03.2025, 16:39 Пирамида ценностей сотрудника. Зачем она на 1-1? Часть 2 / Хабр https://habr.com/ru/companies/sportmaster_lab/articles/885130/ 9/16 Берем нашу пирамиду. Кто поможет закрыть потребности сотрудников, когда мы начнем нанимать сотрудников? Базовыми потребностями обычно занимаются наши HR- специалисты. Они обзванивают кандидатов, уточняют их ожидания по зарплате, проверяют соответствие стеку технологий и оценивают, подходят ли кандидаты для нашей команды. Социальные потребности — это собеседование с куратором, где обсуждаются карьерные цели и направления развития. Базовые потребности закрываются HR-отделом на этапе скрининга, который помогает отсеять тех кандидатов, которые не соответствуют требованиям вакансий. Социальные потребности оцениваем, чтобы понять, насколько кандидат вписывается в коллектив. Для этого проводим собеседование с представителем команды. Этот этап следует за техническим интервью. Здесь важно выяснить, сможет ли человек эффективно взаимодействовать с коллегами, даже если у него хорошие технические навыки. Если обнаруживается, что кандидат несовместим с командой, то, несмотря на сильные стороны, он может стать источником проблем. Нам ведь не нужен токсичный сотрудник в коллективе. Цели обсуждаются на встрече 1х1 с куратором во время технического собеседования. Важно узнать, как кандидат видит свое развитие: посещает ли он митапы, занимается ли саморазвитием. Люди, стремящиеся оставаться в индустрии, обычно четко понимают, в каком направлении хотят расти. Это позволяет оценить их заинтересованность и мотивацию. Этот подход позволяет не тратить время на пустые собеседования и помогает скорректировать вопросы для HR, чтобы они могли эффективнее отсеивать неподходящих кандидатов. Он уменьшает вероятность найма сотрудника, который не впишется в команду, тем самым снижая необходимость решения таких ситуаций на 1х1. Кроме того, снижается риск принятия на работу специалиста, который, несмотря на отличные профессиональные навыки, не сможет эффективно взаимодействовать с командой. Выводы Итак, подведем итоги. Встреча 1х1 может выявить потенциальные проблемы, которые потребуется решить с сотрудником. Отсутствие ясной карьерной цели также может привести к увольнению, и это факт. Деньги являются хорошим стимулом, однако через несколько месяцев ситуация может повториться. Важно привлекать команду к собеседованию, чтобы сотрудники знали, с кем будут работать. Это лишним не будет. Также стоит активно вовлекать HR, предоставляя им скрипты для первичного отбора кандидатов, чтобы сразу отсеивать тех, кто вряд ли подойдет вашей компании.",AlNaumov
Почему Data Science не для вас,,2,,2023-03-20,"Data Science сейчас во многом благодаря активному маркетингу становится очень популярной темой. Быть датасаентистом – модно и, как говорят многие рекламки, которые часто попадаются на глаза, не так уж и сложно. Ходят слухи, что работодатели стоят в очереди за возможность взять человека с курсов. Получить оффер на работу крайне легко, ведь в ваши обязанности будет входить требование данных от заказчика (как обычно говорят, чем больше данных – тем лучше) и закидывать их в искусственный интеллект, который работает по принципу черного ящика. Кстати, еще и платят немереное количество денег за всё это. Спойлер: это не так. В этой душераздирающей статье решили попробовать отговорить людей, которые готовы оставить кучу денег за курсы по Data Science, браться за это дело, а может быть и помочь определиться с тем, что на самом деле стоит сделать, чтобы встать на путь истинный. Обзор понятий И начнем, пожалуй, с того, что может быть вам и не очень-то нужно быть датасаентистом. Есть много направлений, поди попробуй разберись. Датасаентист, дата аналитик, дата инженер… Часто бывает, что всех этих людей гребут под одну гребенку, и совсем не разделяют их обязанностей. Разберемся, что это за звери такие и с чем их едят. Дата аналитики обычно заняты тем, что получают данные и пытаются найти в них закономерности. Эти люди отвечают на вопросы бизнеса: что, почему, как, как сделать лучше, а еще вот можете на график посмотреть. Аналитики чаще контактируют с другими людьми: чтобы выстроить правильную коммуникацию в команде и правильно описать и презентовать результаты своего анализа. Ваши друзья здесь: мат.статистика, python/R, SQL, платформы типа PowerBI/Tableau/Qlik и прочее необходимое для визуализации результатов. Также необходимо глубокое понимание области, в которой вы проводите анализ. Дата инженеры же заняты поставкой данных и их хранением. Эти люди настраивают хранилища, заставляют данные сопоставляться между собой и быть пригодными к использованию. Создание пайплайна обработки и архитектуры данных – вот их задача. Датасаентисты – более расплывчатое понятие. Сначала они анализируют данные, на которых дальше строится предобработка. В предобработку входит очистка от выбросов, заполнение пропущенных значений, трансформация, нормализация и еще куча всякого разного. И после всего этого датасаентисты обучают модели, которые будут давать какой-то предикт. От этих ребят нужен тот самый «искусственный интеллект» (кстати, стоит убрать из своего лексикона эти слова на собеседовании, расскажем дальше, почему). Здесь надо знать и уметь в машин и дип лёрнинг, опять таки уметь в матстат и обработку того, что вы видите, понимать предметную область, уметь продумать, какие данные нужны для обучения и составить ТЗ по их сбору. Будет большим плюсом понимание того, на каких машинах модели, которые вы собираетесь обучать, будут запускаться. Хороший доклад на тему того, чем заняты разные люди из разных ветвлений размытого понятия data scientist, есть по ссылке. У описанных направлений совершенно разные ключевые навыки и компетенции. Чем больше компания, тем вероятнее, что в ней есть разделение на эти направления. Но если вы с курсов решили устроиться в стартап, где есть биг дата, но особо нет людей на эту биг дату, тогда вашим ключевым навыком должно быть умение выживать. Именно здесь обычно нужен Data Scientist, который на все руки мастер. Готовы ли вы к этому? Почему всё-таки Data Science не для вас? 1. Data Science – для усидчивых. Над одной и той же задачей придется сидеть очень долго. Пока найдешь все необходимое в данных, пока найдешь оптимальный способ обработки данных, пока погрузишься в область, пока переберешь все модельки, которые могут потенциально хорошо решить эту задачу… А еще надо дождаться, пока модель обучится - это ж целая вечность пройдет! Потом еще выясняется, что у модели метрики недостаточно хорошие, и весь этот путь нужно пройти заново, но с новыми идеями. Хватит ли вам умения занять себя другими делами во время обучения модели? Вряд ли проджект будет в восторге, если узнает, что несколько дней вы пили кофе, пока модель обучалась. Хватит ли вам терпения перебирать идеи до тех пор, пока точность модели не достигнет нужного бизнесу числа? 2. Математика останавливается не на 2+2 и даже не на интегралах. Вам точно понадобятся знания по мат.статистике, линейной алгебре, теории вероятностей. Нет, вам не хватит встроенных в библиотечки функций. Может быть для какого-то поверхностного анализа и составления простецкой модели – да, Но чтобы лучше анализировать огромные массивы данных, в математике действительно нужно разбираться. 3. Вам нужно знать Python/R/Scala или еще что-то, на чем вы соберете ваше решение. Вы не бэкенд-разработчик, но вам нужно приготовить умную штуку, которую можно будет встроить в прод. Нужно будет составлять POC-шки, функции предобработки данных, которые затем будет имплементировать бэкенд-разраб. Необходимо уметь общаться с бэкендерами на их языке, если им понадобится помощь при внедрении вашего решения. 4. Еще немного о встраивании модели в прод: вам нужно интересоваться тем, на каком железе будет запущена модель. Ну не получится запустить трансформер на сервачке заказчика с двумя ядрами и без малейшего признака GPU. А решение, которое работает на вашем компе и не работает на устройствах клиента не будет принято и, соответственно, оплачено. 5. Нужно знать фреймворки, которые приняты в команде. Если вы до этого делали fit на керасе, а потом внезапно пришли в команду, которая работает на pytorch, уйдет много времени на обучение работе с новыми инструментами. Справедливости ради, вряд ли вас возьмут на работу со стеком, в котором у вас нет опыта, но на всякий случай лучше задавать этот вопрос на первых этапах собеседования. 6. По ходу проектирования решения нужно будет использовать огромное количество, библиотек и лучше знать как ими пользоваться до начала работы над проектом. Или же уметь быстро ориентироваться в документации, а в процессе только добирать необходимое. Обучаться всему и сразу плохая идея. 8. Вам нужно уметь использовать тулзины, которые тесно связаны с разработкой моделей. Модели и данные нуждаются в версионировании. В этом могут помочь DVC, MLflow или, например, Weights&Biases. А еще модели нужно сервить. В этом случае могут быть пригодны TorchServe или Tensorflow Serving. А может быть вам нужно будет написать свою обвязку вокруг модели, чтобы ее можно было быстро потестить. Этот зоопарк огромен. Нужно выбрать из него именно те вещи, которые вам помогут, и учиться ими пользоваться. 9. Нужно уметь не только чистить данные, но и размечать их, когда вам не дали нормальной разметки, искать, какие данные можно добавить к выданному заказчиком датасету, если его явно не хватает. 10. Важной частью вашей работы будет общение с бизнесом для того, чтобы точно определить цель того, что вы делаете, и не сделать того, что не нужно. Иногда заказчики дают расплывчатое ТЗ, внимательно вчитываясь в которое вы обнаружите, что заказчик хочет всё и сразу, но при этом непонятно зачем. Поэтому нужно будет суметь задать такие вопросы, которые прояснят, что действительно требуется от вас, или суметь сократить скоуп работ до реально реализуемого. 11. Если проджект где-то там пообещал нереальные сроки для выполнения вашей задачи, нужно вовремя суметь сказать, что пятилетку за три года – не получится. Нельзя просто так взять и выстроить радужные мечты вокруг того, как быстро вы сможете сделать задачу. А с вас точно потребуют сроки, в которые будет необходимо уложиться. Вопрос оценки времени на реализацию – очень сложен. Здесь важно не только правильно декомпозировать задачу, но и добиться понимания от вышестоящих людей, почему эта задача не может быть реализована в очень сжатые сроки. Так что составляем план, и его придерживаемся. А если что-то выходит из-под контроля срочно трубим о том, что ничего не успевается и с этим надо что-то делать. 12. А еще вам самим придется составлять ТЗ по данным, которые вы хотите получить. Из нашего опыта, чаще приходят заказчики, которые не понимают, что вам от них нужно и в каком объеме. Будьте готовы отвечать на вопрос сколько и каких данных вам надо, вникать в задачу на этапе обсуждения проекта и предполагать, какие кейсы могут возникать в вашей задаче. Также количество размеченных данных, следует запрашивать в таком объеме, который сможет удовлетворить заказчика. Если вы потребуете терабайты размеченных данных, а другой исполнитель скажет, что ему хватит пары гигов, то выберут скорее его, а не вас, просто потому что разметка данных стоит немалых денег. А ответ «чем данных больше – тем лучше» никого не устроит. 13. Data Science – не только про генерацию красивых картинок и красивой музыки без помощи человека. Частый запрос бизнеса – проанализировать информацию о клиентах. Из этой бизнес- задачи может вытекать DS-задача, которая вам не понравится. Например, выстроить контекстную рекламу так, чтобы новые покупатели приобрели товар подороже, или завысить цену продукта в соответствии с предполагаемым уровнем дохода покупателя. Будет ли такая задача вписываться в ваши моральные рамки? Возможно, у вас будет опция отказаться от задач, которые не подходят вашему пониманию хорошо- плохо.Но бывает и так, что вас не спрашивают. Конечно, вопрос, стоит ли работать с компанией с которой у вас разное понимание границ не в плоскости data science, а скорее в общем. Но будьте готовы к тому, что в data science часто встает вопрос этики использования данных, и эта этика зафиксирована в достаточно размытом формате. Вы можете сказать, что на самом деле многие навыки из этого списка входят в программу популярных курсов. Да, это так. Но дадут ли вам необходимое количество разнородных задач, которые присутствуют в реальном мире? Насколько смешанные, странные и неполные данные будут присутствовать в этих задачах? Требования от заказчиков в реальном мире будут сильно сложнее, чем обычно определяют на курсах. На курсах вам создадут рафинированную среду, в которой все понятно, четко и ясно определено в требованиях и точно опирается на предыдущий опыт из лекций. К сожалению, на работе будет не так. Придётся ловить на лету и определять, как действовать, очень быстро. Так что наше мнение: пойти на курсы, чтобы стать датасаентистом, недостаточно. Иметь высшее образование, кстати, тоже. Знаем, бывали. Даже если ваша вышка с уклоном в дата саенс, по факту, она часто дает не сильно больше курсов. Не спорим, есть разные университеты, но, по нашему опыту, вам там дадут примерно такую же базу, как и на курсах, только чуть более разностороннюю (и то не факт). Но все- таки, это обучение длится подольше, и скорее всего, информации вы впитаете побольше. Еще и курсы по философии и экологии подкинут :) Важное качество в профессии датасаентиста – способность постоянно учиться, находить для себя новые задачи и расти на этом практическом опыте. А еще постоянно следить за событиями в сфере, потому что кажется, что она обновляется чаще и кардинальнее, чем другие сферы IT. Поэтому специализированные курсы – это хорошо, высшее образование – тоже, но важно понимать, что это ещё далеко не всё, это только начало длинного и тяжелого пути. Cекция ""вопрос-ответ"" в лучших традициях пиар- страниц курсов 1. Датасаенс – это весело? Конечно! Но только если вы действительно упороты и любите искать что-то странное и копаться в куче данных, чтобы в итоге делать штуку, которая будет помогать бизнесу в реальных задачах с большой точностью. Но будьте готовы и к не самым веселым моментам: на этом самурайском пути много препятствий. В каком-то смысле, датасаенс – это образ жизни, при котором нужно всегда держать руку на пульсе и интересоваться происходящим в области. Став датасаентистом однажды, больше уже не сможешь смотреть на все эти умные штуки как на абсолютную магию. 2. Я математик по образованию и по желанию и очень хочу в DS. Получится? Отличная база. Математика как основа – это очень хорошо. Но вам еще очень многое придется освоить. Удачи вам в этом нелегком пути! 3. А я вот из программиста решил переквалифицироваться в датасаентиста, каковы шансы? Вам будет тяжело. Шанс есть всегда, но это будет очень сложный путь. На нашем опыте, понять программирование для человека с математическим бэкграундом проще, чем программисту математику. Мы все проходили курс по высшей математике в универе и помним матешу со школы, но это не тот уровень, который будет требоваться на месте работы. 4. А почему вы говорите, что искусственного интеллекта сейчас не существует? Искусственный интеллект – интересное понятие. Этим термином внезапно стало принято называть всё, что может делать что-то за человека. Но до «интеллекта» там далеко. Да данный момент то, что мы называем ИИ, это алгоритмы, которые могут решать узконаправленную задачу. Распознавание лиц? Детекция объектов? Генерация контекстной рекламы? Интеллектуальный анализ данных? Все это узконаправленные сферы. Разве мы можем назвать калькулятор искусственным интеллектом? Ну, это вряд ли. Хотя он также решает узконаправленную задачу – решение математических уравнений. А пару столетий назад это казалось задачей искусственного интеллекта. В целом, можно сказать, что ИИ существует, но в очень примитивном виде. Его эволюцию можно сравнить в эволюцией человека. На ранних стадиях задачи человека также были узконаправленными и примитивными, но человек эволюционировал. Так же можно предположить и с ИИ, он может эволюционировать. И не обязательно базой для «прогрессивного ИИ» будут нейронные сети. Подведем итоги • Data Science – это профессия с высоким порогом входа. Нужно многое знать, нужно многое уметь. Если вы хотите в IT, и вам кто-то сказал, что через дата саенс это сделать проще, чем через другие профессии, то не слушайте этого человека. Обратите внимание на другие возможные варианты входа в IT, например, верстку или ручное тестирование. • Data Science – это не только про построить предсказывающую модель в jupyter-notebook и отдать ее дальше. Скорее всего, вам придется делать сильно больше этого функционала. • В работе с данными есть разные ветвления профессий, например, инженер данных или аналитик данных. Для них требуются иные навыки. Может быть, если ваше желание именно работать с данными, вам стоит посмотреть на эти специализации. 5 Карма 2 Рейтинг Tourmaline Core @TourmalineCore Пользователь Комментарии 5 Публикации • Data Science – про постоянное обучение и слежение за новыми разработками в этой сфере. Держать руку на пульсе – это один из ключевых навыков датасаентиста. • Курсов, на которых вам за несколько месяцев дают всё необходимое для профессии – не существует. Вам могут дать базу, но стоит ли эта база тех денег, которые за нее просят, это спорный вопрос. • Есть множество курсов по DS, которые находятся в открытом доступе абсолютно бесплатно, и дают примерно тот же материал, что и дорогостоящие. А практические задачи можно найти на kaggle. Кстати, там же есть и решения других людей, которые могут вам помочь обучиться и понять, как же все-таки делать лучше. Рассмотрите этот вариант. Возможно, в будущем мы сделаем статью-сборник с такими материалами, а пока посоветуем отличный ресурс для начала обучения.",TourmalineCore
"Что вам надо знать в 2025 году про контейнеры, чтобы не пропустить важное",H3LLO.CLOUD,868.09,Облачный провайдер которого вам захочется обнять,2025-03-13,"Контейнер — это типа виртуальной машины, только меньше и другое. Несколько контейнеров запускаются внутри одной машины и разделяются друг от друга. Это значит, что можно запустить приложение с одним набором зависимостей, а рядом — второе с другим. Это значит, что можно сохранить все связки приложения, упаковать его в контейнер и деплоить где угодно — и знать, что оно точно запустится. Есть нюансы с переходом между ARM-архитектурой и x86, но в целом контейнеры универсальны.В контейнерной упаковке огромное количество софта, в том числе очень много опенсорса. Можно поднять готовый контейнер с сервисом из хаба без проблем вообще. И это не создаёт сложных взаимозависимостей. Нужен PostgreSQL? Docker pull postgres — и он у вас. К контейнерам монтируются свои ресурсы — диски, сети, конфиги и секреты. Контейнеры оплачиваются за потребление, то есть можно сделать сервис, который слушает порт и делает обработку, повесить и забыть. За год вы заплатите рублей 20. Рои контейнеров могут масштабировать крупные корпоративные проекты, про это ниже. И, наконец, никакой современный CI/CD почти не делается без контейнеров. Системным администраторам, DevOps-инженерам, разработчикам и СТО критически важно разобраться в контейнеризации. Это будущее, и у этого будущего довольно странная кривая обучения. Сейчас коротко расскажу, попробуем разобраться, как её лучше проходить и где там грабли. Ну и ещё, если вы дойдёте до середины, ваша зарплата будет 450 тысяч рублей в месяц и выше. Короткая вводная Раньше всё было просто: есть админы, которые настраивают серверы, и есть разработчики, которые пишут код. Между ними как будто была стена, через которую разработчики кидали админам сборку софта, а админы в обратку кидали проблемы с деплоем и рантайм-эксепшены. А теперь границы размываются. Разработчики должны понимать, как их код будет жить в продакшне. Админы должны уметь автоматизировать развёртывание. СТО нужно планировать архитектуру с учётом масштабирования и отказоустойчивости. И здесь на сцену выходят контейнеры — технология, которая объединяет эти миры. Но чтобы в 2025 году оставаться в игре, нужно не просто знать, что такое Docker. Нужно понимать всю экосистему — от простейших контейнеров до оркестрации в Kubernetes и интеграции с нейрокодингом. Сначала у нас были физические серверы — железки, которые занимали целые стойки. Хочешь новое приложение? Покупай новый сервер. Нужно больше мощности? Апгрейдь железо. Потом пришла виртуализация и изменила правила игры. Теперь на одном физическом 13.03.2025, 16:39 Что вам надо знать в 2025 году про контейнеры, чтобы не пропустить важное / Хабр https://habr.com/ru/companies/h3llo_cloud/articles/890512/ 2/17 сервере можно запустить несколько виртуальных машин с разными операционками. Это был прорыв: меньше железа, больше гибкости, лучше утилизация ресурсов. Но виртуализация принесла и свои проблемы. Каждая виртуалка — это полноценная операционная система со всеми потрохами. Это гигабайты дискового пространства, сотни мегабайт оперативки только на старте и заметное время запуска. Виртуалки — штука классная, но у них есть несколько серьёзных ограничений: Проблемы масштабирования. Допустим, у вас «чёрная пятница» и нагрузка выросла в три раза. С виртуалками у вас два пути: либо увеличить ресурсы существующей машины (ресайз), либо клонировать её. Оба варианта требуют времени и дополнительной настройки. Сложности с зависимостями. Представьте: у вас на одном сервере два приложения. Одному нужна Java 8, другому — Java 11. С виртуалками решение одно: две отдельные виртуалки. Это лишние ресурсы и головная боль с администрированием. Неэффективное использование ресурсов. Каждая виртуалка жрёт ресурсы даже в простое, потому что крутит полноценную ОС. Это как держать заведённую машину на парковке. Разница между виртуализацией и контейнеризацией Здесь важно понять ключевую разницу в подходах. Виртуальные машины — это «домашние питомцы». Вы их поднимаете, называете по имени, заботитесь об их здоровье, бьётесь над их аптаймом. Вы знаете их всех в лицо, все ваши восемь виртуалок. Стараетесь, чтобы они прожили как можно дольше. Контейнеры — это «стадо». Вы не знаете их по именам, их может быть сотни. Один упал — и ладно, поднимется другой. Это нормально, если за рабочий день вы запустите и грохнете пару сотен контейнеров. Жизненный цикл виртуалки рассчитан на месяцы и годы. Она копит проблемы и болезни, как старый человек. Контейнеры же могут жить минуты или часы, а потом уступить место свежему контейнеру с новой версией кода. Собственно, именно так контейнеры чаще всего и используются. Существует даже кейс с очень коротким жизненным циклом контейнера: за 1–2 секунды 13.03.2025, 16:39 Что вам надо знать в 2025 году про контейнеры, чтобы не пропустить важное / Хабр https://habr.com/ru/companies/h3llo_cloud/articles/890512/ 3/17 поднимается контейнер, отрабатывает запрос, умирает. Это крайне удобно. Так делают на платформах приложений и в CI/CD-pipeline. Контейнер — это изолированный процесс, которому кажется, что он запущен в отдельной системе. Но на самом деле он использует ядро хостовой ОС. Для изоляции контейнеры используют не возможности железа (как виртуалки), а возможности ОС — так называемое пространство имён. Например, Docker использует cgroups в ядре Linux. Виртуалка весит гигабайты, контейнер — мегабайты. Postgres в контейнере (без нагрузки) — всего 9 мегабайт оперативки. Девять! Это просто смешно по сравнению с полноценной виртуалкой. Виртуалки запускаются минуты, контейнеры секунды. Но самое главное — портативность между различными средами. «У меня локально работает, а на проде — нет». С контейнерами такое почти исключено. Запаковали приложение в контейнер, и оно будет работать одинаково, что у вас на ноуте, что на тестовом сервере, что в продакшне. Допустим, у вас типичная система из веб-сервера, приложения и базы данных. Описав всё это в Docker Compose файле, вы можете одной командой поднять всю инфраструктуру на любой машине. Чтобы было удобно разворачивать контейнеры в разных средах, образ заливается в репозиторий (публичный или приватный), и тянется оттуда. Не надо ручками раскидывать образ на хосты — они сами стянут его из репы. Существуют миллионы готовых образов и создавать их с нуля не надо. Вот почему они такие крутые. Первые шаги: знакомство с Docker’ом Сначала вы не понимаете, зачем нужен Docker и все эти статьи на Хабре про него. Скачиваете его из любопытства, долго пытаетесь разобраться, как создать свой первый контейнер. Потом замечаете, что запустить Postgres в Docker — это мизинцем левой ноги, когда в обычной установке это адская головная боль. Docker и GUI к нему (Docker Desktop) можно поставить и на Linux, и на Mac, и на Windows. Для начала работы достаточно знать несколько команд: docker run, docker build, docker ps, docker stop. Не пугайтесь — это проще, чем кажется. Чуть-чуть покурить обучение на Ютубе или официальный get started — и вот у вас появляется первый контейнер. 13.03.2025, 16:39 Что вам надо знать в 2025 году про контейнеры, чтобы не пропустить важное / Хабр https://habr.com/ru/companies/h3llo_cloud/articles/890512/ 4/17 Порог входа обманчиво простой. Дальше идёт работа с образами и репозиториями — Docker Hub и прочие Container Registry. Docker Hub — это как GitHub, только для контейнеров. Там уже есть готовые образы для большинства популярных программ. Первый Dockerfile можно написать за 10 минут. Это просто текстовый файл с инструкциями: откуда взять базовый образ, какие команды выполнить при сборке, что запустить. LLM сейчас отлично умеют генерировать Dockerfile — это ваш первый помощник. Обычно с Docker знакомятся так: когда вы инди-хакер с одной виртуалкой в облаке, контейнеры вам, возможно, и не нужны. Момент истины наступает, когда: Бизнес начинает расширяться. Проект превращается во что-то более крупное. Вы обновляете приложение несколько раз в день. В команде появляется больше 3–5 разработчиков. Дальше идёт описание многокомпонентных систем в Docker Compose. Docker Compose позволяет описать в одном YAML-файле целую инфраструктуру: несколько связанных контейнеров, их сети, тома для хранения данных. Одна команда — и всё поднимается автоматически в нужном порядке. Но до этого вас ждёт ещё пара ответвлений в дереве развития навыков. Podman как альтернатива Docker. Со временем вы узнаёте, что Docker — не единственный вариант. Podman, например, не висит постоянно в памяти. Можно даже сделать алиас, alias docker=podman, и забыть, что у вас на самом деле не Docker. Podman хорош для простого докерного опыта, но если нужен Docker Compose — это фича исключительно докеровская (хотя последние Podman уже поддерживает Docker Compose через плагины). По мере того как вы и ваш проект растёте, вы начинаете задумываться об оркестрации. Продвинутый этап: мультихостовые решения и CI/CD Когда контейнеры на одной машине уже не справляются, вы смотрите в сторону кластеров 13.03.2025, 16:39 Что вам надо знать в 2025 году про контейнеры, чтобы не пропустить важное / Хабр https://habr.com/ru/companies/h3llo_cloud/articles/890512/ 5/17 и автоматизации. Самое главное — контейнеры упрощают CI/CD. До контейнеров CI/CD был про скрипты, которые что-то собирают, тестируют и куда-то выкладывают. Контейнеры всё упростили: среда сборки такая же, как среда запуска. Куча головных болей с зависимостями просто исчезла. CI-системы могут собирать контейнеры в чистой среде, гарантируя, что никакой левый код не попадёт в продакшн. Вы просто пушите изменения в репозиторий, а CI автоматически собирает новый образ. Дальше CD раскатывает его по разным окружениям. Это управление образами и их версиями. Для каждой версии вашего кода создаётся отдельная версия образа. Хотите откатиться? Просто укажите предыдущую версию. Никаких сложных процедур отката. Docker Swarm обычно выступает как первая попытка оркестрации. Docker Swarm кажется логичным продолжением Docker — всё-таки от тех же разработчиков. Вы в нём пытаетесь что-то сделать, плюётесь и думаете: «Хорошо, видимо, судьба ведёт в Кубер». И в этот момент контейнеры ломают вас об колено. Установить «настоящий» Кубер (тот, что в репозитории github.com/kubernetes/kubernetes) ужасно сложно. Тут возникает развилка: либо пользоваться облачными Managed Kubernetes, либо использовать упрощённые дистрибутивы вроде k3s, k0s, Minikube или Kind. Дело в том, что в k8s вообще другой подход к тому, что кажется логичным. Он очень дружелюбен. В основном, как кажется поначалу, к шизофреникам. Но по мере того, как вы научитесь думать контейнерами, всё станет очень логичным и понятным. Возможно. В Kubernetes базовая единица — не контейнер, а pod. Это такая абстракция, в которой может жить несколько контейнеров. Обычно требуется больше одного дня, чтобы врубиться, что же это такое. Вместе с этим меняется представление о контейнерах, ведь теперь у них появляются роли (init container, side-car container). Kubernetes автоматически делает то, о чём ты раньше мог только мечтать: если приложение падает, оно автоматически перезапускается; если нагрузка растёт, оно автоматически масштабируется; если нода выходит из строя, рабочие нагрузки перераспределяются. Ну, то есть после настройки кучи всего, конечно. Например, для 13.03.2025, 16:39 Что вам надо знать в 2025 году про контейнеры, чтобы не пропустить важное / Хабр https://habr.com/ru/companies/h3llo_cloud/articles/890512/ 6/17 автомасштабирования понадобится Horizontal Pod Autoscaler и включённые метрики. Ещё есть манифесты и Helm-чарты. В Kubernetes всё описывается в виде YAML- манифестов. В манифесте описывается желаемое состояние системы и Кубер сам заботится о поддержании этого состояния. Это как инфраструктура-как-код, но для контейнеров. Helm-чарты позволяют упаковать несколько манифестов в один установочный пакет и использовать шаблонизацию. И ещё бывает автоматическое обновление без простоев. Kubernetes умеет обновлять приложения по принципу rolling update: постепенно заменяет старые поды новыми, поддерживая доступность сервиса. Пользователи даже не заметят, что произошло обновление. Дальше надо понять базовые принципы сетевого взаимодействия контейнеров: TCP/IP и Linux bridge. Контейнеры общаются через обычную сеть. В пределах одной машины это виртуализированная сеть через Linux bridge. Создаётся виртуальный адаптер, крепится к контейнеру, появляется интерфейс с IP-адресом. К этой же виртуальной сети подключаются другие контейнеры. Виртуальные сети между контейнерами. С Docker Compose можно создать несколько разных сетей и ограничить общение контейнеров друг с другом. Это как виртуальные VLAN в мире контейнеров. Ручками тоже можно, но с Docker Compose — проще. Ingress и маршрутизация трафика. Ingress в Kubernetes — это способ маршрутизировать внешний трафик к сервисам внутри кластера. Если ставить Кубер на голом железе (или в виртуалках на голом железе), то на этом месте можно сломать голову и любой новичок гарантированно забуксует. Без возможности выставить сервис наружу Кубер превращается в чемодан без ручки дом без окон и дверей — все системы прекрасно работают внутри него, общаются между собой, но достучаться к ним снаружи кластера невозможно. Сейчас рекомендуется переходить с Ingress на Gateway API. Совет — не делайте этого, не освоив Ingress. И ни в коем случае не ставьте сразу Kong (он хорош, но о-о-о-чень сложен для начала). Балансировка нагрузки. Kubernetes умеет распределять запросы между несколькими репликами приложения. Managed Kubernetes обычно использует LoadBalancer от провайдера. Если ставить Кубер на своём железе — можно попробовать MetalLB — проект специально сделан, чтобы дать функционал LoadBalancer там, где он не предусмотрен по дизайну. Без балансировки между инстансами ничего не получится, иначе вы опять будете реплицировать монолит. Монолит, кстати, тоже можно разворачивать через контейнеры, но обычно смысл в другом. 13.03.2025, 16:39 Что вам надо знать в 2025 году про контейнеры, чтобы не пропустить важное / Хабр https://habr.com/ru/companies/h3llo_cloud/articles/890512/ 7/17 Инструменты визуализации (Hubble для Cilium). Kubernetes умеет дружить со множеством различных реализаций контейнеров, сетей и хранилищ. Делается это за счёт интерфейсов. Так вот для сетей есть Cilium, а для Cilium есть Hubble — визуальный интерфейс. Можно буквально увидеть, как взаимодействуют компоненты, куда идёт трафик. Это одна из самых частых проблем — нужно держать в голове карту взаимодействия, и это решение помогает быстрее освоиться. Грабли, которые вы соберёте В этом месте, если вы научитеcь оркестрировать, возникнет следующий уровень сложности — ИБ. Она, как и везде, ошибок не прощает. Настройка портов и их экспозиция. Даже Senior-разработчики, бывает, путаются в настройке портов: что такое publish, expose, когда использовать ClusteIP, а когда NodePort. Это нормально — все через это проходят. Каждый раз, когда вы открываете порт наружу, вы создаёте потенциальную дыру в безопасности. Второе правило контейнерного клуба: не запускайте всё от рута. Контейнеры изолированы не так хорошо, как виртуалки, поэтому использование непривилегированных пользователей критически важно. Контейнеру не нужны все права в мире. Дайте ему только то, что ему действительно необходимо для работы. Используйте seccomp, AppArmor или SELinux для дополнительных ограничений. Если Junior-разработчик предлагает запустить контейнер с флагом --privileged, это повод для серьёзного разговора (или увольнения, в зависимости от обстоятельств). Уделите время освоению модели RBAC и практикам её использования в Kubernetes. Никогда не хардкодьте пароли и ключи в образ контейнера. В Docker есть Docker Secrets, в Kubernetes — Secrets API. Используйте их вместо переменных окружения с чувствительными данными. Это важно, когда вы начнёте раскатывать что-то за пределы одного проекта. Ну и контейнеры должны знать, как обращаться друг к другу. В простом случае это имя контейнера, в Kubernetes — сервис с DNS-именем. Заражённые контейнеры тоже бывают. Образы контейнеров, как и любой код, могут содержать уязвимости. Да и атаки на цепочки поставки никто не отменял. Используйте инструменты вроде Trivy или Clair для проверки образов перед деплоем. 13.03.2025, 16:39 Что вам надо знать в 2025 году про контейнеры, чтобы не пропустить важное / Хабр https://habr.com/ru/companies/h3llo_cloud/articles/890512/ 8/17 Ну и не наступайте на грабли с лицензиями. В мире контейнеров доминирует открытое ПО. Postgres вместо Oracle, Nginx вместо IIS. Это не только дешевле, но и удобнее упаковывается. Если у софта всё же есть коммерческая версия, то чаще всего она подразумевает поддержку от разработчика и дополнительный функционал. Обычно в мире open-source софт лицензируется по фичам, а не по ресурсам. Вы покупаете энтерпрайз- версию, а как её масштабировать — ваше дело. Зачем всё это? Современные LLM генерируют не просто куски кода, а целые приложения. И идеальный способ их запустить — контейнеры. Sonnet 3.7 уже настолько круто кодит, что многие компании смогли увеличить производительность в разы. Вы описываете словами, что хотите получить, и нейронка генерирует готовый код. Дальше дело за малым — упаковать его в контейнер и запустить. Этот процесс становится таким же простым, как отправка письма. Кстати, уже есть люди, которые не умеют в электронную почту ) Ну и сейчас у каждого уважающего себя облачного провайдера есть управляемый Kubernetes. Вам не нужно возиться с установкой и настройкой — просто выбираете количество нод и через несколько минут кластер готов к работе. Но есть нюансы. Решения для запуска приложений в контейнерах часто бывают и serverless, там основная единица — это контейнер, а не ноды кластера. Яндекс, например, предлагает сервис, где контейнер запускается только на время вызова. Происходит HTTP-запрос, запускается контейнер, отрабатывает и умирает. У нас в L1veStack подход, скорее — замена виртуалок. Serverless containers. Это логическое развитие контейнеризации: вы не думаете о серверах вообще. Есть контейнер, он запускается и работает пока вы его не остановите, и тарификация только за потреблённые ресурсы во время его работы. Наши контейнеры рассчитаны на долгую жизнь, но никто не мешает вам убить полстада и запустить заново. И повторить через 5 секунд. Как учиться Отличный первый проект для освоения контейнеризации — простой чат-бот. Он включает и фронтенд, и бэкенд, и базу данных — всё, что нужно для понимания взаимодействия контейнеров. Вот у нас пример. Там ещё и простой CI/CD на базе GitHub Actions, который собирает образ при каждом новом коммите. 13.03.2025, 16:39 Что вам надо знать в 2025 году про контейнеры, чтобы не пропустить важное / Хабр https://habr.com/ru/companies/h3llo_cloud/articles/890512/ 9/17 Начните с запуска отдельных контейнеров, потом объедините их с Docker Compose, затем перенесите в Kubernetes. Это естественный путь освоения технологии. Сейчас есть множество готовых Docker Compose файлов для популярных стеков. Хотите WordPress? Есть готовый рецепт. Хотите LowCode-платформу? Есть готовый рецепт. Нужна CRM-система? Есть готовый рецепт. Не изобретайте велосипед, используйте готовые решения. Проиграйте failsafe-сценарии. Запустите несколько реплик, сымитируйте падение одной из них, посмотрите, как система восстанавливается. Это бесценный опыт, потому что он учит думать именно в философии контейнеров. Грейды примерно такие: Junior: базовые знания Docker, умение запускать и останавливать контейнеры. Middle: сборка своих образов, Docker Compose, CI/CD с контейнерами, основы Kubernetes. Senior: глубокое понимание Kubernetes, настройка производительности, мониторинг. Если говорить про администрирование, то джуны ломаются на Ingress и выставлении портов наружу. Мидлы поломаются на Gateweay API и настройке Auto Provisioning для persistent volumes — как настроить систему, чтобы она автоматически выделяла дисковое пространство, например в Ceph. Даже некоторые сеньоры спотыкаются на сетевых тонкостях Kubernetes. Контейнеризация нужна везде: от стартапов до корпораций. Но особенно востребованы специалисты в компаниях, которым нужно быстро и часто деплоить обновления. В 2025 году умение работать с контейнерами — это уже не преимущество, а базовый навык. Не важно, кто вы — разработчик, администратор или CTO, вам нужно понимать, как работают контейнеры и зачем они нужны. Нейронки сделали возможным выкатывать Proof-of-Concept целых систем за дни, а MVP — за недели. Скорость Time-to-Market решает в этой гонке, и её не повысить без инструментов автоматизации сборки и деплоя. И Kubernetes тут как нельзя в тему. Если вы освоили всю стопку технологий и умеете их применять на практике (и не просто применять, а устанавливать и правильно настраивать), вы — мега-DevOps с зарплатой от 500к и выше. И спрос на таких специалистов только растёт. Начать можно с официальной документации Docker и Kubernetes. Для углубления знаний 13.03.2025, 16:39 Что вам надо знать в 2025 году про контейнеры, чтобы не пропустить важное / Хабр https://habr.com/ru/companies/h3llo_cloud/articles/890512/ 10/17 рекомендую «Kubernetes в действии» и Docker: Up & Running. Есть даже отдельная книга «Kubernetes и сети» — для тех, кто хочет разобраться в сетевом взаимодействии. Лучший способ учиться — делать. Пройдите интерактивные курсы на Katacoda или Play with Docker. Устройте себе хакатон выходного дня — запакуйте своё приложение в контейнер и запустите его в Kubernetes. Почитайте Open Container Initiative. Покурите service mesh решения (Istio, Envoy), которые важны при работе с микросервисами. Разберитесь со StatefulSets (хранение состояния в контейнерах) — если не сломаете голову до этого. На сладкое оставьте CRD и операторы (возможно, даже свои). Подписывайтесь на телеграм-каналы DevOps-сообществ, следите за кейсами компаний, которые внедрили контейнеризацию. Вступайте в сообщества на Reddit, Stack Overflow, GitHub Discussions. Задавайте вопросы, делитесь своими находками. Контейнеризация — это живая экосистема, и лучший способ оставаться в курсе — быть её частью. Какой у вас опыт с контейнерами? 0% Не пробовал и не хочу 0 0% Не пробовал, но хочу 0 0% Пробовал, но не использую 0 33.33% Могу собрать образ, но не больше 1 0% Пишу docker-compose 0 Только зарегистрированные пользователи могут участвовать в опросе. Войдите, пожалуйста. 13.03.2025, 16:39 Что вам надо знать в 2025 году про контейнеры, чтобы не пропустить важное / Хабр https://habr.com/ru/companies/h3llo_cloud/articles/890512/ 11/17 H3LLO.CLOUD Облачный провайдер которого вам захочется обнять Сайт 83 Карма 379 Рейтинг Konstantin @km1337 CTO Публикации 33.33% Умею деплоить в Kubernetes 1 0% Могу сам поднять кластер Kubernetes 0 33.33% Я и есть контейнер 1 Проголосовали 3 пользователя. Воздержавшихся нет.",km1337
"Чужое открытое ПО внутри BI-систем проблемы лицензирования, о которых не все задумываются",Luxms BI,29.52,,2025-03-13,"Для быстрого создания какого-то продукта часто используют различные open source решения. Но применение открытого ПО несёт в себе определенные риски: от финансовых до юридических. Например, из open source для исследования и визуализации данных наиболее популярен Superset. Около десятка российских систем взяли его за основу своих BI. Но его применение несёт риски: Стратегические риски. Roadmap Superset формируется с учётом интересов западных якорных заказчиков и интересов американских инвесторов, а не российского бизнеса. Владелец проекта — американская компания Airbnb, и в любой момент по политическим или другим причинам может ограничить доступ к своему ПО для российских разработчиков; Компании, использующие Superset, зависят от своих внутренних ИТ-команд, что может привести к кадровым и финансовым рискам при уходе ключевых разработчиков; Организационные риски. Нет централизованной технической поддержки от вендора — пользователи вынуждены искать решения проблем в документации, на форумах и сторонних источниках. Стоить добавить, что популярность открытого ПО также связана с иллюзией не только «бесплатной» разработки и дальнейшей поддержки со стороны собственного ИТ- персонала, но и кажущимся отсутствием лицензионных платежей. Но разработка на базеПО с открытым исходным кодом не означает бесконтрольное использование созданных на этом коде продуктов. Существует ряд open source лицензий: BSD, GNU, MIT, LGPL, AGPL, BSPL, SSPL, Demoware License, Apache License 2.0, RSAL и другие, в которых нужно разбираться. Соблюдать их важно не только компании, которая интегрировала такое решение в свой продукт, но и той, которая такой продукт купила и использует. В соответствии со статьями 1252 и 1301 ГК РФ иски о нарушении лицензионного права предъявляются к юридическому лицу, использующему указанное программное обеспечение. Меня зовут Алексей Розанов, я руководитель пресейл-направления и работы с партнёрами ГК Luxms, вендора российской платформы бизнес-аналитики Luxms BI. Luxms BI — платформа бизнес-аналитики данных с высочайшим быстродействием и горизонтальной масштабируемостью. У неё мощные функциональные и визуальные возможности, а также быстрая обработка больших объёмов данных благодаря своей датацентричной архитектуре. В Реестре российского ПО. Рассмотрим несколько примеров использования open source решений на рынке BI- аналитики, и я постараюсь подсветить лицензионные риски для компаний, которые используют BI-системы на их основе. 1. Использование open source как основы нового продукта. 13.03.2025, 16:39 Чужое открытое ПО внутри BI-систем: проблемы лицензирования, о которых не все задумываются / Хабр https://habr.com/ru/companies/luxms_bi/articles/890568/ 2/11 Есть российская BI-система, в основе которой лежит open source BI система «Metabase». Metabase Open Source Edition предоставляется: 1. под лицензией GNU Affero General Public License (AGPL), которая требует раскрытия исходного кода тем лицам, которым вы также предоставляете двоичный файл; 2. и под коммерческой лицензией с соответствующей оплатой. Если российская BI-система использует Metabase под AGPL и не соблюдает требования AGPL, это может привести к юридическим спорам со стороны правообладателей Metabase, а если исходный код системы раскрыт, это может привести к утечке конкурентных преимуществ или уязвимостей. Если российская BI-система использует Metabase под коммерческой лицензией и не оплачивает её, это может привести к юридическим последствиям, включая судебные иски. Помимо этого, правообладатель Metabase базируется в Сан-Франциско, Калифорния, что несёт в себе дополнительные риски закрытия или изменения проекта open source. В том числе, остается зависимость от политики лицензирования Metabase: если условия лицензий изменятся, это может повлиять на стоимость или доступность системы в целом. 2. Использование open source, как части системы. Highcharts — это библиотека для создания интерактивных графиков и диаграмм, написанная на JavaScript. И некоторые популярные российские BI-системы используют её, как основу для своих визуализаций, но при её интеграции в коммерческих проектах применяется лицензия Original Equipment Manufacturer (OEM), которая подразумевает оплату. OEM-лицензия обычно предусматривает использование библиотеки только в рамках конкретного продукта, в данном случае — BI-системы, и её использование, например, для собственных разработок, может нарушить условия лицензии. Поэтому при внедрении BI- системы со встроенной библиотекой Highcharts нужно убедиться, что вендор приобрел лицензию легально, и она соответствует условиям использования библиотеки в конкретном случае. Если лицензия не оплачена или нарушены её условия, это может привести к юридическим последствиям для компании-пользователя (например, судебным искам со стороны правообладателя Highcharts). И так же, как с любым open source, использование Highcharts означает зависимость от сторонней библиотеки. Если они изменят свои условия лицензирования или прекратят поддержку, это может повлиять на работу BI-системы. 3. Поставка целиком BI-системы зарубежного вендора под видом российского BI. 13.03.2025, 16:39 Чужое открытое ПО внутри BI-систем: проблемы лицензирования, о которых не все задумываются / Хабр https://habr.com/ru/companies/luxms_bi/articles/890568/ 3/11 На российском рынке, где уход зарубежных вендоров вызвал всплеск спроса на импортозамещение, есть примеры и подобных решений, разделить которые можно на: Поставка по OEM-лицензии — вполне официально, но с рисками аналогичными использованию, например, Highcharts, разобранным выше; Полное копирование зарубежной системы, даже не open source, и её выдача за российскую разработку после незначительных доработок. Здесь риски существеннее, поскольку помимо лицензионного вопроса непонятно, какова экспертиза разработчика, его влияние или возможность поддержки и развития исходного кода. Лицензионные риски состоят также в том, что свободный, в полном смысле этого слова, open source — это не навсегда. И в последние годы явственно наметился тренд на коммерциализацию open source решений. Вот несколько примеров: 2018 – MongoDB. Для MongoDB ограничили свободное использование ПО (ранее использовали AGPL, перешли на SSPL). Подробный разбор есть в статье на Хабре от @PeterZaitsev 2020 – HortonWorks (Hadoop). После поглощения HortonWorks, Cloudera закрывает доступ к сборкам HDP и бесплатный доступ к CDH. 2021 – ElasticSearch Kibana. Elastic меняет правила лицензирования (ранее — Apache 2.0, потом — SSPL, а в 2024 году пообещали вернуться в open source под лицензией AGPL). – CentOS. RedHat меняет правила разработки бесплатного CentOS. 2023 – Terraform. HashiCorp изменил тип лицензии на BSL. – The Document Foundation. Российских разработчиков исключили из консультационного совета OSS проекта LibreOffice. 2024 – CockroachDB. Cockroach Labs изменил тип лицензии. – Greenplum. Закрытие OSS-проекта после поглощения Broadcom. 13.03.2025, 16:39 Чужое открытое ПО внутри BI-систем: проблемы лицензирования, о которых не все задумываются / Хабр https://habr.com/ru/companies/luxms_bi/articles/890568/ 4/11 – CentOS. RedHat прекратила поддержку CentOS. – Redis. Redis изменил тип лицензии с BSD на Redis Source Available License (RSALv2) и Server Side Public License (SSPLv1). – Linux. Российских разработчиков исключили из OSS проекта Linux и доступ к репозиторию ПО. Я не хочу сказать, что использовать open source — это плохо и вредно. Мы в платформе Luxms BI тоже используем open source компоненты, но все они соответствуют следующим условиям: Открытые исходные коды (open source); Наличие публично доступного репозитария для скачивания исходных кодов; Выход новых версий (или исправление ошибок) не реже двух раз в год; Свободная лицензия: Apache 2.0, MIT, BSD, LGPL или совместимая. Полный список — в нашей документации. Поэтому всё, что я хочу донести, это важность для компании, которая выбирает BI- систему, провести дополнительный юридический анализ, чтобы избежать лицензионных рисков: Выяснить, какое открытое ПО используется в системе; Проверить лицензию не только выбранной BI-системы, но и всех её компонентов; Проанализировать все лицензионные соглашения и действующие в них разрешения и ограничения; Убедиться, что по этим параметрам всё устраивает и подходит; Убедиться, что вендор выполняет требования лицензии: например, делает все необходимые отчисления. А вы обращали внимание на тренд смены лицензий у open source проектов? Используете ли открытое ПО в своей работе и смены лицензии какого проекта не хотите больше всего?",iamrozanov
